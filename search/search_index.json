{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Project Mu \u00b6 Project Mu is a modular adaptation of TianoCore's edk2 tuned for building modern devices using a scalable, maintainable, and reusable pattern. Mu is built around the idea that shipping and maintaining a UEFI product is an ongoing collaboration between numerous partners. For too long, the industry has built products using a \"forking\" model combined with copy/paste/rename and with each new product, the maintenance burden grows to such a level that updates are near impossible due to cost and risk. Project Mu also tries to address the complex business relationships and legal challenges facing partners today. To build most products, it often requires both closed-source, proprietary assets as well as open-source and industry-standard code. The distributed build system and multi-repository design allow product teams to keep code separate and connected to their original source while respecting legal and business boundaries. Project Mu originated from building modern Windows PCs but its patterns and design allow it to be scaled down or up for whatever the final product's intent. IoT, Server, PC, or any other form factor should be able to leverage the content. Primary Goals \u00b6 Initially, this project will focus on two central goals. Share our active code tree to both solicit feedback and entice partners to collaborate \u00b6 Project Mu is an active project. This is not a side project, mirror, clone, or example. This is the same code used today on many of Microsoft's 1 st party devices and it will be kept current because it must be to continue to enable shipping products. Promote, evangelize, and support an industry shift to a more collaborative environment so we all can build and maintain products with lower costs and higher quality \u00b6 Today's open source projects, although extremely valuable, are very resource-intensive to interact with. This friction leads to major industry players avoiding public interaction, thus diminishing the overall community\u2019s value. The modern era of open source projects has incorporated new tools and procedures to lower this friction and it is our goal to leverage those tools. GitHub provides issue tracking, Pull Requests, Gated builds, tracked/required web-based code reviews, and CI/CD (Continuous integration and delivery). It is our belief that by leveraging and extending this automation and workflow, we can lower the friction and foster a safe place for all contributors to work. Guiding Principles \u00b6 Less is More * Be open to change / flexible - Keep learning. If it was easy this would have been solved before Design for code reuse Leverage tools / invest in automation Navigation \u00b6 Have a look around this site to see what is Project Mu. Start by reviewing the details of the community and our process. See how to interact and get involved, why it's different, how to work within or extend it, as well as where everything is located. Finally, explore the Developer Docs if you want to review more in-depth details. Having trouble? \u00b6 Skim the FAQ Roadmap \u00b6 After the first few months of Mu, our initial roadmap is largely complete. Any remaining items have been moved to the GitHub Issues and will continue to be tracked there. We hope to use GitHub Issues to track new roadmap items going forwards. Project Mu GitHub Issues Join Us \u00b6 Contact info and additional methods to collaborate coming soon. Code of conduct \u00b6 This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments. Reporting Issues \u00b6 Short answer: Open a github issue. More details: Contributing Contributing \u00b6 Short answer: Open a pull request. More details: Contributing License \u00b6 Refer to License Documentation Build Information Version: 0.8.0 Build Time: 2019-12-11 04:30","title":"Home"},{"location":"#welcome-to-project-mu","text":"Project Mu is a modular adaptation of TianoCore's edk2 tuned for building modern devices using a scalable, maintainable, and reusable pattern. Mu is built around the idea that shipping and maintaining a UEFI product is an ongoing collaboration between numerous partners. For too long, the industry has built products using a \"forking\" model combined with copy/paste/rename and with each new product, the maintenance burden grows to such a level that updates are near impossible due to cost and risk. Project Mu also tries to address the complex business relationships and legal challenges facing partners today. To build most products, it often requires both closed-source, proprietary assets as well as open-source and industry-standard code. The distributed build system and multi-repository design allow product teams to keep code separate and connected to their original source while respecting legal and business boundaries. Project Mu originated from building modern Windows PCs but its patterns and design allow it to be scaled down or up for whatever the final product's intent. IoT, Server, PC, or any other form factor should be able to leverage the content.","title":"Welcome to Project Mu"},{"location":"#primary-goals","text":"Initially, this project will focus on two central goals.","title":"Primary Goals"},{"location":"#share-our-active-code-tree-to-both-solicit-feedback-and-entice-partners-to-collaborate","text":"Project Mu is an active project. This is not a side project, mirror, clone, or example. This is the same code used today on many of Microsoft's 1 st party devices and it will be kept current because it must be to continue to enable shipping products.","title":"Share our active code tree to both solicit feedback and entice partners to collaborate"},{"location":"#promote-evangelize-and-support-an-industry-shift-to-a-more-collaborative-environment-so-we-all-can-build-and-maintain-products-with-lower-costs-and-higher-quality","text":"Today's open source projects, although extremely valuable, are very resource-intensive to interact with. This friction leads to major industry players avoiding public interaction, thus diminishing the overall community\u2019s value. The modern era of open source projects has incorporated new tools and procedures to lower this friction and it is our goal to leverage those tools. GitHub provides issue tracking, Pull Requests, Gated builds, tracked/required web-based code reviews, and CI/CD (Continuous integration and delivery). It is our belief that by leveraging and extending this automation and workflow, we can lower the friction and foster a safe place for all contributors to work.","title":"Promote, evangelize, and support an industry shift to a more collaborative environment so we all can build and maintain products with lower costs and higher quality"},{"location":"#guiding-principles","text":"Less is More * Be open to change / flexible - Keep learning. If it was easy this would have been solved before Design for code reuse Leverage tools / invest in automation","title":"Guiding Principles"},{"location":"#navigation","text":"Have a look around this site to see what is Project Mu. Start by reviewing the details of the community and our process. See how to interact and get involved, why it's different, how to work within or extend it, as well as where everything is located. Finally, explore the Developer Docs if you want to review more in-depth details.","title":"Navigation"},{"location":"#having-trouble","text":"Skim the FAQ","title":"Having trouble?"},{"location":"#roadmap","text":"After the first few months of Mu, our initial roadmap is largely complete. Any remaining items have been moved to the GitHub Issues and will continue to be tracked there. We hope to use GitHub Issues to track new roadmap items going forwards. Project Mu GitHub Issues","title":"Roadmap"},{"location":"#join-us","text":"Contact info and additional methods to collaborate coming soon.","title":"Join Us"},{"location":"#code-of-conduct","text":"This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.","title":"Code of conduct"},{"location":"#reporting-issues","text":"Short answer: Open a github issue. More details: Contributing","title":"Reporting Issues"},{"location":"#contributing","text":"Short answer: Open a pull request. More details: Contributing","title":"Contributing"},{"location":"#license","text":"Refer to License Documentation Build Information Version: 0.8.0 Build Time: 2019-12-11 04:30","title":"License"},{"location":"faq/","text":"FAQ \u00b6 Purpose/Goals \u00b6 How is this related to TianoCore? \u00b6 As you can probably tell, Project Mu is based on TianoCore . It represents a variant of TianoCore that was customized within Microsoft for scaling and maintainability. It's not exactly a staging branch for TianoCore, as there are some changes that may not have application within or meet the explicit goals of that project, but it is a place where features and changes can be publicly featured and discussed. So, is this a fork? \u00b6 Not entirely. It is our goal to continue to treat TianoCore as a true upstream. Our release branches will always be based on the latest stable TianoCore release, and we will always try to PR viable fixes and features into the TianoCore project. What is it? Where is it going? \u00b6 Project Mu is a product of the Microsoft Core UEFI team and is the basis for the system firmware within a number of Microsoft products. It will continue to be maintained to reflect the FW practices and features leveraged for the best experience with Windows and other Microsoft products. A secondary purpose is to engage with the community, both in TianoCore and the industry at large. We hope that Project Mu serves as a concrete example for discussing different approaches to managing the challenges faced by the UEFI ecosystem. Content/Structure \u00b6 Is this really following \"Less is More\"? \u00b6 Yes. The idea is lowering the entanglement of code, lowering the coupling, and allowing the product to pick and choose the code it needs. This means when building any given product, you don't need all the Project Mu code. Why are there so many repos? \u00b6 Project Mu makes liberal use of multiple repositories due to the mixture of requirements in the UEFI ecosystem. Some repos are split for technical reasons, some for organizational, and some for legal. For details, see \"Repo Philosophy\" in What and Why .","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"faq/#purposegoals","text":"","title":"Purpose/Goals"},{"location":"faq/#how-is-this-related-to-tianocore","text":"As you can probably tell, Project Mu is based on TianoCore . It represents a variant of TianoCore that was customized within Microsoft for scaling and maintainability. It's not exactly a staging branch for TianoCore, as there are some changes that may not have application within or meet the explicit goals of that project, but it is a place where features and changes can be publicly featured and discussed.","title":"How is this related to TianoCore?"},{"location":"faq/#so-is-this-a-fork","text":"Not entirely. It is our goal to continue to treat TianoCore as a true upstream. Our release branches will always be based on the latest stable TianoCore release, and we will always try to PR viable fixes and features into the TianoCore project.","title":"So, is this a fork?"},{"location":"faq/#what-is-it-where-is-it-going","text":"Project Mu is a product of the Microsoft Core UEFI team and is the basis for the system firmware within a number of Microsoft products. It will continue to be maintained to reflect the FW practices and features leveraged for the best experience with Windows and other Microsoft products. A secondary purpose is to engage with the community, both in TianoCore and the industry at large. We hope that Project Mu serves as a concrete example for discussing different approaches to managing the challenges faced by the UEFI ecosystem.","title":"What is it? Where is it going?"},{"location":"faq/#contentstructure","text":"","title":"Content/Structure"},{"location":"faq/#is-this-really-following-less-is-more","text":"Yes. The idea is lowering the entanglement of code, lowering the coupling, and allowing the product to pick and choose the code it needs. This means when building any given product, you don't need all the Project Mu code.","title":"Is this really following \"Less is More\"?"},{"location":"faq/#why-are-there-so-many-repos","text":"Project Mu makes liberal use of multiple repositories due to the mixture of requirements in the UEFI ecosystem. Some repos are split for technical reasons, some for organizational, and some for legal. For details, see \"Repo Philosophy\" in What and Why .","title":"Why are there so many repos?"},{"location":"license/","text":"Licensing for Project Mu \u00b6 Project Mu has numerous repositories. Each of these can have different licenses depending on the content and partner but in general we want OSS friendly licenses. For this documentation we use the following license. License \u00b6 BSD 2-Clause License Copyright \u00a9 Microsoft All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"License"},{"location":"license/#licensing-for-project-mu","text":"Project Mu has numerous repositories. Each of these can have different licenses depending on the content and partner but in general we want OSS friendly licenses. For this documentation we use the following license.","title":"Licensing for Project Mu"},{"location":"license/#license","text":"BSD 2-Clause License Copyright \u00a9 Microsoft All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"License"},{"location":"CodeDevelopment/compile/","text":"How to Build/Compile \u00b6 The repository/product/project should describe any unique steps required. Project Mu currently supports two methods of building. Those will be described here to encourage pattern/code reuse and limit the required repository specific documentation. Compile Testing aka Mu_Build \u00b6 Mu_Build is a framework for running a battery of tests against a single Mu repository (and its dependencies). A plugin model is used for adding additional tests. Today one such plugin is a basic compile test. The repository maintainer may add additional tests such as linters, etc. It is often desirable to compile test code and at times there might not be a product to test with. This is also how the Pull Requests gates are implemented and enforced. Note This also runs the other static code tests so it does more than compile. Assumption is that the repository to compile has been cloned to your filesystem and is in the state ready to compile. Open cmd prompt at workspace root Suggestion: Activate your python virtual environment Install or update Python dependencies using pip Run Mu_Build to: Clone code dependencies Download binary dependencies Statically test code Compile test code mu_build -c <Mu Repo Build Config File> -p <1st Mu Pkg Build Config File> <2nd Mu Pkg Build Config File...> Open TestResults.xml for results Open log files to debug any errors Project Build aka PlatformBuild \u00b6 Info There is currently no example in Project Mu. An example platform is in the works! When you actually want to compile for a platform that will create a firmware binary which can be flashed and execute on a platform this process is necessary. open cmd prompt at workspace root Suggestion: Activate your python virtual environment Install or update Python dependencies using pip Locate the PlatformBuild.py file (generally in the platform build dir) Run PlatformBuild.py Other features \u00b6 PlatformBuild.py leverages a common UefiBuild python component. This component provides a common set of features. The UefiBuild component documentation is published from the mu_pip_environment repository but here are a few of the common features developers find useful. Control the target of the build. Pass Target=RELEASE Build a single module: BuildModule=MdePkg/ModuleToBuild.inf Build with reporting: Single report type BUILDREPORTING=TRUE BUILDREPORT_TYPES=\"PCD\" Change report file BUILDREPORT_FILE=filename.txt default is BUILD_REPORT.TXT All report types. BUILDREPORTING=TRUE BUILDREPORT_TYPES=\"PCD DEPEX FLASH BUILD_FLAGS LIBRARY\" Clean build: --clean Clean only (no compile): --cleanonly Skip some of the build steps: Skip the Edk2 build step: --skipbuild Skip pre or post build steps: --skipprebuild or --skippostbuild Change a Build variable that is used in Edk2 build process: BLD_*_DEBUG_OUTPUT_LEVEL=0x80000004 will be passed to DSC/FDF as DEBUG_OUTPUT_LEVEL . These variable names and behavior are platform defined. BLD_*_<var name> is used for builds of any target type unless there is a more specific version for the given target type. BLD_DEBUG_<var name> is used for debug builds only BLD_RELEASE_<var name> is used for release builds only Using a config file. To simplify calling of PlatformBuild.py if there is a BuildConfig.conf in the root of your UEFI workspace those parameters will be used as well. The command line overrides anything from the conf file. Example BuildConfig.conf \u00b6 # Turn on full build reports BUILDREPORTING=TRUE BUILDREPORT_TYPES=\"PCD DEPEX FLASH BUILD_FLAGS LIBRARY\"","title":"Compiling"},{"location":"CodeDevelopment/compile/#how-to-buildcompile","text":"The repository/product/project should describe any unique steps required. Project Mu currently supports two methods of building. Those will be described here to encourage pattern/code reuse and limit the required repository specific documentation.","title":"How to Build/Compile"},{"location":"CodeDevelopment/compile/#compile-testing-aka-mu_build","text":"Mu_Build is a framework for running a battery of tests against a single Mu repository (and its dependencies). A plugin model is used for adding additional tests. Today one such plugin is a basic compile test. The repository maintainer may add additional tests such as linters, etc. It is often desirable to compile test code and at times there might not be a product to test with. This is also how the Pull Requests gates are implemented and enforced. Note This also runs the other static code tests so it does more than compile. Assumption is that the repository to compile has been cloned to your filesystem and is in the state ready to compile. Open cmd prompt at workspace root Suggestion: Activate your python virtual environment Install or update Python dependencies using pip Run Mu_Build to: Clone code dependencies Download binary dependencies Statically test code Compile test code mu_build -c <Mu Repo Build Config File> -p <1st Mu Pkg Build Config File> <2nd Mu Pkg Build Config File...> Open TestResults.xml for results Open log files to debug any errors","title":"Compile Testing aka Mu_Build"},{"location":"CodeDevelopment/compile/#project-build-aka-platformbuild","text":"Info There is currently no example in Project Mu. An example platform is in the works! When you actually want to compile for a platform that will create a firmware binary which can be flashed and execute on a platform this process is necessary. open cmd prompt at workspace root Suggestion: Activate your python virtual environment Install or update Python dependencies using pip Locate the PlatformBuild.py file (generally in the platform build dir) Run PlatformBuild.py","title":"Project Build aka PlatformBuild"},{"location":"CodeDevelopment/compile/#other-features","text":"PlatformBuild.py leverages a common UefiBuild python component. This component provides a common set of features. The UefiBuild component documentation is published from the mu_pip_environment repository but here are a few of the common features developers find useful. Control the target of the build. Pass Target=RELEASE Build a single module: BuildModule=MdePkg/ModuleToBuild.inf Build with reporting: Single report type BUILDREPORTING=TRUE BUILDREPORT_TYPES=\"PCD\" Change report file BUILDREPORT_FILE=filename.txt default is BUILD_REPORT.TXT All report types. BUILDREPORTING=TRUE BUILDREPORT_TYPES=\"PCD DEPEX FLASH BUILD_FLAGS LIBRARY\" Clean build: --clean Clean only (no compile): --cleanonly Skip some of the build steps: Skip the Edk2 build step: --skipbuild Skip pre or post build steps: --skipprebuild or --skippostbuild Change a Build variable that is used in Edk2 build process: BLD_*_DEBUG_OUTPUT_LEVEL=0x80000004 will be passed to DSC/FDF as DEBUG_OUTPUT_LEVEL . These variable names and behavior are platform defined. BLD_*_<var name> is used for builds of any target type unless there is a more specific version for the given target type. BLD_DEBUG_<var name> is used for debug builds only BLD_RELEASE_<var name> is used for release builds only Using a config file. To simplify calling of PlatformBuild.py if there is a BuildConfig.conf in the root of your UEFI workspace those parameters will be used as well. The command line overrides anything from the conf file.","title":"Other features"},{"location":"CodeDevelopment/compile/#example-buildconfigconf","text":"# Turn on full build reports BUILDREPORTING=TRUE BUILDREPORT_TYPES=\"PCD DEPEX FLASH BUILD_FLAGS LIBRARY\"","title":"Example BuildConfig.conf"},{"location":"CodeDevelopment/overview/","text":"Code Development Overview \u00b6 Tools \u00b6 First you will need to setup your UEFI development environment. Project Mu leverages most of the tools from TianoCore EDK2 . We have streamlined the process for the tool chains and systems we use but our project's goals are to support various tool chains and development environments. For the best experience or for those new to UEFI and Project Mu we have provided guidance in our prerequisites page. Code \u00b6 Next you will need to clone a repository or set of repositories to work on. For core work (Project Mu Repos) you can clone the desired repo, make your changes, run CI builds, run your tests, and submit a PR. For platform work (outside of Project Mu) you will need to clone the platform repository and then follow the platform setup process. See details on the compile page for more information about CI builds and how to compile a package or platform. Code should follow best practices. We are working to add some best practices on the requirements page. We also attempt to enforce these best practices thru our CI build process. Tests \u00b6 One area of focus for Project Mu is on testing. Firmware testing has traditionally been hard and very manual. We hope to describe techniques and provide resources to make this easier and more automated. Testing needs to be part of the code development process. Check out the testing page for more details.","title":"Overview"},{"location":"CodeDevelopment/overview/#code-development-overview","text":"","title":"Code Development Overview"},{"location":"CodeDevelopment/overview/#tools","text":"First you will need to setup your UEFI development environment. Project Mu leverages most of the tools from TianoCore EDK2 . We have streamlined the process for the tool chains and systems we use but our project's goals are to support various tool chains and development environments. For the best experience or for those new to UEFI and Project Mu we have provided guidance in our prerequisites page.","title":"Tools"},{"location":"CodeDevelopment/overview/#code","text":"Next you will need to clone a repository or set of repositories to work on. For core work (Project Mu Repos) you can clone the desired repo, make your changes, run CI builds, run your tests, and submit a PR. For platform work (outside of Project Mu) you will need to clone the platform repository and then follow the platform setup process. See details on the compile page for more information about CI builds and how to compile a package or platform. Code should follow best practices. We are working to add some best practices on the requirements page. We also attempt to enforce these best practices thru our CI build process.","title":"Code"},{"location":"CodeDevelopment/overview/#tests","text":"One area of focus for Project Mu is on testing. Firmware testing has traditionally been hard and very manual. We hope to describe techniques and provide resources to make this easier and more automated. Testing needs to be part of the code development process. Check out the testing page for more details.","title":"Tests"},{"location":"CodeDevelopment/prerequisites/","text":"Prerequisites for building Code \u00b6 Generally there are a set of tools required on the platform. Project Mu tries to minimize the number of global tools but there are a few. There could be more depending on the repository/product/platform you are building but this should get you started. If the repo requires other tools those should be documented within the repo. The tools also vary by Operating System and Compiler choice. Project Mu will document what is currently supported but the expectation is that between Project Mu and TianoCore Edk2 you could use any of those tool sets. Windows \u00b6 Python \u00b6 Download latest Python from https://www.python.org/downloads https://www.python.org/ftp/python/3.7.4/python-3.7.4-amd64.exe It is recommended you use the following options when installing python: include pip support include test support Git \u00b6 Download latest Git For Windows from https://git-scm.com/download/win https://github.com/git-for-windows/git/releases/download/v2.20.1.windows.1/Git-2.20.1-64-bit.exe It is recommended you use the following options: Checkout as is, commit as is. Native Channel support (this will help in corp environments) Check the box to \"Enable Git Credential Manager\" Visual Studio 2017 \u00b6 Download latest version of VS build Tools to c:\\TEMP https://aka.ms/vs/15/release/vs_buildtools.exe Install from cmd line with required features (this set will change overtime). C:\\TEMP\\vs_buildtools.exe --quiet --wait --norestart --nocache --installPath C:\\BuildTools --add Microsoft.VisualStudio.Component.VC.CoreBuildTools --add Microsoft.VisualStudio.Component.VC.Tools.x86.x64 --add Microsoft.VisualStudio.Component.Windows10SDK.17763 --add Microsoft.VisualStudio.Component.VC.Tools.ARM --add Microsoft.VisualStudio.Component.VC.Tools.ARM64 See component list here for more options. https://docs.microsoft.com/en-us/visualstudio/install/workload-component-id-vs-build-tools?view=vs-2017 Visual Studio 2019 Early Support \u00b6 Download latest version of VS build Tools to c:\\TEMP https://aka.ms/vs/16/release/vs_buildtools.exe Install from cmd line with required features (this set will change over time). C:\\TEMP\\vs_buildtools.exe --quiet --wait --norestart --nocache --installPath C:\\BuildTools --add Microsoft.VisualStudio.Component.VC.CoreBuildTools --add Microsoft.VisualStudio.Component.VC.Tools.x86.x64 --add Microsoft.VisualStudio.Component.Windows10SDK.17763 --add Microsoft.VisualStudio.Component.VC.Tools.ARM --add Microsoft.VisualStudio.Component.VC.Tools.ARM64 See component list here for more options. https://docs.microsoft.com/en-us/visualstudio/install/workload-component-id-vs-build-tools?view=vs-2019 Optional - Windows Driver Kit \u00b6 Provides Inf2Cat.exe, needed to prepare Windows firmware update packages for signing . Download the WDK installer https://go.microsoft.com/fwlink/?linkid=2085767 Install from cmd line with required features (this set will change over time). wdksetup.exe /features OptionId.WindowsDriverKitComplete /q Optional - Create an Omnicache \u00b6 An Omnicache is a Project Mu tool that leverages git features to speed up git update operations. This helps speed up git operations if you have multiple workspaces by using the git \"--reference\" feature. Omnicache is documented in the Mu Pip Environment section of this site. Windows Subsystem For Linux (WSL) \u00b6 Coming soon All Operating Systems - Python Virtual Environment and Mu Build Tools \u00b6 In all Operating Systems environments the Project Mu Build tools are needed. Python virtual environments are strongly suggested especially when doing development in multiple workspaces. Each workspace should have its own virtual environment as to not modify the global system state. Since Project Mu uses Pip modules this allows each workspace to keep the versions in sync with the workspace requirements. More info on Python Virtual Environments: https://docs.python.org/3/library/venv.html Workspace Virtual Environment Setup Process \u00b6 A sample directory layout of workspaces and Python Virtual Environments: \u00b6 code |-- edk2 |-- env_dev <--- env for Mu Dev |-- env_docs <--- env for Mu Docs |-- env_edk <--- env for TianoCore |-- env_local <--- env for -e installations of mu_pip/edk2tool |-- Omnicache |-- Palindrome |-- Palindrome2 Do this one time per workspace Open Cmd Prompt in the directory where you want to store your virtual environment. A directory adjacent to workspace directories is convenient. run python cmd python -m venv <your virtual env name> Activate it for your session. Activate Virtual Environment \u00b6 Do this each time you open a new command window to build your workspace. Open Cmd Prompt run activate script - for windows cmd prompt (cmd.exe) do this <your virtual env name>\\Script\\activate cd into your workspace directory Update/Install your python pip requirements. This is generally at the workspace root. pip install --upgrade -r requirements.txt Do dev work and run your builds! More About Project Mu tools using Pip \u00b6 Project Mu currently has 3 pip modules mu_python_library \u00b6 UEFI, Edk2, Acpi, and TPM common library functions. python -m pip install --upgrade mu_python_library mu_environment \u00b6 Self Describing Environment (SDE) code which is used to organize and coordinate UEFI builds. This is the Project Mu Build system, plugin manager, edk2 build wrapper, logging, etc. python -m pip install --upgrade mu_environment mu_build \u00b6 CI and package test scripts. Supports compiling as well as running other build test plugins. python -m pip install --upgrade mu_build","title":"Tools and Prerequisite"},{"location":"CodeDevelopment/prerequisites/#prerequisites-for-building-code","text":"Generally there are a set of tools required on the platform. Project Mu tries to minimize the number of global tools but there are a few. There could be more depending on the repository/product/platform you are building but this should get you started. If the repo requires other tools those should be documented within the repo. The tools also vary by Operating System and Compiler choice. Project Mu will document what is currently supported but the expectation is that between Project Mu and TianoCore Edk2 you could use any of those tool sets.","title":"Prerequisites for building Code"},{"location":"CodeDevelopment/prerequisites/#windows","text":"","title":"Windows"},{"location":"CodeDevelopment/prerequisites/#python","text":"Download latest Python from https://www.python.org/downloads https://www.python.org/ftp/python/3.7.4/python-3.7.4-amd64.exe It is recommended you use the following options when installing python: include pip support include test support","title":"Python"},{"location":"CodeDevelopment/prerequisites/#git","text":"Download latest Git For Windows from https://git-scm.com/download/win https://github.com/git-for-windows/git/releases/download/v2.20.1.windows.1/Git-2.20.1-64-bit.exe It is recommended you use the following options: Checkout as is, commit as is. Native Channel support (this will help in corp environments) Check the box to \"Enable Git Credential Manager\"","title":"Git"},{"location":"CodeDevelopment/prerequisites/#visual-studio-2017","text":"Download latest version of VS build Tools to c:\\TEMP https://aka.ms/vs/15/release/vs_buildtools.exe Install from cmd line with required features (this set will change overtime). C:\\TEMP\\vs_buildtools.exe --quiet --wait --norestart --nocache --installPath C:\\BuildTools --add Microsoft.VisualStudio.Component.VC.CoreBuildTools --add Microsoft.VisualStudio.Component.VC.Tools.x86.x64 --add Microsoft.VisualStudio.Component.Windows10SDK.17763 --add Microsoft.VisualStudio.Component.VC.Tools.ARM --add Microsoft.VisualStudio.Component.VC.Tools.ARM64 See component list here for more options. https://docs.microsoft.com/en-us/visualstudio/install/workload-component-id-vs-build-tools?view=vs-2017","title":"Visual Studio 2017"},{"location":"CodeDevelopment/prerequisites/#visual-studio-2019-early-support","text":"Download latest version of VS build Tools to c:\\TEMP https://aka.ms/vs/16/release/vs_buildtools.exe Install from cmd line with required features (this set will change over time). C:\\TEMP\\vs_buildtools.exe --quiet --wait --norestart --nocache --installPath C:\\BuildTools --add Microsoft.VisualStudio.Component.VC.CoreBuildTools --add Microsoft.VisualStudio.Component.VC.Tools.x86.x64 --add Microsoft.VisualStudio.Component.Windows10SDK.17763 --add Microsoft.VisualStudio.Component.VC.Tools.ARM --add Microsoft.VisualStudio.Component.VC.Tools.ARM64 See component list here for more options. https://docs.microsoft.com/en-us/visualstudio/install/workload-component-id-vs-build-tools?view=vs-2019","title":"Visual Studio 2019 Early Support"},{"location":"CodeDevelopment/prerequisites/#optional-windows-driver-kit","text":"Provides Inf2Cat.exe, needed to prepare Windows firmware update packages for signing . Download the WDK installer https://go.microsoft.com/fwlink/?linkid=2085767 Install from cmd line with required features (this set will change over time). wdksetup.exe /features OptionId.WindowsDriverKitComplete /q","title":"Optional - Windows Driver Kit"},{"location":"CodeDevelopment/prerequisites/#optional-create-an-omnicache","text":"An Omnicache is a Project Mu tool that leverages git features to speed up git update operations. This helps speed up git operations if you have multiple workspaces by using the git \"--reference\" feature. Omnicache is documented in the Mu Pip Environment section of this site.","title":"Optional - Create an Omnicache"},{"location":"CodeDevelopment/prerequisites/#windows-subsystem-for-linux-wsl","text":"Coming soon","title":"Windows Subsystem For Linux (WSL)"},{"location":"CodeDevelopment/prerequisites/#all-operating-systems-python-virtual-environment-and-mu-build-tools","text":"In all Operating Systems environments the Project Mu Build tools are needed. Python virtual environments are strongly suggested especially when doing development in multiple workspaces. Each workspace should have its own virtual environment as to not modify the global system state. Since Project Mu uses Pip modules this allows each workspace to keep the versions in sync with the workspace requirements. More info on Python Virtual Environments: https://docs.python.org/3/library/venv.html","title":"All Operating Systems - Python Virtual Environment and Mu Build Tools"},{"location":"CodeDevelopment/prerequisites/#workspace-virtual-environment-setup-process","text":"","title":"Workspace Virtual Environment Setup Process"},{"location":"CodeDevelopment/prerequisites/#a-sample-directory-layout-of-workspaces-and-python-virtual-environments","text":"code |-- edk2 |-- env_dev <--- env for Mu Dev |-- env_docs <--- env for Mu Docs |-- env_edk <--- env for TianoCore |-- env_local <--- env for -e installations of mu_pip/edk2tool |-- Omnicache |-- Palindrome |-- Palindrome2 Do this one time per workspace Open Cmd Prompt in the directory where you want to store your virtual environment. A directory adjacent to workspace directories is convenient. run python cmd python -m venv <your virtual env name> Activate it for your session.","title":"A sample directory layout of workspaces and Python Virtual Environments:"},{"location":"CodeDevelopment/prerequisites/#activate-virtual-environment","text":"Do this each time you open a new command window to build your workspace. Open Cmd Prompt run activate script - for windows cmd prompt (cmd.exe) do this <your virtual env name>\\Script\\activate cd into your workspace directory Update/Install your python pip requirements. This is generally at the workspace root. pip install --upgrade -r requirements.txt Do dev work and run your builds!","title":"Activate Virtual Environment"},{"location":"CodeDevelopment/prerequisites/#more-about-project-mu-tools-using-pip","text":"Project Mu currently has 3 pip modules","title":"More About Project Mu tools using Pip"},{"location":"CodeDevelopment/prerequisites/#mu_python_library","text":"UEFI, Edk2, Acpi, and TPM common library functions. python -m pip install --upgrade mu_python_library","title":"mu_python_library"},{"location":"CodeDevelopment/prerequisites/#mu_environment","text":"Self Describing Environment (SDE) code which is used to organize and coordinate UEFI builds. This is the Project Mu Build system, plugin manager, edk2 build wrapper, logging, etc. python -m pip install --upgrade mu_environment","title":"mu_environment"},{"location":"CodeDevelopment/prerequisites/#mu_build","text":"CI and package test scripts. Supports compiling as well as running other build test plugins. python -m pip install --upgrade mu_build","title":"mu_build"},{"location":"CodeDevelopment/requirements/","text":"Requirements for contributing Source Code \u00b6 Basics \u00b6 Make sure it follows the package, repo, and codebase rules Make sure it builds Write a unit test for it. Test positive cases as well as negative cases. Make sure it has docs. Even a minimal readme.md will get collected and added to the docs. Make sure it has only valid characters encoded (often copy paste from Microsoft Word docs or the internet will lead to invalid characters) If it is a small change/tweak to existing code that originates outside of Project Mu please mark it with //MUCHANGE Uefi Package \u00b6 UEFI Components \u00b6 All new modules must be listed in their containing package DSC in the components section All modules must follow the dependency rules of their containing package All modules within common layers should avoid silicon or architecture dependencies. Use existing libraries and functionality when possible Build out minimal required abstraction to allow other silicon or architectures to leverage common capabilities Public Header files \u00b6 Don't include other header files Don't mix public and private information in the same header file Implementation details should be contained to the instance Use \"doxygen\" style function header comments to clearly specify parameters and return results. Use a guidgen tool to define any guids For libraries: Library class should be listed in Package DEC file A NULL instance must be created that allows compiling and linking with minimal dependencies. Library Instance \u00b6 The supported module types in the INFs must be accurate. LIBRARY_CLASS: <Library Class Name>|<Module types supported by this instance> Use STATIC on each non-public function and non-public global to avoid conflicts with other modules. Use EFIAPI on all public library class functions. More info \u00b6 For general Edk2 and UEFI development additional information can be found at the TianoCore.org website.","title":"Code Requirements"},{"location":"CodeDevelopment/requirements/#requirements-for-contributing-source-code","text":"","title":"Requirements for contributing Source Code"},{"location":"CodeDevelopment/requirements/#basics","text":"Make sure it follows the package, repo, and codebase rules Make sure it builds Write a unit test for it. Test positive cases as well as negative cases. Make sure it has docs. Even a minimal readme.md will get collected and added to the docs. Make sure it has only valid characters encoded (often copy paste from Microsoft Word docs or the internet will lead to invalid characters) If it is a small change/tweak to existing code that originates outside of Project Mu please mark it with //MUCHANGE","title":"Basics"},{"location":"CodeDevelopment/requirements/#uefi-package","text":"","title":"Uefi Package"},{"location":"CodeDevelopment/requirements/#uefi-components","text":"All new modules must be listed in their containing package DSC in the components section All modules must follow the dependency rules of their containing package All modules within common layers should avoid silicon or architecture dependencies. Use existing libraries and functionality when possible Build out minimal required abstraction to allow other silicon or architectures to leverage common capabilities","title":"UEFI Components"},{"location":"CodeDevelopment/requirements/#public-header-files","text":"Don't include other header files Don't mix public and private information in the same header file Implementation details should be contained to the instance Use \"doxygen\" style function header comments to clearly specify parameters and return results. Use a guidgen tool to define any guids For libraries: Library class should be listed in Package DEC file A NULL instance must be created that allows compiling and linking with minimal dependencies.","title":"Public Header files"},{"location":"CodeDevelopment/requirements/#library-instance","text":"The supported module types in the INFs must be accurate. LIBRARY_CLASS: <Library Class Name>|<Module types supported by this instance> Use STATIC on each non-public function and non-public global to avoid conflicts with other modules. Use EFIAPI on all public library class functions.","title":"Library Instance"},{"location":"CodeDevelopment/requirements/#more-info","text":"For general Edk2 and UEFI development additional information can be found at the TianoCore.org website.","title":"More info"},{"location":"CodeDevelopment/test/","text":"Tests \u00b6 Testing firmware is hard. Lets just stop there. If you want to read on please do at your own risk. Project Mu supports a few types of testing and this page will help provide some high level info and links for more information. Static Code Tests (analysis) \u00b6 Mu_Build provides a framework for running static tests on the code base. Simple tests like character encoding are examples. In Project Mu we are working to expand this set of tests to include checking guids, checking for library classes, etc. UEFI Shell Based Unit Tests \u00b6 UEFI Shell Based Functional Tests \u00b6 UEFI Shell Based Audit Tests \u00b6 Testing Python \u00b6 Create pytest and/or python unit-test compatible tests. Make sure the python code passes the flake8 \"linter\"","title":"Testing"},{"location":"CodeDevelopment/test/#tests","text":"Testing firmware is hard. Lets just stop there. If you want to read on please do at your own risk. Project Mu supports a few types of testing and this page will help provide some high level info and links for more information.","title":"Tests"},{"location":"CodeDevelopment/test/#static-code-tests-analysis","text":"Mu_Build provides a framework for running static tests on the code base. Simple tests like character encoding are examples. In Project Mu we are working to expand this set of tests to include checking guids, checking for library classes, etc.","title":"Static Code Tests (analysis)"},{"location":"CodeDevelopment/test/#uefi-shell-based-unit-tests","text":"","title":"UEFI Shell Based Unit Tests"},{"location":"CodeDevelopment/test/#uefi-shell-based-functional-tests","text":"","title":"UEFI Shell Based Functional Tests"},{"location":"CodeDevelopment/test/#uefi-shell-based-audit-tests","text":"","title":"UEFI Shell Based Audit Tests"},{"location":"CodeDevelopment/test/#testing-python","text":"Create pytest and/or python unit-test compatible tests. Make sure the python code passes the flake8 \"linter\"","title":"Testing Python"},{"location":"DeveloperDocs/attribution/","text":"Documentation framework attribution \u00b6 A special thank you to the people and projects that helped make Project Mu Documentation possible. Projects \u00b6 Mkdocs \u00b6 https://www.mkdocs.org/ MkDocs License (BSD) Copyright \u00a9 2014, Tom Christie. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. mkdocs macros plugin \u00b6 https://github.com/fralau/mkdocs_macros_plugin MIT License Copyright (C) 2018 Laurent Franceschetti Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. Material for MkDocs \u00b6 https://squidfunk.github.io/mkdocs-material/ License MIT License Copyright \u00a9 2016 - 2017 Martin Donath Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. PyMdown Extensions \u00b6 https://facelessuser.github.io/pymdown-extensions/ PyMdown Extensions The MIT License (MIT) (Except where stated below) Copyright \u00a9 2014 - 2018 Isaac Muse Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"Doc Framework Attribution"},{"location":"DeveloperDocs/attribution/#documentation-framework-attribution","text":"A special thank you to the people and projects that helped make Project Mu Documentation possible.","title":"Documentation framework attribution"},{"location":"DeveloperDocs/attribution/#projects","text":"","title":"Projects"},{"location":"DeveloperDocs/attribution/#mkdocs","text":"https://www.mkdocs.org/ MkDocs License (BSD) Copyright \u00a9 2014, Tom Christie. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"Mkdocs"},{"location":"DeveloperDocs/attribution/#mkdocs-macros-plugin","text":"https://github.com/fralau/mkdocs_macros_plugin MIT License Copyright (C) 2018 Laurent Franceschetti Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"mkdocs macros plugin"},{"location":"DeveloperDocs/attribution/#material-for-mkdocs","text":"https://squidfunk.github.io/mkdocs-material/ License MIT License Copyright \u00a9 2016 - 2017 Martin Donath Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"Material for MkDocs"},{"location":"DeveloperDocs/attribution/#pymdown-extensions","text":"https://facelessuser.github.io/pymdown-extensions/ PyMdown Extensions The MIT License (MIT) (Except where stated below) Copyright \u00a9 2014 - 2018 Isaac Muse Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"PyMdown Extensions"},{"location":"DeveloperDocs/build_community_docs/","text":"Building Community Docs \u00b6 Info Today this process has been validated for use on Windows 10. This setup process is expected to roughly the same on other operating systems and none of the actual documentation source or tools should have any OS dependency. Get the docs repository \u00b6 First, you need to clone the project mu docs repository. git clone https://github.com/Microsoft/mu.git Install required tools \u00b6 Install python (Current suggested version is 3.7.x). Current min requirement is python 3.4+. Checkout python.org for directions. Install pip. Generally, this is done when installing python but can also be done as its own process. Details here https://pip.pypa.io/en/stable/installing/#do-i-need-to-install-pip Update pip. python -m pip install --upgrade pip Install dependencies. pip install --upgrade -r requirements.txt if wanting to use spell check Install nodejs from https://nodejs.org/en/ Install cspell npm install -g cspell Install Git on your path (Required for generating dynamic repo based content during preprocess) General Suggested documentation workflow \u00b6 open two command windows at the root of docs repository Window 1: Use to serve files locally Use mkdocs serve Any changes from the DocBuild process will be picked up and served Window 2: Use to preprocess the source repo files Run the DocBuild.py command from this window Make changes to the docs in source repos or this repo and then re-run the DocBuild.py build command Pre-process with dynamic content from source repo(s) \u00b6 Create \"repos\" folder (somewhere outside of workspace) Clone all repositories for dynamic content here Set each repo to the branch/commit that you want to document run the DocBuild.py command supplying the parameters DocBuild.py --clean --build --OutputDir docs --yml mkdocs_base.yml --RootDir ..\\repos Pre-process with no source repo(s) content \u00b6 run the DocBuild.py command supplying minimal parameters DocBuild.py --clean --build --yml mkdocs_base.yml Clean / Remove all pre-processed content \u00b6 use DocBuild.py command DocBuild.py --clean --yml <path to yml base file> --OutputDir <docs folder> Check for character encoding issues \u00b6 navigate to root of repository (should see a docs folder, the mkdocs_base.yml file, and a few other things) open command window run Utf8Test python script cmd prompt Utf8Test.py --RootDir docs should complete with no errors Note Note you can also run it on any dynamic content by using a different RootDir parameter. Use -h for usage to get more detailed information of any failures Use mkdocs to build the docs \u00b6 navigate to root of repository (should see a docs folder, the mkdocs_base.yml file, and a few other things) open command window run mkdocs build from cmd prompt at root mkdocs build -s -v should complete with no errors Spell check the docs \u00b6 navigate to root of repository (should see a docs folder, the mkdocs_base.yml file, and a few other things) open command window run command to spell check cspell docs/**/*.md should complete with no errors False Spelling Errors If the spelling error is a false positive there are two solutions: If it is a valid word or commonly understood term then add the word to the cspell.json config file words section Update the cspell.json file ignorePaths element to ignore the entire file. Locally serve the docs \u00b6 One great feature of mkdocs is how easy it is to locally serve the docs to validate your changes. Use mkdocs to serve your local copy mkdocs serve navigate to 127.0.0.1:8000 in web browser Important If you get an error like Config file 'mkdocs.yml' does not exist you must run the preprocess step. Advanced doc features \u00b6 We do turn on a few advanced/extension features. Please use these carefully as they may break compatibility if the publishing engine is changed. Checkout the sample syntax / test page for syntax and information.","title":"How To Build"},{"location":"DeveloperDocs/build_community_docs/#building-community-docs","text":"Info Today this process has been validated for use on Windows 10. This setup process is expected to roughly the same on other operating systems and none of the actual documentation source or tools should have any OS dependency.","title":"Building Community Docs"},{"location":"DeveloperDocs/build_community_docs/#get-the-docs-repository","text":"First, you need to clone the project mu docs repository. git clone https://github.com/Microsoft/mu.git","title":"Get the docs repository"},{"location":"DeveloperDocs/build_community_docs/#install-required-tools","text":"Install python (Current suggested version is 3.7.x). Current min requirement is python 3.4+. Checkout python.org for directions. Install pip. Generally, this is done when installing python but can also be done as its own process. Details here https://pip.pypa.io/en/stable/installing/#do-i-need-to-install-pip Update pip. python -m pip install --upgrade pip Install dependencies. pip install --upgrade -r requirements.txt if wanting to use spell check Install nodejs from https://nodejs.org/en/ Install cspell npm install -g cspell Install Git on your path (Required for generating dynamic repo based content during preprocess)","title":"Install required tools"},{"location":"DeveloperDocs/build_community_docs/#general-suggested-documentation-workflow","text":"open two command windows at the root of docs repository Window 1: Use to serve files locally Use mkdocs serve Any changes from the DocBuild process will be picked up and served Window 2: Use to preprocess the source repo files Run the DocBuild.py command from this window Make changes to the docs in source repos or this repo and then re-run the DocBuild.py build command","title":"General Suggested documentation workflow"},{"location":"DeveloperDocs/build_community_docs/#pre-process-with-dynamic-content-from-source-repos","text":"Create \"repos\" folder (somewhere outside of workspace) Clone all repositories for dynamic content here Set each repo to the branch/commit that you want to document run the DocBuild.py command supplying the parameters DocBuild.py --clean --build --OutputDir docs --yml mkdocs_base.yml --RootDir ..\\repos","title":"Pre-process with dynamic content from source repo(s)"},{"location":"DeveloperDocs/build_community_docs/#pre-process-with-no-source-repos-content","text":"run the DocBuild.py command supplying minimal parameters DocBuild.py --clean --build --yml mkdocs_base.yml","title":"Pre-process with no source repo(s) content"},{"location":"DeveloperDocs/build_community_docs/#clean-remove-all-pre-processed-content","text":"use DocBuild.py command DocBuild.py --clean --yml <path to yml base file> --OutputDir <docs folder>","title":"Clean / Remove all pre-processed content"},{"location":"DeveloperDocs/build_community_docs/#check-for-character-encoding-issues","text":"navigate to root of repository (should see a docs folder, the mkdocs_base.yml file, and a few other things) open command window run Utf8Test python script cmd prompt Utf8Test.py --RootDir docs should complete with no errors Note Note you can also run it on any dynamic content by using a different RootDir parameter. Use -h for usage to get more detailed information of any failures","title":"Check for character encoding issues"},{"location":"DeveloperDocs/build_community_docs/#use-mkdocs-to-build-the-docs","text":"navigate to root of repository (should see a docs folder, the mkdocs_base.yml file, and a few other things) open command window run mkdocs build from cmd prompt at root mkdocs build -s -v should complete with no errors","title":"Use mkdocs to build the docs"},{"location":"DeveloperDocs/build_community_docs/#spell-check-the-docs","text":"navigate to root of repository (should see a docs folder, the mkdocs_base.yml file, and a few other things) open command window run command to spell check cspell docs/**/*.md should complete with no errors False Spelling Errors If the spelling error is a false positive there are two solutions: If it is a valid word or commonly understood term then add the word to the cspell.json config file words section Update the cspell.json file ignorePaths element to ignore the entire file.","title":"Spell check the docs"},{"location":"DeveloperDocs/build_community_docs/#locally-serve-the-docs","text":"One great feature of mkdocs is how easy it is to locally serve the docs to validate your changes. Use mkdocs to serve your local copy mkdocs serve navigate to 127.0.0.1:8000 in web browser Important If you get an error like Config file 'mkdocs.yml' does not exist you must run the preprocess step.","title":"Locally serve the docs"},{"location":"DeveloperDocs/build_community_docs/#advanced-doc-features","text":"We do turn on a few advanced/extension features. Please use these carefully as they may break compatibility if the publishing engine is changed. Checkout the sample syntax / test page for syntax and information.","title":"Advanced doc features"},{"location":"DeveloperDocs/developer_docs/","text":"Developer Docs \u00b6 Philosophy \u00b6 Documentation is critical. There is a steep learning curve in UEFI and no amount of documentation will change that, but at a minimum quick, clear, and easy documentation can help everyone adopt features faster and with higher confidence. Our documentation system will focus on making this an easy, low friction, and collaborative process. The pull request process will eventually compel developers to submit documentation whenever they submit new components and refactoring. Documentation will be done in markdown as this has the benefit of being easily readable in both plain text as well as transformed into a richer experience. It also is quick to learn and to write. Currently, we leverage mkdocs as our publishing engine but since all content is in markdown it could be transitioned to another engine without significant reinvestment. Community documentation \u00b6 This content is documented in static markdown files within the Project Mu repository. We leverage mkdocs to generate web-hosted content on every change and host these using github.io. These static files focus on how the project and community interact. We strongly encourage contribution and follow the standard PR model for all changes, big and small. Developer documentation \u00b6 This content is documented in a couple of ways. There are static markdown files in the Project Mu repository. This contains details about high level concepts, howto articles, and features of the project and all repos within Project Mu. Examples: Code layout, git usage, tools, building, packaging, etc. There is repo and package level documentation for features. These are also static markdown files but these are contained within the repo that contains the feature. A \u201cdocs\u201d folder for each repo and each package will host this content. Changes will also follow the standard PR model for the containing repo. Next, there is feature and instance documentation. This should inform a developer interested in the implementation specifics of what this module is and what additional requirements it has including code dependencies and limitations. This should be documented in markdown files located with the component. These should be updated whenever the component is updated and should be part of a code PR. Finally, for API and traditional functional documentation, our current stance is this is required in code (public APIs) but the published documentation (doxygen html, pdf, etc) is not necessary. Code tools like vscode already provide a lower friction method to index, find def, and search that uses this content directly embedded in the code.","title":"Overview"},{"location":"DeveloperDocs/developer_docs/#developer-docs","text":"","title":"Developer Docs"},{"location":"DeveloperDocs/developer_docs/#philosophy","text":"Documentation is critical. There is a steep learning curve in UEFI and no amount of documentation will change that, but at a minimum quick, clear, and easy documentation can help everyone adopt features faster and with higher confidence. Our documentation system will focus on making this an easy, low friction, and collaborative process. The pull request process will eventually compel developers to submit documentation whenever they submit new components and refactoring. Documentation will be done in markdown as this has the benefit of being easily readable in both plain text as well as transformed into a richer experience. It also is quick to learn and to write. Currently, we leverage mkdocs as our publishing engine but since all content is in markdown it could be transitioned to another engine without significant reinvestment.","title":"Philosophy"},{"location":"DeveloperDocs/developer_docs/#community-documentation","text":"This content is documented in static markdown files within the Project Mu repository. We leverage mkdocs to generate web-hosted content on every change and host these using github.io. These static files focus on how the project and community interact. We strongly encourage contribution and follow the standard PR model for all changes, big and small.","title":"Community documentation"},{"location":"DeveloperDocs/developer_docs/#developer-documentation","text":"This content is documented in a couple of ways. There are static markdown files in the Project Mu repository. This contains details about high level concepts, howto articles, and features of the project and all repos within Project Mu. Examples: Code layout, git usage, tools, building, packaging, etc. There is repo and package level documentation for features. These are also static markdown files but these are contained within the repo that contains the feature. A \u201cdocs\u201d folder for each repo and each package will host this content. Changes will also follow the standard PR model for the containing repo. Next, there is feature and instance documentation. This should inform a developer interested in the implementation specifics of what this module is and what additional requirements it has including code dependencies and limitations. This should be documented in markdown files located with the component. These should be updated whenever the component is updated and should be part of a code PR. Finally, for API and traditional functional documentation, our current stance is this is required in code (public APIs) but the published documentation (doxygen html, pdf, etc) is not necessary. Code tools like vscode already provide a lower friction method to index, find def, and search that uses this content directly embedded in the code.","title":"Developer documentation"},{"location":"DeveloperDocs/doc_sample_test/","text":"Documentation Sample / Test file / Advanced doc features \u00b6 mkdocs macros plugin \u00b6 This plugin allows providing some variables in mkdocs.yml file and then reference those variables using jinja2 syntax in md files. Most of these variables are populated and created during the DocBuild step and inserted into the yml file. https://github.com/fralau/mkdocs_macros_plugin Material theme \u00b6 This theme provides the skin for the site. This also provides capabilities thru plugins. https://squidfunk.github.io/mkdocs-material/ Markdown Extensions \u00b6 The Material theme supports markdown extensions. Check the yml file for what extensions are currently on. Below is more specific info. https://squidfunk.github.io/mkdocs-material/extensions/permalinks/ https://squidfunk.github.io/mkdocs-material/extensions/pymdown/ Admonition plugin \u00b6 This plugin in combo with the material theme provides great looking ways for doc developers to highlight parts of their message. Please check out: https://squidfunk.github.io/mkdocs-material/extensions/admonition/ for the capabilities and syntax. One example: Note Sample note here. emoji support \u00b6 Who doesn't love using emojis. Icon usage has shown to help communicate directions and cross language barriers. https://facelessuser.github.io/pymdown-extensions/extensions/emoji/ Twitter, github, and emojione tags available. Others \u00b6 Check out the mkdocs.yml file for other extensions and details can be found in the links above.","title":"Sample Syntax"},{"location":"DeveloperDocs/doc_sample_test/#documentation-sample-test-file-advanced-doc-features","text":"","title":"Documentation Sample / Test file / Advanced doc features"},{"location":"DeveloperDocs/doc_sample_test/#mkdocs-macros-plugin","text":"This plugin allows providing some variables in mkdocs.yml file and then reference those variables using jinja2 syntax in md files. Most of these variables are populated and created during the DocBuild step and inserted into the yml file. https://github.com/fralau/mkdocs_macros_plugin","title":"mkdocs macros plugin"},{"location":"DeveloperDocs/doc_sample_test/#material-theme","text":"This theme provides the skin for the site. This also provides capabilities thru plugins. https://squidfunk.github.io/mkdocs-material/","title":"Material theme"},{"location":"DeveloperDocs/doc_sample_test/#markdown-extensions","text":"The Material theme supports markdown extensions. Check the yml file for what extensions are currently on. Below is more specific info. https://squidfunk.github.io/mkdocs-material/extensions/permalinks/ https://squidfunk.github.io/mkdocs-material/extensions/pymdown/","title":"Markdown Extensions"},{"location":"DeveloperDocs/doc_sample_test/#admonition-plugin","text":"This plugin in combo with the material theme provides great looking ways for doc developers to highlight parts of their message. Please check out: https://squidfunk.github.io/mkdocs-material/extensions/admonition/ for the capabilities and syntax. One example: Note Sample note here.","title":"Admonition plugin"},{"location":"DeveloperDocs/doc_sample_test/#emoji-support","text":"Who doesn't love using emojis. Icon usage has shown to help communicate directions and cross language barriers. https://facelessuser.github.io/pymdown-extensions/extensions/emoji/ Twitter, github, and emojione tags available.","title":"emoji support"},{"location":"DeveloperDocs/doc_sample_test/#others","text":"Check out the mkdocs.yml file for other extensions and details can be found in the links above.","title":"Others"},{"location":"DeveloperDocs/requirements/","text":"Requirements for contributing documentation \u00b6 Conventions and lessons learned \u00b6 Please update this list as you learn more. filenames should all be lowercase. filenames should use \"_\" to separate words and should not have spaces. all links to pages are case sensitive (when published to GitHub the server is case sensitive) use a code editor like vscode for markdown. It has linting support and will identify issues prior to build. If you markdown has images: Awesome. Images help make docs more informative and easier to understand Path in markdown to image must be relative Suggested to put in same directory as md file image filename must end with _mu. extension . Example my_image_name_mu.png Supported image extensions are gif, jpg, png","title":"Documentation Requirements"},{"location":"DeveloperDocs/requirements/#requirements-for-contributing-documentation","text":"","title":"Requirements for contributing documentation"},{"location":"DeveloperDocs/requirements/#conventions-and-lessons-learned","text":"Please update this list as you learn more. filenames should all be lowercase. filenames should use \"_\" to separate words and should not have spaces. all links to pages are case sensitive (when published to GitHub the server is case sensitive) use a code editor like vscode for markdown. It has linting support and will identify issues prior to build. If you markdown has images: Awesome. Images help make docs more informative and easier to understand Path in markdown to image must be relative Suggested to put in same directory as md file image filename must end with _mu. extension . Example my_image_name_mu.png Supported image extensions are gif, jpg, png","title":"Conventions and lessons learned"},{"location":"How/contributing/","text":"How to contribute \u00b6 There are three common ways to contribute. Participate in discussions using GitHub issues. Contribute documentation by opening a GitHub Pull Request. Contribute code by opening a GitHub Pull Request Issue Tracker Usage \u00b6 https://github.com/Microsoft/mu/issues General feedback and discussions \u00b6 Please start a discussion on the issue tracker. Bugs and feature requests \u00b6 For non-security related bugs please log a new issue on the Project Mu repo issue tracker . The best way to get your bug fixed is to be as detailed as you can be about the problem. Providing a code snippet or sample driver that exposes the issue with steps to reproduce the problem is ideal. Reporting security issues and bugs \u00b6 Security issues and bugs should be reported privately, via email, to the Microsoft Security Response Center (MSRC) secure@microsoft.com . You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Further information, including the MSRC PGP key, can be found in the Security TechCenter . Contributions of Documentation and/or Code \u00b6 Pull Requests \u00b6 If you don't know what a pull request is read this article: https://help.github.com/articles/about-pull-requests . Make sure the repository can build and all tests pass. Familiarize yourself with the project workflow and our coding conventions. General workflow \u00b6 Fork Repository in GitHub Make desired changes. Build it, test it, document it Submit a Pull Request back to the development branch you would like to target. You will be asked to digitally sign a CLA The server will run some builds and tests and report status Community and reviewers will provide feedback in the Pull Request Make changes / adjust based on feedback and discussion Keep your PR branch in-sync with the branch you are targeting and resolve any merge conflicts Once the the PR status is all passing it can be squashed and merged (just press the button in the PR). If the PR is ready the maintainers may complete it for you. That is it. Thanks for contributing. More details on : Code Development Tests Development Documentation Development Contributor License Agreement (CLA) \u00b6 This project welcomes contributions and suggestions. Most (code and documentation) contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.microsoft.com . When you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.","title":"Contributing"},{"location":"How/contributing/#how-to-contribute","text":"There are three common ways to contribute. Participate in discussions using GitHub issues. Contribute documentation by opening a GitHub Pull Request. Contribute code by opening a GitHub Pull Request","title":"How to contribute"},{"location":"How/contributing/#issue-tracker-usage","text":"https://github.com/Microsoft/mu/issues","title":"Issue Tracker Usage"},{"location":"How/contributing/#general-feedback-and-discussions","text":"Please start a discussion on the issue tracker.","title":"General feedback and discussions"},{"location":"How/contributing/#bugs-and-feature-requests","text":"For non-security related bugs please log a new issue on the Project Mu repo issue tracker . The best way to get your bug fixed is to be as detailed as you can be about the problem. Providing a code snippet or sample driver that exposes the issue with steps to reproduce the problem is ideal.","title":"Bugs and feature requests"},{"location":"How/contributing/#reporting-security-issues-and-bugs","text":"Security issues and bugs should be reported privately, via email, to the Microsoft Security Response Center (MSRC) secure@microsoft.com . You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Further information, including the MSRC PGP key, can be found in the Security TechCenter .","title":"Reporting security issues and bugs"},{"location":"How/contributing/#contributions-of-documentation-andor-code","text":"","title":"Contributions of Documentation and/or Code"},{"location":"How/contributing/#pull-requests","text":"If you don't know what a pull request is read this article: https://help.github.com/articles/about-pull-requests . Make sure the repository can build and all tests pass. Familiarize yourself with the project workflow and our coding conventions.","title":"Pull Requests"},{"location":"How/contributing/#general-workflow","text":"Fork Repository in GitHub Make desired changes. Build it, test it, document it Submit a Pull Request back to the development branch you would like to target. You will be asked to digitally sign a CLA The server will run some builds and tests and report status Community and reviewers will provide feedback in the Pull Request Make changes / adjust based on feedback and discussion Keep your PR branch in-sync with the branch you are targeting and resolve any merge conflicts Once the the PR status is all passing it can be squashed and merged (just press the button in the PR). If the PR is ready the maintainers may complete it for you. That is it. Thanks for contributing. More details on : Code Development Tests Development Documentation Development","title":"General workflow"},{"location":"How/contributing/#contributor-license-agreement-cla","text":"This project welcomes contributions and suggestions. Most (code and documentation) contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.microsoft.com . When you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.","title":"Contributor License Agreement (CLA)"},{"location":"How/release_process/","text":"Overview \u00b6 Contents and Process Under Active Development The basics of this process are identical to those followed by the Project Mu firmware integration and release process internal to Microsoft, but the formal documentation, branch naming, and tagging process is a work in progress. While this is how we expect things to work, there may be changes within the the first few releases driven by feedback within the team and any external consumers/contributors. In the interest of maintaining a close, well-defined relationship with the upstream project, TianoCore, the active release branch of Project Mu is periodically deprecated and all Mu-related changes are rebased onto a selected commit of TianoCore. This keeps Project Mu up to date with TianoCore while highlighting all Project Mu differences in the most recent commits and encouraging the reverse integration of all changes/fixes back into TianoCore In general, the life-cycle of active code follows the following path: All active work in Project Mu is performed on a release/* branch, named sequentially according to the date of TianoCore commit that it's based on (e.g. release/201808 is based on the edk2-stable201808 branch in TianoCore). Work proceeds on that branch until a new TianoCore integration is targeted, at which point a new branch is created and all existing changes are rebased onto the new branch and the new branch is used for all active development going forward. At this point, the previous branch enters a stabilization period where further tests are performed and only bug fixes are allowed to be committed. After stabilization, the branch is labeled as stable and will only receive critical bug fixes either directly to the branch or backported from a more recent release. release/* branches will be maintained in LTS (Long-Term Support) for at least the next two releases. The below diagram illustrates the life-cycle of a single branch and indicates the critical points in its lifetime. These critical points will be applied as tags for reference and documentation. The tags are given a name relative to the target branch and consist of: Upstream base, Rebase complete, Rebase builds, Rebase boots, RCn, and Stable. These tags are discussed in more detail below. Important Due to the impacts of the rebase process on the history of Mu release branches, any downstream consumers will have to follow a similar integration process when upgrading to a new release. Any custom changes made within the Project Mu repos will have to be rebased from one release to the next. This is why we strongly discourage forking Project Mu for direct modification (ie. consumption, not contribution). Instead, leverage the distributed repo management system and override management system to integrate proprietary code/modules. Current Branch Status \u00b6 While the earliest release branches may not be included in this process, starting with release/201903 and going forwards the status of each branch will be recorded in the README.rst file at the root of the branch. In general, the README found in Basecore will contain information that is common to all of the Mu submodules, but each submodule will also have its own README for each release branch that contains notes specific to the development that occurs in that submodule during a release cycle. The README will also contain a summary of the branch status at a given time. For example, here is a sample status for Basecore release/201903 as of the time of this writing: Current Phase: Development Entered Current Phase: 2019/03/25 Planned Exit Date: May 2019 Upstream Integration Phase \u00b6 At this time, we are targeting upstream integrations for roughly once a quarter, attempting to align 1:1 with the TianoCore stable release cadence. Prior to an integration, the status dashboard (not yet created) will be updated with the target date of completion and the target TianoCore commit and/or release. For example, a plan was made to transition off of release/20180529 when TianoCore announced the edk2-stable201808 release. Once a commit is selected, a set of rebase commits will be chosen from the active (previous) release/* branch. Ideally, these commits would include everything from the previous rebase through the most recent *_RC tag. For example, when moving from the release/201808 branch, the commits will be selected from 1808_Upstream (not inclusive) tag to 1808_RC1 . After selection, this list of commits will be evaluated to determine whether any changes are no longer needed in the Mu history. The most likely causes of this action are: A change was submitted to TianoCore and has been accepted since the last rebase. Therefore, the change is no longer needed in Mu history. A change was reverted or modified more recently in Mu history, and the history of this change was squashed to maintain simplicity when comparing with upstream (TianoCore). Once all evaluation is completed, the rebase will be performed in the new release/* branch. This branch will then be built for a reference platform (to be selected by internal team) and booted, at which point it will be considered the active development branch. Integration Milestone Tags \u00b6 During integration, multiple tags are applied to the branch to serve as milestones. They also serve as reference point for changelog documentation that is produced during the integration process. These tags are described below: *_Upstream This tag is placed on the exact TianoCore commit that a given release branch started from. This is used as a reference point between branches and relative to the rebase operation. The documentation produced for this tag contains the differences in TianoCore between this branch and the previous branch. For branches that originated from TianoCore releases, this changelog should be identical to the TianoCore changelog. *_Rebase This tag is placed on the commit at the branch HEAD once the rebase is completed. The only changes to the commits from the last branch should be merge conflict resolutions and any history simplification as described above. The documentation produced for this tag contains a record of these resolutions and simplifications. *_RefBuild This tag is placed on the commit where a reference platform consuming a large portion of the Mu code can successfully build. The documentation produced for this tag contains any changes required to get the reference platform building. It includes a list of changes outside the Mu project that are recommended for any consuming platform. *_RefBoot This tag is placed on the commit where a reference platform consuming a large portion of the Mu code can successfully boot. The documentation produced for this tag contains any changes required to get the reference platform booting. It includes a list of changes outside the Mu project that are recommended for any consuming platform. In each of these cases, the * will be replaced with a corresponding branch name. For example, the tags associated with release/201808 will be prefixed with 1808 (e.g. 1808_Rebase , 1808_RC1 , etc.). Active Development Phase \u00b6 During the active development phase, the release branch is open for comment and contribution both internally and publicly. All work contributed by the Project Mu team will be publicly available after an internal PR review. This commits will automatically be mirrored to the public repos. Similarly, all completed public PRs are mirrored in internal review repos (with preference being given to the public PR in event of a conflict). While this means that there will be times where Project Mu team will make contributions without going through a full public PR review, all code is open to comment and contribution, up to and including a full revert of the internal Mu team contribution. Public Contribution/Commentary \u00b6 For information on the contribution policies and steps, see the How to Contribute document. Upstream Cherry-Picks \u00b6 In the event that a critical change is made in the TianoCore upstream during the Active Development phase, the Project Mu team (with any suggestions or comment from downstream contributors) will evaluate the change for a mid-release cherry pick. If warranted, the commit(s) will be cherry-picked directly from TianoCore and prefixed with a \"CHERRY-PICK\" tag in the commit message so they can be cleaned up in the next rebase. Stabilization Phase \u00b6 When warranted, active development on the active release/* branch will be halted so that it may enter a period of rigorous testing and stabilization. Upon entering the Stabilization phase, the branch will be tagged with a *_RC1 tag and only bug fixes will be accepted from then on. Any defects or regressions found during stabilization will be fixed and documented. Once confidence is built in the stability of the code, the branch will be tagged as *_Stable and it will enter LTS. It is Project Mu's goal that this cadence be aligned with the TianoCore release cadence, with the previous branch stabilizing at the same time a new TianoCore release is available. In this way, development can seamlessly move to the next release/* branch without lapse in availability. Note It is possible that the *_RC1 tag be applied to the same commit as *_Stable if there are no defects found in the branch. (Because that happens all the time.) It is also possible that multiple *_RCn tags may be useful to distinguish between milestones of a particularly protracted Stabilization phase. Transition Branches \u00b6 In the event that it becomes necessary to stabilize a release/* branch prior to the availability of a suitable TianoCore commit for rebasing, all active development will move to a dev/* branch that will branch from the previous *_RC1 tag. If bugs are discovered in the Stabilization phase for the release/* branch, they will also be fixed in the dev/* branch and all changes made in the dev/* branch will be rebased as part of the next release/* branch when it is ready. Long-Term Support (LTS) \u00b6 It is Project Mu's goal that all release/* branches continue to be maintained with active bug fixes -- as necessary -- for at least two full releases after the branch becomes stable. The Project Mu team will serve as the primary deciding body for whether a bug fix to the current release/* branch merits porting back to the prior two branches, but community input or suggestions are always welcome. All release branches that make it to the Stabilization phase will be hosted and kept in the repository in perpetuity. If any change was required to this policy (perhaps for server considerations), the branches will remain archived for posterity and should be available by request. Lifetime of a Single Integration \u00b6 TBD","title":"Release Process"},{"location":"How/release_process/#overview","text":"Contents and Process Under Active Development The basics of this process are identical to those followed by the Project Mu firmware integration and release process internal to Microsoft, but the formal documentation, branch naming, and tagging process is a work in progress. While this is how we expect things to work, there may be changes within the the first few releases driven by feedback within the team and any external consumers/contributors. In the interest of maintaining a close, well-defined relationship with the upstream project, TianoCore, the active release branch of Project Mu is periodically deprecated and all Mu-related changes are rebased onto a selected commit of TianoCore. This keeps Project Mu up to date with TianoCore while highlighting all Project Mu differences in the most recent commits and encouraging the reverse integration of all changes/fixes back into TianoCore In general, the life-cycle of active code follows the following path: All active work in Project Mu is performed on a release/* branch, named sequentially according to the date of TianoCore commit that it's based on (e.g. release/201808 is based on the edk2-stable201808 branch in TianoCore). Work proceeds on that branch until a new TianoCore integration is targeted, at which point a new branch is created and all existing changes are rebased onto the new branch and the new branch is used for all active development going forward. At this point, the previous branch enters a stabilization period where further tests are performed and only bug fixes are allowed to be committed. After stabilization, the branch is labeled as stable and will only receive critical bug fixes either directly to the branch or backported from a more recent release. release/* branches will be maintained in LTS (Long-Term Support) for at least the next two releases. The below diagram illustrates the life-cycle of a single branch and indicates the critical points in its lifetime. These critical points will be applied as tags for reference and documentation. The tags are given a name relative to the target branch and consist of: Upstream base, Rebase complete, Rebase builds, Rebase boots, RCn, and Stable. These tags are discussed in more detail below. Important Due to the impacts of the rebase process on the history of Mu release branches, any downstream consumers will have to follow a similar integration process when upgrading to a new release. Any custom changes made within the Project Mu repos will have to be rebased from one release to the next. This is why we strongly discourage forking Project Mu for direct modification (ie. consumption, not contribution). Instead, leverage the distributed repo management system and override management system to integrate proprietary code/modules.","title":"Overview"},{"location":"How/release_process/#current-branch-status","text":"While the earliest release branches may not be included in this process, starting with release/201903 and going forwards the status of each branch will be recorded in the README.rst file at the root of the branch. In general, the README found in Basecore will contain information that is common to all of the Mu submodules, but each submodule will also have its own README for each release branch that contains notes specific to the development that occurs in that submodule during a release cycle. The README will also contain a summary of the branch status at a given time. For example, here is a sample status for Basecore release/201903 as of the time of this writing: Current Phase: Development Entered Current Phase: 2019/03/25 Planned Exit Date: May 2019","title":"Current Branch Status"},{"location":"How/release_process/#upstream-integration-phase","text":"At this time, we are targeting upstream integrations for roughly once a quarter, attempting to align 1:1 with the TianoCore stable release cadence. Prior to an integration, the status dashboard (not yet created) will be updated with the target date of completion and the target TianoCore commit and/or release. For example, a plan was made to transition off of release/20180529 when TianoCore announced the edk2-stable201808 release. Once a commit is selected, a set of rebase commits will be chosen from the active (previous) release/* branch. Ideally, these commits would include everything from the previous rebase through the most recent *_RC tag. For example, when moving from the release/201808 branch, the commits will be selected from 1808_Upstream (not inclusive) tag to 1808_RC1 . After selection, this list of commits will be evaluated to determine whether any changes are no longer needed in the Mu history. The most likely causes of this action are: A change was submitted to TianoCore and has been accepted since the last rebase. Therefore, the change is no longer needed in Mu history. A change was reverted or modified more recently in Mu history, and the history of this change was squashed to maintain simplicity when comparing with upstream (TianoCore). Once all evaluation is completed, the rebase will be performed in the new release/* branch. This branch will then be built for a reference platform (to be selected by internal team) and booted, at which point it will be considered the active development branch.","title":"Upstream Integration Phase"},{"location":"How/release_process/#integration-milestone-tags","text":"During integration, multiple tags are applied to the branch to serve as milestones. They also serve as reference point for changelog documentation that is produced during the integration process. These tags are described below: *_Upstream This tag is placed on the exact TianoCore commit that a given release branch started from. This is used as a reference point between branches and relative to the rebase operation. The documentation produced for this tag contains the differences in TianoCore between this branch and the previous branch. For branches that originated from TianoCore releases, this changelog should be identical to the TianoCore changelog. *_Rebase This tag is placed on the commit at the branch HEAD once the rebase is completed. The only changes to the commits from the last branch should be merge conflict resolutions and any history simplification as described above. The documentation produced for this tag contains a record of these resolutions and simplifications. *_RefBuild This tag is placed on the commit where a reference platform consuming a large portion of the Mu code can successfully build. The documentation produced for this tag contains any changes required to get the reference platform building. It includes a list of changes outside the Mu project that are recommended for any consuming platform. *_RefBoot This tag is placed on the commit where a reference platform consuming a large portion of the Mu code can successfully boot. The documentation produced for this tag contains any changes required to get the reference platform booting. It includes a list of changes outside the Mu project that are recommended for any consuming platform. In each of these cases, the * will be replaced with a corresponding branch name. For example, the tags associated with release/201808 will be prefixed with 1808 (e.g. 1808_Rebase , 1808_RC1 , etc.).","title":"Integration Milestone Tags"},{"location":"How/release_process/#active-development-phase","text":"During the active development phase, the release branch is open for comment and contribution both internally and publicly. All work contributed by the Project Mu team will be publicly available after an internal PR review. This commits will automatically be mirrored to the public repos. Similarly, all completed public PRs are mirrored in internal review repos (with preference being given to the public PR in event of a conflict). While this means that there will be times where Project Mu team will make contributions without going through a full public PR review, all code is open to comment and contribution, up to and including a full revert of the internal Mu team contribution.","title":"Active Development Phase"},{"location":"How/release_process/#public-contributioncommentary","text":"For information on the contribution policies and steps, see the How to Contribute document.","title":"Public Contribution/Commentary"},{"location":"How/release_process/#upstream-cherry-picks","text":"In the event that a critical change is made in the TianoCore upstream during the Active Development phase, the Project Mu team (with any suggestions or comment from downstream contributors) will evaluate the change for a mid-release cherry pick. If warranted, the commit(s) will be cherry-picked directly from TianoCore and prefixed with a \"CHERRY-PICK\" tag in the commit message so they can be cleaned up in the next rebase.","title":"Upstream Cherry-Picks"},{"location":"How/release_process/#stabilization-phase","text":"When warranted, active development on the active release/* branch will be halted so that it may enter a period of rigorous testing and stabilization. Upon entering the Stabilization phase, the branch will be tagged with a *_RC1 tag and only bug fixes will be accepted from then on. Any defects or regressions found during stabilization will be fixed and documented. Once confidence is built in the stability of the code, the branch will be tagged as *_Stable and it will enter LTS. It is Project Mu's goal that this cadence be aligned with the TianoCore release cadence, with the previous branch stabilizing at the same time a new TianoCore release is available. In this way, development can seamlessly move to the next release/* branch without lapse in availability. Note It is possible that the *_RC1 tag be applied to the same commit as *_Stable if there are no defects found in the branch. (Because that happens all the time.) It is also possible that multiple *_RCn tags may be useful to distinguish between milestones of a particularly protracted Stabilization phase.","title":"Stabilization Phase"},{"location":"How/release_process/#transition-branches","text":"In the event that it becomes necessary to stabilize a release/* branch prior to the availability of a suitable TianoCore commit for rebasing, all active development will move to a dev/* branch that will branch from the previous *_RC1 tag. If bugs are discovered in the Stabilization phase for the release/* branch, they will also be fixed in the dev/* branch and all changes made in the dev/* branch will be rebased as part of the next release/* branch when it is ready.","title":"Transition Branches"},{"location":"How/release_process/#long-term-support-lts","text":"It is Project Mu's goal that all release/* branches continue to be maintained with active bug fixes -- as necessary -- for at least two full releases after the branch becomes stable. The Project Mu team will serve as the primary deciding body for whether a bug fix to the current release/* branch merits porting back to the prior two branches, but community input or suggestions are always welcome. All release branches that make it to the Stabilization phase will be hosted and kept in the repository in perpetuity. If any change was required to this policy (perhaps for server considerations), the branches will remain archived for posterity and should be available by request.","title":"Long-Term Support (LTS)"},{"location":"How/release_process/#lifetime-of-a-single-integration","text":"TBD","title":"Lifetime of a Single Integration"},{"location":"How/using_project_mu/","text":"How to setup a new Repo for a Platform that will use Project MU? \u00b6 This document will describe the base guidelines for setting up a Project MU repo. You will need: 1) Git 2) Python 3.7 3) A text editor 4) Look at layout to understand our recommended repository layout. You can also look at ms-iot iMX8 for a real platform implementation. 0) Nomenclature \u00b6 I will use the term workspace root to reference the base folder for your code tree. Ordinarily, we use the Platform Repository as the outer-most layer. This means that the outermost git repository is where we store Platform specific files and libraries. In this case, our Platform Repo is also our workspace root . If you choose to have a different repository layout, it will be important to note what your workspace root is, as it should still be the base folder of your code tree. Submodules are full git repos on their own. What we do with these repos is add them as sub-repos to the workspace root . Git will create a .gitmodules file that contains links to the repo and default branches. There are git submodule commands that you can use to work with your submodules, such as: git submodule add <url> <path> # url to submodule, path to submodule installation git submodule update --init --recursive # Recursively initializes and updates all submodules. git submodule foreach git status # git submodule foreach can be used to run a command in each submodule. git status is just an example. For more information available here . 1) Create Git Repo \u00b6 Make new directory. mkdir NewPlatformRepo cd NewPlatformRepo git init This will serve as our Platform Repository as well as our Workspace Root. For more information on creating a Git repo, here are command line instructions and here are web instructions . 2) Add pertinent submodules \u00b6 Project MU is separated into submodules. For each submodule that you need for your project, run the \"git submodule add\" command to add it to your base Repository. The path after the URL is the path we typically use to group the submodules. You can change it if you'd like, just remember your environment will diverge from the one in these instructions. MU_BASECORE \u00b6 This is the core section of TianoCore. Contains the guts of UEFI, forked from TianoCore, as well as the BaseTools needed to build. You will need this to continue. git submodule add https://github.com/Microsoft/mu_basecore.git MU_BASECORE MU_PLUS \u00b6 Additional, optional libraries and tools we've added to make MU great! git submodule add https://github.com/Microsoft/mu_plus.git Common/MU MU_TIANO_PLUS \u00b6 Additional, optional libraries and tools forked from TianoCore. git submodule add https://github.com/Microsoft/mu_tiano_plus.git Common/TIANO MU_OEM_SAMPLE \u00b6 This module is a sample implementation of a FrontPage and several BDS support libraries. This module is intended to be forked and customized. git submodule add https://github.com/Microsoft/mu_oem_sample.git Common/MU_OEM_SAMPLE MU_SILICON_ARM_TIANO \u00b6 Silicon code from TianoCore has been broken out into individual submodules. This is the ARM specific submodule. git submodule add https://github.com/Microsoft/mu_silicon_arm_tiano.git Silicon/ARM/TIANO MU_SILICON_INTEL_TIANO \u00b6 Silicon code from TianoCore has been broken out into individual submodules. This is the Intel specific submodule. git submodule add https://github.com/Microsoft/mu_silicon_intel_tiano.git Silicon/INTEL/TIANO You can run git submodule --update --init to make sure all the submodules are set up. 3) Adding your platform contents \u00b6 New_Platform_Repo/ \u251c\u2500\u2500 Common/ \u2502 \u2514\u2500\u2500 ... # MU_PLUS, MU_OEM_SAMPLE, MU_TIANO_PLUS are generally created by the \"git submodule ...\" commands shown above \u251c\u2500\u2500 MU_BASECORE/ \u251c\u2500\u2500 PlatformGroup/ \u2502 \u2514\u2500\u2500 PlatformName/ \u2502 \u2514\u2500\u2500 PlatformBuild.py # Python script to provide information to the build process. \u2502 \u2514\u2500\u2500 Platform.dsc # List of UEFI libraries and drivers to compile, as well as platform settings. \u2502 \u2514\u2500\u2500 Platform.fdf # List of UEFI Drivers to put into Firmware Volumes. \u251c\u2500\u2500 Silicon/ \u2502 \u2514\u2500\u2500 SiProvider/ # You may want to create a separate git repo for Silicon code to enable development with partners. \u2502 \u2514\u2500\u2500 REF_CODE/ # Enablement code for your architecture \u251c\u2500\u2500 .gitattributes \u251c\u2500\u2500 .gitignore \u2514\u2500\u2500 .gitmodules You will need to create PlatformBuild.py, Platform.dsc, and Platform.fdf. These files will go inside the platform folder, which will be New_Platform_Repo/PlatformGroup/PlatformName . The ms-iot iMX8 repo can help you get started as a layout reference and can demonstrate the PlatformBuild file. More information about PlatformBuild can be found here . 4) Build instructions \u00b6 There are documents explaining our build system. This will be updated as things change.","title":"Using"},{"location":"How/using_project_mu/#how-to-setup-a-new-repo-for-a-platform-that-will-use-project-mu","text":"This document will describe the base guidelines for setting up a Project MU repo. You will need: 1) Git 2) Python 3.7 3) A text editor 4) Look at layout to understand our recommended repository layout. You can also look at ms-iot iMX8 for a real platform implementation.","title":"How to setup a new Repo for a Platform that will use Project MU?"},{"location":"How/using_project_mu/#0-nomenclature","text":"I will use the term workspace root to reference the base folder for your code tree. Ordinarily, we use the Platform Repository as the outer-most layer. This means that the outermost git repository is where we store Platform specific files and libraries. In this case, our Platform Repo is also our workspace root . If you choose to have a different repository layout, it will be important to note what your workspace root is, as it should still be the base folder of your code tree. Submodules are full git repos on their own. What we do with these repos is add them as sub-repos to the workspace root . Git will create a .gitmodules file that contains links to the repo and default branches. There are git submodule commands that you can use to work with your submodules, such as: git submodule add <url> <path> # url to submodule, path to submodule installation git submodule update --init --recursive # Recursively initializes and updates all submodules. git submodule foreach git status # git submodule foreach can be used to run a command in each submodule. git status is just an example. For more information available here .","title":"0) Nomenclature"},{"location":"How/using_project_mu/#1-create-git-repo","text":"Make new directory. mkdir NewPlatformRepo cd NewPlatformRepo git init This will serve as our Platform Repository as well as our Workspace Root. For more information on creating a Git repo, here are command line instructions and here are web instructions .","title":"1) Create Git Repo"},{"location":"How/using_project_mu/#2-add-pertinent-submodules","text":"Project MU is separated into submodules. For each submodule that you need for your project, run the \"git submodule add\" command to add it to your base Repository. The path after the URL is the path we typically use to group the submodules. You can change it if you'd like, just remember your environment will diverge from the one in these instructions.","title":"2) Add pertinent submodules"},{"location":"How/using_project_mu/#mu_basecore","text":"This is the core section of TianoCore. Contains the guts of UEFI, forked from TianoCore, as well as the BaseTools needed to build. You will need this to continue. git submodule add https://github.com/Microsoft/mu_basecore.git MU_BASECORE","title":"MU_BASECORE"},{"location":"How/using_project_mu/#mu_plus","text":"Additional, optional libraries and tools we've added to make MU great! git submodule add https://github.com/Microsoft/mu_plus.git Common/MU","title":"MU_PLUS"},{"location":"How/using_project_mu/#mu_tiano_plus","text":"Additional, optional libraries and tools forked from TianoCore. git submodule add https://github.com/Microsoft/mu_tiano_plus.git Common/TIANO","title":"MU_TIANO_PLUS"},{"location":"How/using_project_mu/#mu_oem_sample","text":"This module is a sample implementation of a FrontPage and several BDS support libraries. This module is intended to be forked and customized. git submodule add https://github.com/Microsoft/mu_oem_sample.git Common/MU_OEM_SAMPLE","title":"MU_OEM_SAMPLE"},{"location":"How/using_project_mu/#mu_silicon_arm_tiano","text":"Silicon code from TianoCore has been broken out into individual submodules. This is the ARM specific submodule. git submodule add https://github.com/Microsoft/mu_silicon_arm_tiano.git Silicon/ARM/TIANO","title":"MU_SILICON_ARM_TIANO"},{"location":"How/using_project_mu/#mu_silicon_intel_tiano","text":"Silicon code from TianoCore has been broken out into individual submodules. This is the Intel specific submodule. git submodule add https://github.com/Microsoft/mu_silicon_intel_tiano.git Silicon/INTEL/TIANO You can run git submodule --update --init to make sure all the submodules are set up.","title":"MU_SILICON_INTEL_TIANO"},{"location":"How/using_project_mu/#3-adding-your-platform-contents","text":"New_Platform_Repo/ \u251c\u2500\u2500 Common/ \u2502 \u2514\u2500\u2500 ... # MU_PLUS, MU_OEM_SAMPLE, MU_TIANO_PLUS are generally created by the \"git submodule ...\" commands shown above \u251c\u2500\u2500 MU_BASECORE/ \u251c\u2500\u2500 PlatformGroup/ \u2502 \u2514\u2500\u2500 PlatformName/ \u2502 \u2514\u2500\u2500 PlatformBuild.py # Python script to provide information to the build process. \u2502 \u2514\u2500\u2500 Platform.dsc # List of UEFI libraries and drivers to compile, as well as platform settings. \u2502 \u2514\u2500\u2500 Platform.fdf # List of UEFI Drivers to put into Firmware Volumes. \u251c\u2500\u2500 Silicon/ \u2502 \u2514\u2500\u2500 SiProvider/ # You may want to create a separate git repo for Silicon code to enable development with partners. \u2502 \u2514\u2500\u2500 REF_CODE/ # Enablement code for your architecture \u251c\u2500\u2500 .gitattributes \u251c\u2500\u2500 .gitignore \u2514\u2500\u2500 .gitmodules You will need to create PlatformBuild.py, Platform.dsc, and Platform.fdf. These files will go inside the platform folder, which will be New_Platform_Repo/PlatformGroup/PlatformName . The ms-iot iMX8 repo can help you get started as a layout reference and can demonstrate the PlatformBuild file. More information about PlatformBuild can be found here .","title":"3) Adding your platform contents"},{"location":"How/using_project_mu/#4-build-instructions","text":"There are documents explaining our build system. This will be updated as things change.","title":"4) Build instructions"},{"location":"WhatAndWhy/features/","text":"Features \u00b6 Summary \u00b6 Project Mu features will generally be found in a \"MU\" sub-module, for example, \"Common/MU\" or \"Silicon/Intel/MU\". What major features does Project Mu bring to the table above/beyond EDK2? Feature List \u00b6 Pluggable, cross-device, performance-optimized BDS Device Firmware Configuration Interface (DFCI) - enables practical MDM management PBKDF2-based BIOS password example Support for EKU-based trust anchors during signature validation Microsoft unit test framework Audit, function, & performance tests for platform features Scalable Python build environment Build plug in: override tracking tool Build plug in: flash descriptor analysis Binary package management via NuGet Capsule signing via signtool.exe Up-to-date Visual Studio compiler support Base64 encode for binary objects XML Support Package Features Coming Soon \u00b6 Modern BIOS menu example (Surface inspired) On screen keyboard (OSK) with mouse, touch support Graphical end-to-end boot performance analysis library and tool Infineon TPM firmware update via Capsule On screen notifications: color bars to inform users that a device is not in a production configuration Features integrated into Tiano \u00b6 Safe Integer library Heap Guard ESRT DXE driver Scalable device FMP framework Progress bar for Capsule Updates TCG FV pre hashing optimization NVME shutdown","title":"Features"},{"location":"WhatAndWhy/features/#features","text":"","title":"Features"},{"location":"WhatAndWhy/features/#summary","text":"Project Mu features will generally be found in a \"MU\" sub-module, for example, \"Common/MU\" or \"Silicon/Intel/MU\". What major features does Project Mu bring to the table above/beyond EDK2?","title":"Summary"},{"location":"WhatAndWhy/features/#feature-list","text":"Pluggable, cross-device, performance-optimized BDS Device Firmware Configuration Interface (DFCI) - enables practical MDM management PBKDF2-based BIOS password example Support for EKU-based trust anchors during signature validation Microsoft unit test framework Audit, function, & performance tests for platform features Scalable Python build environment Build plug in: override tracking tool Build plug in: flash descriptor analysis Binary package management via NuGet Capsule signing via signtool.exe Up-to-date Visual Studio compiler support Base64 encode for binary objects XML Support Package","title":"Feature List"},{"location":"WhatAndWhy/features/#features-coming-soon","text":"Modern BIOS menu example (Surface inspired) On screen keyboard (OSK) with mouse, touch support Graphical end-to-end boot performance analysis library and tool Infineon TPM firmware update via Capsule On screen notifications: color bars to inform users that a device is not in a production configuration","title":"Features Coming Soon"},{"location":"WhatAndWhy/features/#features-integrated-into-tiano","text":"Safe Integer library Heap Guard ESRT DXE driver Scalable device FMP framework Progress bar for Capsule Updates TCG FV pre hashing optimization NVME shutdown","title":"Features integrated into Tiano"},{"location":"WhatAndWhy/layout/","text":"Dependencies and Layout \u00b6 Conceptual Layers \u00b6 A modern, full-featured, product-ready UEFI firmware codebase combines code from a multitude of sources: TianoCore EDK2 UEFI standard-based code Value-add code from TianoCore Silicon vendor hardware initialization code Silicon vendor value-add code Independent BIOS Vendor code ODM/OEM customization code OS firmware support code Legacy BIOS compatibility code Board-specific code etc. Some of the above components come from closed-source projects (silicon vendors, IBVs, OEMs), others are open source. Each component is supported at its own schedule with new features and bug fixes, creating a problem of stale code if not synced up regularly. Compound the version and source problem with the sheer size: a common UEFI codebase is typically well above 1 million LOC and only goes up from there. What is a dependency \u00b6 To understand the layering you must first understand the terminology. There are two types of code assets. A definition of something. Generally, this is defined in an accessible header file. This is the API provided by some asset. This API can be \"depended\" upon to provide some capability. An implementation of something. Example of a dependency: DxeCore in the Basecore layer includes a TimerLib interface. TimerLib interface is defined in the same Basecore layer as DxeCore, so in this case a Basecore module is depending on a Basecore interface. This is allowed. Another example: Silicon-layer module implements a TimerLib interface defined in Basecore. Here, a Silicon layer module depends on a Basecore interface. This is allowed. Architecture \u00b6 Project Mu is an attempt to create a rigid layering scheme that defines the hierarchy of dependencies. Architectural goal kept in mind when designing this layering scheme is a controlled, limited scope, and allowed dependencies for each module within a given layer. It is important to know, when implementing a module, what the module is allowed to depend on. When creating an interface, it is important to identify the correct layer for it such that all the consuming modules are located in the layers below. Motivation and goals of the layering scheme: Easy component integration Code reuse Only carry relevant code Dependency Block Diagram \u00b6 File Layout \u00b6 To best preserve and delineate these concepts of componentization and unidirectional dependency, we have chosen to lay out our repository files in a structure that reinforces the same mentality. The underlying logic of this layout is to clearly distinguish each layer from the rest. As such, the Basecore -- which is considered foundational -- is broken out on its own, followed by the Common repos, followed by the Silicon, followed by the Platform. As mentioned elsewhere, Project Mu makes liberal use of multiple repositories due to the mixture of requirements in the firmware ecosystem. Some repos are split for technical reasons, some for organizational, and some for legal. One of the goals of Project Mu is to make this seemingly complicated layout easier to work with. Min Platform Example \u00b6 A simple tree might look like this... project_mu/ \u251c\u2500\u2500 Build/ \u251c\u2500\u2500 Common/ \u2502 \u2514\u2500\u2500 ... # Common code optional, but probably not required \u251c\u2500\u2500 Conf/ \u251c\u2500\u2500 MU_BASECORE/ \u251c\u2500\u2500 Platform/ \u2502 \u2514\u2500\u2500 Sample/ \u2502 \u2514\u2500\u2500 MyMinPlatform # Platform-specific build files and code \u251c\u2500\u2500 Silicon/ \u2502 \u2514\u2500\u2500 SiProvider/ \u2502 \u2514\u2500\u2500 REF_CODE/ # Enablement code for your architecture \u251c\u2500\u2500 .gitattributes \u251c\u2500\u2500 .gitignore \u2514\u2500\u2500 .gitmodules Note that this file structure is likely located in a Git repository, and every \"ALL CAPS\" directory in this example is a Git submodule/nested repository. Surface Laptop Example \u00b6 For a real-world example, this is a tree that could build the Surface Laptop product, including both open- and closed-source repositories: project_mu/ \u251c\u2500\u2500 Build/ \u251c\u2500\u2500 Common/ \u2502 \u251c\u2500\u2500 MSCORE_INTERNAL/ # Proprietary code and code not yet approved for public distribution \u2502 \u251c\u2500\u2500 MU/ \u2502 \u251c\u2500\u2500 MU_TIANO/ \u2502 \u2514\u2500\u2500 SURFACE/ # Shared code to enable common features like FrontPage \u251c\u2500\u2500 Conf/ \u251c\u2500\u2500 MU_BASECORE/ \u251c\u2500\u2500 Platform/ \u2502 \u251c\u2500\u2500 Surface/ \u2502 \u2502 \u251c\u2500\u2500 SurfKbl/ \u2502 \u2502 \u2502 \u2514\u2500\u2500 Laptop/ # Surface Laptop-Specific Platform Code \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u2514\u2500\u2500 Others/ \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 Silicon/ \u2502 \u251c\u2500\u2500 Intel/ \u2502 \u2502 \u251c\u2500\u2500 KBL/ # Intel KBL Reference Code \u2502 \u2502 \u251c\u2500\u2500 MU/ # Project Mu Intel Common Code \u2502 \u2502 \u251c\u2500\u2500 MU_TIANO/ # Project Mu Intel Code from TianoCore \u2502 \u2502 \u2514\u2500\u2500 SURF_KBL/ # Surface Customizations/Overrides for KBL Ref Code \u2502 \u2514\u2500\u2500 SURFACE/ # Shared code to enable common HW like ECs \u251c\u2500\u2500 .gitattributes \u251c\u2500\u2500 .gitignore \u2514\u2500\u2500 .gitmodules Once again, the \"ALL CAPS\" directories are submodules.","title":"Dependencies and Layout"},{"location":"WhatAndWhy/layout/#dependencies-and-layout","text":"","title":"Dependencies and Layout"},{"location":"WhatAndWhy/layout/#conceptual-layers","text":"A modern, full-featured, product-ready UEFI firmware codebase combines code from a multitude of sources: TianoCore EDK2 UEFI standard-based code Value-add code from TianoCore Silicon vendor hardware initialization code Silicon vendor value-add code Independent BIOS Vendor code ODM/OEM customization code OS firmware support code Legacy BIOS compatibility code Board-specific code etc. Some of the above components come from closed-source projects (silicon vendors, IBVs, OEMs), others are open source. Each component is supported at its own schedule with new features and bug fixes, creating a problem of stale code if not synced up regularly. Compound the version and source problem with the sheer size: a common UEFI codebase is typically well above 1 million LOC and only goes up from there.","title":"Conceptual Layers"},{"location":"WhatAndWhy/layout/#what-is-a-dependency","text":"To understand the layering you must first understand the terminology. There are two types of code assets. A definition of something. Generally, this is defined in an accessible header file. This is the API provided by some asset. This API can be \"depended\" upon to provide some capability. An implementation of something. Example of a dependency: DxeCore in the Basecore layer includes a TimerLib interface. TimerLib interface is defined in the same Basecore layer as DxeCore, so in this case a Basecore module is depending on a Basecore interface. This is allowed. Another example: Silicon-layer module implements a TimerLib interface defined in Basecore. Here, a Silicon layer module depends on a Basecore interface. This is allowed.","title":"What is a dependency"},{"location":"WhatAndWhy/layout/#architecture","text":"Project Mu is an attempt to create a rigid layering scheme that defines the hierarchy of dependencies. Architectural goal kept in mind when designing this layering scheme is a controlled, limited scope, and allowed dependencies for each module within a given layer. It is important to know, when implementing a module, what the module is allowed to depend on. When creating an interface, it is important to identify the correct layer for it such that all the consuming modules are located in the layers below. Motivation and goals of the layering scheme: Easy component integration Code reuse Only carry relevant code","title":"Architecture"},{"location":"WhatAndWhy/layout/#dependency-block-diagram","text":"","title":"Dependency Block Diagram"},{"location":"WhatAndWhy/layout/#file-layout","text":"To best preserve and delineate these concepts of componentization and unidirectional dependency, we have chosen to lay out our repository files in a structure that reinforces the same mentality. The underlying logic of this layout is to clearly distinguish each layer from the rest. As such, the Basecore -- which is considered foundational -- is broken out on its own, followed by the Common repos, followed by the Silicon, followed by the Platform. As mentioned elsewhere, Project Mu makes liberal use of multiple repositories due to the mixture of requirements in the firmware ecosystem. Some repos are split for technical reasons, some for organizational, and some for legal. One of the goals of Project Mu is to make this seemingly complicated layout easier to work with.","title":"File Layout"},{"location":"WhatAndWhy/layout/#min-platform-example","text":"A simple tree might look like this... project_mu/ \u251c\u2500\u2500 Build/ \u251c\u2500\u2500 Common/ \u2502 \u2514\u2500\u2500 ... # Common code optional, but probably not required \u251c\u2500\u2500 Conf/ \u251c\u2500\u2500 MU_BASECORE/ \u251c\u2500\u2500 Platform/ \u2502 \u2514\u2500\u2500 Sample/ \u2502 \u2514\u2500\u2500 MyMinPlatform # Platform-specific build files and code \u251c\u2500\u2500 Silicon/ \u2502 \u2514\u2500\u2500 SiProvider/ \u2502 \u2514\u2500\u2500 REF_CODE/ # Enablement code for your architecture \u251c\u2500\u2500 .gitattributes \u251c\u2500\u2500 .gitignore \u2514\u2500\u2500 .gitmodules Note that this file structure is likely located in a Git repository, and every \"ALL CAPS\" directory in this example is a Git submodule/nested repository.","title":"Min Platform Example"},{"location":"WhatAndWhy/layout/#surface-laptop-example","text":"For a real-world example, this is a tree that could build the Surface Laptop product, including both open- and closed-source repositories: project_mu/ \u251c\u2500\u2500 Build/ \u251c\u2500\u2500 Common/ \u2502 \u251c\u2500\u2500 MSCORE_INTERNAL/ # Proprietary code and code not yet approved for public distribution \u2502 \u251c\u2500\u2500 MU/ \u2502 \u251c\u2500\u2500 MU_TIANO/ \u2502 \u2514\u2500\u2500 SURFACE/ # Shared code to enable common features like FrontPage \u251c\u2500\u2500 Conf/ \u251c\u2500\u2500 MU_BASECORE/ \u251c\u2500\u2500 Platform/ \u2502 \u251c\u2500\u2500 Surface/ \u2502 \u2502 \u251c\u2500\u2500 SurfKbl/ \u2502 \u2502 \u2502 \u2514\u2500\u2500 Laptop/ # Surface Laptop-Specific Platform Code \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u2514\u2500\u2500 Others/ \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 Silicon/ \u2502 \u251c\u2500\u2500 Intel/ \u2502 \u2502 \u251c\u2500\u2500 KBL/ # Intel KBL Reference Code \u2502 \u2502 \u251c\u2500\u2500 MU/ # Project Mu Intel Common Code \u2502 \u2502 \u251c\u2500\u2500 MU_TIANO/ # Project Mu Intel Code from TianoCore \u2502 \u2502 \u2514\u2500\u2500 SURF_KBL/ # Surface Customizations/Overrides for KBL Ref Code \u2502 \u2514\u2500\u2500 SURFACE/ # Shared code to enable common HW like ECs \u251c\u2500\u2500 .gitattributes \u251c\u2500\u2500 .gitignore \u2514\u2500\u2500 .gitmodules Once again, the \"ALL CAPS\" directories are submodules.","title":"Surface Laptop Example"},{"location":"WhatAndWhy/overview/","text":"Overview \u00b6 Project Organization \u00b6 This documentation is hosted in the main repository for Project Mu, which is used as a central collection point for community interaction and documentation. The build system and firmware code for the project is hosted in a number of other repositories, grouped/divided by function, partner, license, and dependencies. Several of these repositories are brought together by the build system to create a FW project, but we'll get into those details later. ;) For now, an overview of the repositories and what code you'll find there... Mu Basecore \u00b6 This repository is considered foundational and fundamental to Project Mu. The guiding philosophy is that this code should be one or more of the following: Part of the build system Common to any silicon architecture Part of the \"API layer\" that contains protocol and library definitions including Industry Standards UEFI Specifications ACPI Specifications Part of the \"PI\" layer that contains driver dispatch logic, event/signaling logic, or memory management logic This can also include central technologies like variable services Mu Common Plus \u00b6 The packages found in this repository are contributed entirely by Project Mu. They should be common to all silicon architectures and only depend on Mu Basecore. These packages provide features and functionality that are entirely optional, but may be recommended for PC platform FW. Mu Tiano Plus \u00b6 This repository contains only modules that were originally sourced from TianoCore. They are not essential for any particular platform, but are likely useful to many platforms. The versions contained in this repo are modified and/or improved to work with the rest of Project Mu. Repo Philosophy \u00b6 Project Mu makes liberal use of multiple repositories due to the mixture of requirements in the UEFI ecosystem. Some repos are split for technical reasons, some for organizational, and some for legal. Examples of this are: A downstream contributor wants to add a generic feature with a silicon-specific implementation. This feature would be leveraged by Common code. If all code were in one repository, no barriers would be in place to prevent the contributor from directly calling from Common code into the Silicon implementation. By forcing the API/interface to be published in a separate repository, we can ensure that the unidirectional dependency relationship is maintained. Module A and Module B both provide optional functionality. However, Module A is far more likely to be consume by a wide audience than Module B. To achieve \"Less is More\", Module A may be placed in a different repos to enable downstream consumers to carry as little \"unused\" code as possible, since it's likely they would not need Module B in their code tree. A downstream consumer is producing a product in conjunction with a vendor/partner. While most of the enabling code for the vendor component is open-source, a portion of it is only released under NDA. By having multiple repositories comprise a single workspace, the downstream consumer is able to maximize their open-source consumption (which minimizes forking) while maintaining the legal requirements of closed-source/proprietary partitioning.","title":"Overview"},{"location":"WhatAndWhy/overview/#overview","text":"","title":"Overview"},{"location":"WhatAndWhy/overview/#project-organization","text":"This documentation is hosted in the main repository for Project Mu, which is used as a central collection point for community interaction and documentation. The build system and firmware code for the project is hosted in a number of other repositories, grouped/divided by function, partner, license, and dependencies. Several of these repositories are brought together by the build system to create a FW project, but we'll get into those details later. ;) For now, an overview of the repositories and what code you'll find there...","title":"Project Organization"},{"location":"WhatAndWhy/overview/#mu-basecore","text":"This repository is considered foundational and fundamental to Project Mu. The guiding philosophy is that this code should be one or more of the following: Part of the build system Common to any silicon architecture Part of the \"API layer\" that contains protocol and library definitions including Industry Standards UEFI Specifications ACPI Specifications Part of the \"PI\" layer that contains driver dispatch logic, event/signaling logic, or memory management logic This can also include central technologies like variable services","title":"Mu Basecore"},{"location":"WhatAndWhy/overview/#mu-common-plus","text":"The packages found in this repository are contributed entirely by Project Mu. They should be common to all silicon architectures and only depend on Mu Basecore. These packages provide features and functionality that are entirely optional, but may be recommended for PC platform FW.","title":"Mu Common Plus"},{"location":"WhatAndWhy/overview/#mu-tiano-plus","text":"This repository contains only modules that were originally sourced from TianoCore. They are not essential for any particular platform, but are likely useful to many platforms. The versions contained in this repo are modified and/or improved to work with the rest of Project Mu.","title":"Mu Tiano Plus"},{"location":"WhatAndWhy/overview/#repo-philosophy","text":"Project Mu makes liberal use of multiple repositories due to the mixture of requirements in the UEFI ecosystem. Some repos are split for technical reasons, some for organizational, and some for legal. Examples of this are: A downstream contributor wants to add a generic feature with a silicon-specific implementation. This feature would be leveraged by Common code. If all code were in one repository, no barriers would be in place to prevent the contributor from directly calling from Common code into the Silicon implementation. By forcing the API/interface to be published in a separate repository, we can ensure that the unidirectional dependency relationship is maintained. Module A and Module B both provide optional functionality. However, Module A is far more likely to be consume by a wide audience than Module B. To achieve \"Less is More\", Module A may be placed in a different repos to enable downstream consumers to carry as little \"unused\" code as possible, since it's likely they would not need Module B in their code tree. A downstream consumer is producing a product in conjunction with a vendor/partner. While most of the enabling code for the vendor component is open-source, a portion of it is only released under NDA. By having multiple repositories comprise a single workspace, the downstream consumer is able to maximize their open-source consumption (which minimizes forking) while maintaining the legal requirements of closed-source/proprietary partitioning.","title":"Repo Philosophy"},{"location":"Where/external_resources/","text":"External Resources \u00b6 UEFI Industry Organization \u00b6 UEFI is the industry standards body that develops and distributes the UEFI, PI, and ACPI specifications. These specifications govern the firmware interfaces between OS, OEM/Device Manufacturer, and Silicon partner. This is a great site to download the industry specifications and if you are a member you can join working groups for future specifications. TianoCore Project \u00b6 Tianocore is an existing open source project. Their EDK2 repository is the basis for many/most UEFI implementations used on products today. It provides UEFI spec compliant code modules, supports industry standard hardware, and a multi-platform build environment. This is a great site to download specifications for the different file types and build process. It also has links to repositories that Project Mu tracks as \"upstreams\". MkDocs \u00b6 Great tool for creating documentation websites based on markdown. In fact it was used to generate this documentation. Markdown Help \u00b6 Quick link for common markdown support.","title":"External Resources"},{"location":"Where/external_resources/#external-resources","text":"","title":"External Resources"},{"location":"Where/external_resources/#uefi-industry-organization","text":"UEFI is the industry standards body that develops and distributes the UEFI, PI, and ACPI specifications. These specifications govern the firmware interfaces between OS, OEM/Device Manufacturer, and Silicon partner. This is a great site to download the industry specifications and if you are a member you can join working groups for future specifications.","title":"UEFI Industry Organization"},{"location":"Where/external_resources/#tianocore-project","text":"Tianocore is an existing open source project. Their EDK2 repository is the basis for many/most UEFI implementations used on products today. It provides UEFI spec compliant code modules, supports industry standard hardware, and a multi-platform build environment. This is a great site to download specifications for the different file types and build process. It also has links to repositories that Project Mu tracks as \"upstreams\".","title":"TianoCore Project"},{"location":"Where/external_resources/#mkdocs","text":"Great tool for creating documentation websites based on markdown. In fact it was used to generate this documentation.","title":"MkDocs"},{"location":"Where/external_resources/#markdown-help","text":"Quick link for common markdown support.","title":"Markdown Help"},{"location":"Where/project_resources/","text":"Project Resources \u00b6 Public Source Code Repositories \u00b6 Listed here: GitHub Project Mu Repo List Issue/Bug/Feature Tracking \u00b6 https://github.com/Microsoft/mu/issues Builds \u00b6 https://dev.azure.com/projectmu/mu/_build Docs \u00b6 https://microsoft.github.io/mu/ Collaborate \u00b6 Send an email request to join the discussion on our Teams channels. Help \u00b6 For one-off questions, feel free to open an Issue against the Mu repo with the \"question\" tag https://github.com/Microsoft/mu/issues For deeper discussion & faster communication, join our Microsoft Teams channels. To join send an email request .","title":"Project Resources"},{"location":"Where/project_resources/#project-resources","text":"","title":"Project Resources"},{"location":"Where/project_resources/#public-source-code-repositories","text":"Listed here: GitHub Project Mu Repo List","title":"Public Source Code Repositories"},{"location":"Where/project_resources/#issuebugfeature-tracking","text":"https://github.com/Microsoft/mu/issues","title":"Issue/Bug/Feature Tracking"},{"location":"Where/project_resources/#builds","text":"https://dev.azure.com/projectmu/mu/_build","title":"Builds"},{"location":"Where/project_resources/#docs","text":"https://microsoft.github.io/mu/","title":"Docs"},{"location":"Where/project_resources/#collaborate","text":"Send an email request to join the discussion on our Teams channels.","title":"Collaborate"},{"location":"Where/project_resources/#help","text":"For one-off questions, feel free to open an Issue against the Mu repo with the \"question\" tag https://github.com/Microsoft/mu/issues For deeper discussion & faster communication, join our Microsoft Teams channels. To join send an email request .","title":"Help"},{"location":"dyn/mu_basecore/RepoDetails/","text":"Project Mu Basecore Repository \u00b6 Git Details Repository Url: https://github.com/Microsoft/mu_basecore.git Branch: release/201911 Commit: 5bb53ef5f2575785965a9553c5ef05adc9969e56 Commit Date: 2019-12-07 19:09:18 +0000 This repository is considered foundational and fundamental to Project Mu. The guiding philosophy is that this any code within this repository should be one or more of the following Part of the build system Common to any silicon architecture Part of the \"API layer\" that contains protocol and library definitions including Industry Standards UEFI Specifications ACPI Specifications Part of the \"PI\" layer that contains driver dispatch logic, event/signaling logic, or memory management logic This can also include central technologies like variable services More Info \u00b6 Please see the Project Mu docs ( https://github.com/Microsoft/mu ) for more information. This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments. Issues \u00b6 Please open any issues in the Project Mu GitHub tracker. More Details Contributing Code or Docs \u00b6 Please follow the general Project Mu Pull Request process. More Details Code Requirements Doc Requirements Builds \u00b6 pip install mu_build mu_build -c corebuild.mu.json More info Copyright & License \u00b6 Copyright (C) Microsoft Corporation SPDX-License-Identifier: BSD-2-Clause-Patent Upstream License (TianoCore) \u00b6 Copyright \u00a9 2019, TianoCore and contributors. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Subject to the terms and conditions of this license, each copyright holder and contributor hereby grants to those receiving rights under this license a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except for failure to satisfy the conditions of this license) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer this software, where such license applies only to those patent claims, already acquired or hereafter acquired, licensable by such copyright holder or contributor that are necessarily infringed by: (a) their Contribution(s) (the licensed copyrights of copyright holders and non-copyrightable additions of contributors, in source or binary form) alone; or (b) combination of their Contribution(s) with the work of authorship to which such Contribution(s) was added by such copyright holder or contributor, if, at the time the Contribution is added, such addition causes such combination to be necessarily infringed. The patent license shall not apply to any other combinations which include the Contribution. Except as expressly stated above, no rights or licenses from any copyright holder or contributor is granted under this license, whether expressly, by implication, estoppel or otherwise. DISCLAIMER THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"Repo Details"},{"location":"dyn/mu_basecore/RepoDetails/#project-mu-basecore-repository","text":"Git Details Repository Url: https://github.com/Microsoft/mu_basecore.git Branch: release/201911 Commit: 5bb53ef5f2575785965a9553c5ef05adc9969e56 Commit Date: 2019-12-07 19:09:18 +0000 This repository is considered foundational and fundamental to Project Mu. The guiding philosophy is that this any code within this repository should be one or more of the following Part of the build system Common to any silicon architecture Part of the \"API layer\" that contains protocol and library definitions including Industry Standards UEFI Specifications ACPI Specifications Part of the \"PI\" layer that contains driver dispatch logic, event/signaling logic, or memory management logic This can also include central technologies like variable services","title":"Project Mu Basecore Repository"},{"location":"dyn/mu_basecore/RepoDetails/#more-info","text":"Please see the Project Mu docs ( https://github.com/Microsoft/mu ) for more information. This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.","title":"More Info"},{"location":"dyn/mu_basecore/RepoDetails/#issues","text":"Please open any issues in the Project Mu GitHub tracker. More Details","title":"Issues"},{"location":"dyn/mu_basecore/RepoDetails/#contributing-code-or-docs","text":"Please follow the general Project Mu Pull Request process. More Details Code Requirements Doc Requirements","title":"Contributing Code or Docs"},{"location":"dyn/mu_basecore/RepoDetails/#builds","text":"pip install mu_build mu_build -c corebuild.mu.json More info","title":"Builds"},{"location":"dyn/mu_basecore/RepoDetails/#copyright-license","text":"Copyright (C) Microsoft Corporation SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright &amp; License"},{"location":"dyn/mu_basecore/RepoDetails/#upstream-license-tianocore","text":"Copyright \u00a9 2019, TianoCore and contributors. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Subject to the terms and conditions of this license, each copyright holder and contributor hereby grants to those receiving rights under this license a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except for failure to satisfy the conditions of this license) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer this software, where such license applies only to those patent claims, already acquired or hereafter acquired, licensable by such copyright holder or contributor that are necessarily infringed by: (a) their Contribution(s) (the licensed copyrights of copyright holders and non-copyrightable additions of contributors, in source or binary form) alone; or (b) combination of their Contribution(s) with the work of authorship to which such Contribution(s) was added by such copyright holder or contributor, if, at the time the Contribution is added, such addition causes such combination to be necessarily infringed. The patent license shall not apply to any other combinations which include the Contribution. Except as expressly stated above, no rights or licenses from any copyright holder or contributor is granted under this license, whether expressly, by implication, estoppel or otherwise. DISCLAIMER THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"Upstream License (TianoCore)"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/Mu-Basetools/","text":"Mu BaseTools Notes \u00b6 This is a set of compiled tools for Edk2 development on x86 for Windows and Linux. This set has both the standard Edk2 tools as well as additional tools created for Project Mu. Where \u00b6 Information about the TianoCore Edk2 Basetools can be found here: * https://tianocore.org * https://github.com/tianocore/edk2 * https://github.com/tianocore/edk2-BaseTools-win32 Information about Project Mu can be found here: * https://microsoft.github.io/mu/ * https://github.com/Microsoft/mu * https://github.com/microsoft/mu_basecore What \u00b6 TianoCore/Project Mu Edk2 Build tools Version \u00b6 BaseTools binaries are versioned based on the Release branch they are associated with (e.g. release/201808, release/201811, etc.). The version format is YYYY.MM.XX where: YYYY is the 4-digit year MM is the 2-digit month XX is a point-release in case fixes are required Nuget version is AA.BB.CC If the version is a single number then make it the AA field and use zeros for BB.CC Example: version command is 20160912 then NuGet version is 20160912.0.0 If a version has two numbers partitioned by a \"-\" then make those the AA.BB fields and use zero for the CC Example: version command is 1234-56 then NuGet version is 1234.56.0 Process to publish new version of tool \u00b6 Download desired version from Unzip Make a new folder (for my example I will call it \"new\") Make proper subfolders for each host. (Details in NugetPublishing/ReadMe.md) Copy the assets to publish into this new folder Run the < TOOL > -v command to see the version. Open cmd prompt in the NugetPublishing dir Pack and push NugetPublishing.py --Operation PackAndPush --ConfigFilePath Mu-Basetools.config.json --Version <nuget version here> --InputFolderPath <path to newly created folder here> --ApiKey <your key here>","title":"Mu-Basetools"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/Mu-Basetools/#mu-basetools-notes","text":"This is a set of compiled tools for Edk2 development on x86 for Windows and Linux. This set has both the standard Edk2 tools as well as additional tools created for Project Mu.","title":"Mu BaseTools Notes"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/Mu-Basetools/#where","text":"Information about the TianoCore Edk2 Basetools can be found here: * https://tianocore.org * https://github.com/tianocore/edk2 * https://github.com/tianocore/edk2-BaseTools-win32 Information about Project Mu can be found here: * https://microsoft.github.io/mu/ * https://github.com/Microsoft/mu * https://github.com/microsoft/mu_basecore","title":"Where"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/Mu-Basetools/#what","text":"TianoCore/Project Mu Edk2 Build tools","title":"What"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/Mu-Basetools/#version","text":"BaseTools binaries are versioned based on the Release branch they are associated with (e.g. release/201808, release/201811, etc.). The version format is YYYY.MM.XX where: YYYY is the 4-digit year MM is the 2-digit month XX is a point-release in case fixes are required Nuget version is AA.BB.CC If the version is a single number then make it the AA field and use zeros for BB.CC Example: version command is 20160912 then NuGet version is 20160912.0.0 If a version has two numbers partitioned by a \"-\" then make those the AA.BB fields and use zero for the CC Example: version command is 1234-56 then NuGet version is 1234.56.0","title":"Version"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/Mu-Basetools/#process-to-publish-new-version-of-tool","text":"Download desired version from Unzip Make a new folder (for my example I will call it \"new\") Make proper subfolders for each host. (Details in NugetPublishing/ReadMe.md) Copy the assets to publish into this new folder Run the < TOOL > -v command to see the version. Open cmd prompt in the NugetPublishing dir Pack and push NugetPublishing.py --Operation PackAndPush --ConfigFilePath Mu-Basetools.config.json --Version <nuget version here> --InputFolderPath <path to newly created folder here> --ApiKey <your key here>","title":"Process to publish new version of tool"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/Mu-Nasm/","text":"Mu-Nasm Notes \u00b6 This tool is the open source NASM assembler. More information can be found at https://nasm.us/ Where \u00b6 Go to https://nasm.us and find the desired download. What \u00b6 nasm.exe is the assembler. Version \u00b6 nasm.exe -v Nuget version is AA.BB.CC The version command generally outputs a version in AA.BB.CC format. Process to publish new version of tool \u00b6 Download desired version from nasm.us (Windows .exe and Linux .rpm) Unzip (unzipping RPM requires 7z) Make a new folder (for my example I will call it \"new\") Make proper subfolders for each host. (Details in NugetPublishing/ReadMe.md) Copy the assets to publish into this new folder (in this case just nasm and ndisasm) Run the nasm.exe -v command to see the version. Open cmd prompt in the NugetPublishing dir Pack and push NugetPublishing.py --Operation PackAndPush --ConfigFilePath Mu-Nasm.config.json --Version <nuget version here> --InputFolderPath <path to newly created folder here> --ApiKey <your key here>","title":"Mu-Nasm"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/Mu-Nasm/#mu-nasm-notes","text":"This tool is the open source NASM assembler. More information can be found at https://nasm.us/","title":"Mu-Nasm Notes"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/Mu-Nasm/#where","text":"Go to https://nasm.us and find the desired download.","title":"Where"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/Mu-Nasm/#what","text":"nasm.exe is the assembler.","title":"What"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/Mu-Nasm/#version","text":"nasm.exe -v Nuget version is AA.BB.CC The version command generally outputs a version in AA.BB.CC format.","title":"Version"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/Mu-Nasm/#process-to-publish-new-version-of-tool","text":"Download desired version from nasm.us (Windows .exe and Linux .rpm) Unzip (unzipping RPM requires 7z) Make a new folder (for my example I will call it \"new\") Make proper subfolders for each host. (Details in NugetPublishing/ReadMe.md) Copy the assets to publish into this new folder (in this case just nasm and ndisasm) Run the nasm.exe -v command to see the version. Open cmd prompt in the NugetPublishing dir Pack and push NugetPublishing.py --Operation PackAndPush --ConfigFilePath Mu-Nasm.config.json --Version <nuget version here> --InputFolderPath <path to newly created folder here> --ApiKey <your key here>","title":"Process to publish new version of tool"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/ReadMe/","text":"NugetPublishing \u00b6 Tool to help create and publish nuget packages for Project Mu resources Usage \u00b6 See NugetPublishing.py -h OPTIONAL: host_specific folders \u00b6 The possible different setups for the host are: OS: Linux, Windows, Java Architecture: x86 or ARM Highest Order Bit: 32 or 64 Before the path to the NuGet package contents is published, the Python environment can look inside at several subfolders and decide which one to use based on the Host OS, highest order bit available, and the architecture of the processor. To do so, add \"separated\" to your flags like so: \"flags\": [\"host_specific\"], If this flag is present, the environment will make a list possible subfolders that would be acceptable for the host machine. For this example, a 64 bit Windows machine with an x86 processor was used: Windows-x86-64 Windows-x86 Windows-64 x86-64 Windows x86 64 The environment will look for these folders, following this order, and select the first one it finds. If none are found, the flag will be ignored. Authentication \u00b6 For publishing most service providers require authentication. The --ApiKey parameter allows the caller to supply a unique key for authorization. There are numerous ways to authenticate. For example * Azure Dev Ops: * VSTS credential manager. In an interactive session a dialog will popup for the user to login * Tokens can also be used as the API key. Go to your account page to generate a token that can push packages * NuGet.org * Must use an API key. Go to your account page and generate a key. Example: Creating new config file for first use \u00b6 This will create the config files and place them in the current directory: NugetPublishing.py --Operation New --Name iasl --Author ProjectMu --ConfigFileFolderPath . --Description \"Description of item.\" --FeedUrl https://api.nuget.org/v3/index.json --ProjectUrl http://aka.ms/projectmu --LicenseType BSD2 For help run: NugetPublishing.py --Operation New --help Example: Publishing new version of tool \u00b6 Using an existing config file publish a new iasl.exe. See the example file iasl.config.json 1. Download version from acpica.org 2. Unzip 3. Make a new folder (for my example I will call it \"new\") 4. Copy the assets to publish into this new folder (in this case just iasl.exe) 5. Run the iasl.exe -v command to see the version. 6. Open cmd prompt in the NugetPublishing dir 7. Pack and push (here is my example command. ) NugetPublishing.py --Operation PackAndPush --ConfigFilePath iasl.config.json --Version 20180209.0.0 --InputFolderPath \"C:\\temp\\iasl-win-20180209\\new\" --ApiKey <your key here>","title":"Read Me"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/ReadMe/#nugetpublishing","text":"Tool to help create and publish nuget packages for Project Mu resources","title":"NugetPublishing"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/ReadMe/#usage","text":"See NugetPublishing.py -h","title":"Usage"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/ReadMe/#optional-host_specific-folders","text":"The possible different setups for the host are: OS: Linux, Windows, Java Architecture: x86 or ARM Highest Order Bit: 32 or 64 Before the path to the NuGet package contents is published, the Python environment can look inside at several subfolders and decide which one to use based on the Host OS, highest order bit available, and the architecture of the processor. To do so, add \"separated\" to your flags like so: \"flags\": [\"host_specific\"], If this flag is present, the environment will make a list possible subfolders that would be acceptable for the host machine. For this example, a 64 bit Windows machine with an x86 processor was used: Windows-x86-64 Windows-x86 Windows-64 x86-64 Windows x86 64 The environment will look for these folders, following this order, and select the first one it finds. If none are found, the flag will be ignored.","title":"OPTIONAL: host_specific folders"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/ReadMe/#authentication","text":"For publishing most service providers require authentication. The --ApiKey parameter allows the caller to supply a unique key for authorization. There are numerous ways to authenticate. For example * Azure Dev Ops: * VSTS credential manager. In an interactive session a dialog will popup for the user to login * Tokens can also be used as the API key. Go to your account page to generate a token that can push packages * NuGet.org * Must use an API key. Go to your account page and generate a key.","title":"Authentication"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/ReadMe/#example-creating-new-config-file-for-first-use","text":"This will create the config files and place them in the current directory: NugetPublishing.py --Operation New --Name iasl --Author ProjectMu --ConfigFileFolderPath . --Description \"Description of item.\" --FeedUrl https://api.nuget.org/v3/index.json --ProjectUrl http://aka.ms/projectmu --LicenseType BSD2 For help run: NugetPublishing.py --Operation New --help","title":"Example: Creating new config file for first use"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/ReadMe/#example-publishing-new-version-of-tool","text":"Using an existing config file publish a new iasl.exe. See the example file iasl.config.json 1. Download version from acpica.org 2. Unzip 3. Make a new folder (for my example I will call it \"new\") 4. Copy the assets to publish into this new folder (in this case just iasl.exe) 5. Run the iasl.exe -v command to see the version. 6. Open cmd prompt in the NugetPublishing dir 7. Pack and push (here is my example command. ) NugetPublishing.py --Operation PackAndPush --ConfigFilePath iasl.config.json --Version 20180209.0.0 --InputFolderPath \"C:\\temp\\iasl-win-20180209\\new\" --ApiKey <your key here>","title":"Example: Publishing new version of tool"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/iasl/","text":"IASL Notes \u00b6 This tool is the open source ACPI compiler. More information can be found at https://acpica.org/ Where \u00b6 For Windows Binary tools: https://acpica.org/downloads/binary-tools What \u00b6 iasl.exe is the compiler. Version \u00b6 iasl.exe -v Nuget version is AA.BB.CC If the version is a single number then make it the AA field and use zeros for BB.CC Example: version command is 20160912 then NuGet version is 20160912.0.0 If a version has two numbers partitioned by a \"-\" then make those the AA.BB fields and use zero for the CC Example: version command is 1234-56 then NuGet version is 1234.56.0 Process to publish new version of tool \u00b6 Download desired version from acpica.org Unzip Make a new folder (for my example I will call it \"new\") Copy the assets to publish into this new folder (in this case just iasl.exe) Run the iasl.exe -v command to see the version. Open cmd prompt in the NugetPublishing dir Pack and push NugetPublishing.py --Operation PackAndPush --ConfigFilePath iasl.config.json --Version <nuget version here> --InputFolderPath <path to newly created folder here> --ApiKey <your key here>","title":"iasl"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/iasl/#iasl-notes","text":"This tool is the open source ACPI compiler. More information can be found at https://acpica.org/","title":"IASL Notes"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/iasl/#where","text":"For Windows Binary tools: https://acpica.org/downloads/binary-tools","title":"Where"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/iasl/#what","text":"iasl.exe is the compiler.","title":"What"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/iasl/#version","text":"iasl.exe -v Nuget version is AA.BB.CC If the version is a single number then make it the AA field and use zeros for BB.CC Example: version command is 20160912 then NuGet version is 20160912.0.0 If a version has two numbers partitioned by a \"-\" then make those the AA.BB fields and use zero for the CC Example: version command is 1234-56 then NuGet version is 1234.56.0","title":"Version"},{"location":"dyn/mu_basecore/BaseTools/NugetPublishing/iasl/#process-to-publish-new-version-of-tool","text":"Download desired version from acpica.org Unzip Make a new folder (for my example I will call it \"new\") Copy the assets to publish into this new folder (in this case just iasl.exe) Run the iasl.exe -v command to see the version. Open cmd prompt in the NugetPublishing dir Pack and push NugetPublishing.py --Operation PackAndPush --ConfigFilePath iasl.config.json --Version <nuget version here> --InputFolderPath <path to newly created folder here> --ApiKey <your key here>","title":"Process to publish new version of tool"},{"location":"dyn/mu_basecore/BaseTools/Plugin/FdSizeReport/ReadMe/","text":"Flash Descriptor Size Report Generator Plugin and Command Line Tool \u00b6 Copyright \u00b6 Copyright \u00a9 Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent About \u00b6 FdSizeReportGenerator is a UEFI Build Plugin and Command Line Tool used to parse EDK2 build reports and FDF files and then produce an HTML report of the module sizes and fd sizes. The HTML report then allows deeper analysis of the Flash Usage, the Module Sizes, and overall breakdown of usage. UEFI Build Plugin \u00b6 When used in the plugin capacity this plugin will do its work in the do_post_build function. This plugin uses the following variables from the build environment: BUILDREPORTING - [REQUIRED] - must be True otherwise plugin will not run FLASH_DEFINITION - [REQUIRED] - must point to the platform FDF file BUILDREPORT_FILE - [REQUIRED] - must point to the build report file FDSIZEREPORT_FILE - [OPTIONAL] - should be path for output HTML report. If not set default path will be set based on BUILD_OUTPUT_BASE variable PRODUCT_NAME - [OPTIONAL] - should give friendly product name BUILDID_STRING - [OPTIONAL] - should give friendly version string of firmware version Command Line Tool \u00b6 When used as a command line tool check the required parameters by using the -h option.","title":"Fd Size Report"},{"location":"dyn/mu_basecore/BaseTools/Plugin/FdSizeReport/ReadMe/#flash-descriptor-size-report-generator-plugin-and-command-line-tool","text":"","title":"Flash Descriptor Size Report Generator Plugin and Command Line Tool"},{"location":"dyn/mu_basecore/BaseTools/Plugin/FdSizeReport/ReadMe/#copyright","text":"Copyright \u00a9 Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_basecore/BaseTools/Plugin/FdSizeReport/ReadMe/#about","text":"FdSizeReportGenerator is a UEFI Build Plugin and Command Line Tool used to parse EDK2 build reports and FDF files and then produce an HTML report of the module sizes and fd sizes. The HTML report then allows deeper analysis of the Flash Usage, the Module Sizes, and overall breakdown of usage.","title":"About"},{"location":"dyn/mu_basecore/BaseTools/Plugin/FdSizeReport/ReadMe/#uefi-build-plugin","text":"When used in the plugin capacity this plugin will do its work in the do_post_build function. This plugin uses the following variables from the build environment: BUILDREPORTING - [REQUIRED] - must be True otherwise plugin will not run FLASH_DEFINITION - [REQUIRED] - must point to the platform FDF file BUILDREPORT_FILE - [REQUIRED] - must point to the build report file FDSIZEREPORT_FILE - [OPTIONAL] - should be path for output HTML report. If not set default path will be set based on BUILD_OUTPUT_BASE variable PRODUCT_NAME - [OPTIONAL] - should give friendly product name BUILDID_STRING - [OPTIONAL] - should give friendly version string of firmware version","title":"UEFI Build Plugin"},{"location":"dyn/mu_basecore/BaseTools/Plugin/FdSizeReport/ReadMe/#command-line-tool","text":"When used as a command line tool check the required parameters by using the -h option.","title":"Command Line Tool"},{"location":"dyn/mu_basecore/BaseTools/Plugin/OverrideValidation/ReadMe/","text":"Override Validation Plugin \u00b6 Module Level Override Validation Plugin and Linkage Creation Command Line Tool About \u00b6 OverrideValidation is a UEFI Build Plugin and Command Line Tool used to create linkage between overriding and overridden modules and parse INF files referenced in platform DSC files during build process and then produce a TXT report of the module overriding status. The TXT report then allows deeper analysis of the Overriding Hierarchy, the Override Linkage Validity, the Override Linkage Ages, and overall breakdown of usage. UEFI Build Plugin \u00b6 When used in the plugin capacity this plugin will do its override linkage validation work in the do_pre_build function. This plugin uses the following variables from the build environment: ACTIVE_PLATFORM - [REQUIRED] - must be workspace relative or package path relative pointing to the target platform dsc file, otherwise this validation will not run BUILD_OUTPUT_BASE - [REQUIRED] - must be an absolute path specified to store override log at $(BUILD_OUTPUT_BASE)/OVERRIDELOG.TXT, otherwise no report will be generated BUILDSHA - [OPTIONAL] - should have valid commit sha value for report purpose, if not provided, 'None' will be used for the corresponding field PRODUCT_NAME - [OPTIONAL] - should give friendly product name, if not provided, 'None' will be used for the corresponding field BUILDID_STRING - [OPTIONAL] - should give friendly version string of firmware version, if not provided, 'None' will be used for the corresponding field Command Line Tool \u00b6 When used as a command line tool, this tool takes the absolute path of workspace (the root directory of Devices repo) as well as the absolute path of overridden module's inf file and then generate a screen-print line for users to include in overriding modules in order to create override linkage. Check the required parameters by using the -h option for command line argument details. Example \u00b6 Command to generate an override record: OverrideValidation.py -w C:\\Repo -m C:\\Repo\\SM_UDK\\MdePkg\\Library\\BaseMemoryLib\\BaseMemoryLib.inf Override record to be included in overriding module's inf: #Override : 00000001 | MdePkg/Library/BaseMemoryLib/BaseMemoryLib.inf | cc255d9de141fccbdfca9ad02e0daa47 | 2018-05-09T17-54-17 Override log generated during pre-build process: Platform : PlatformName Version : 123.456 . 7890 Date : 2018 - 05 - 11 T17 - 56 - 27 Commit : _SHA_2c9def7a4ce84ef26ed6597afcc60cee4e5c92c0 State : 3 / 4 Overrides ---------------------------------------------------------------- OVERRIDER : MdePkg /Library/BaseMemoryLibOptDxe/ BaseMemoryLibOptDxe . inf ORIGINALS : + MdePkg /Library/BaseMemoryLib/ BaseMemoryLib . inf | SUCCESS | 2 days OVERRIDER : PlatformNamePkg /Library/NvmConfigLib/ NvmConfigLib . inf ORIGINALS : + MdeModulePkg /Bus/Pci/NvmExpressDxe/ NvmExpressDxe . inf | MISMATCH | 35 days | Current State : 62929532257365 b261080b7e7b1c4e7a | Last Fingerprint : dc9f5e3af1efbac6cf5485b672291903 + MdePkg /Library/BaseMemoryLibOptDxe/ BaseMemoryLibOptDxe . inf | SUCCESS | 0 days + MdePkg /Library/BaseMemoryLib/ BaseMemoryLib . inf | SUCCESS | 2 days Copyright & License \u00b6 Copyright \u00a9 Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Override Validation"},{"location":"dyn/mu_basecore/BaseTools/Plugin/OverrideValidation/ReadMe/#override-validation-plugin","text":"Module Level Override Validation Plugin and Linkage Creation Command Line Tool","title":"Override Validation Plugin"},{"location":"dyn/mu_basecore/BaseTools/Plugin/OverrideValidation/ReadMe/#about","text":"OverrideValidation is a UEFI Build Plugin and Command Line Tool used to create linkage between overriding and overridden modules and parse INF files referenced in platform DSC files during build process and then produce a TXT report of the module overriding status. The TXT report then allows deeper analysis of the Overriding Hierarchy, the Override Linkage Validity, the Override Linkage Ages, and overall breakdown of usage.","title":"About"},{"location":"dyn/mu_basecore/BaseTools/Plugin/OverrideValidation/ReadMe/#uefi-build-plugin","text":"When used in the plugin capacity this plugin will do its override linkage validation work in the do_pre_build function. This plugin uses the following variables from the build environment: ACTIVE_PLATFORM - [REQUIRED] - must be workspace relative or package path relative pointing to the target platform dsc file, otherwise this validation will not run BUILD_OUTPUT_BASE - [REQUIRED] - must be an absolute path specified to store override log at $(BUILD_OUTPUT_BASE)/OVERRIDELOG.TXT, otherwise no report will be generated BUILDSHA - [OPTIONAL] - should have valid commit sha value for report purpose, if not provided, 'None' will be used for the corresponding field PRODUCT_NAME - [OPTIONAL] - should give friendly product name, if not provided, 'None' will be used for the corresponding field BUILDID_STRING - [OPTIONAL] - should give friendly version string of firmware version, if not provided, 'None' will be used for the corresponding field","title":"UEFI Build Plugin"},{"location":"dyn/mu_basecore/BaseTools/Plugin/OverrideValidation/ReadMe/#command-line-tool","text":"When used as a command line tool, this tool takes the absolute path of workspace (the root directory of Devices repo) as well as the absolute path of overridden module's inf file and then generate a screen-print line for users to include in overriding modules in order to create override linkage. Check the required parameters by using the -h option for command line argument details.","title":"Command Line Tool"},{"location":"dyn/mu_basecore/BaseTools/Plugin/OverrideValidation/ReadMe/#example","text":"Command to generate an override record: OverrideValidation.py -w C:\\Repo -m C:\\Repo\\SM_UDK\\MdePkg\\Library\\BaseMemoryLib\\BaseMemoryLib.inf Override record to be included in overriding module's inf: #Override : 00000001 | MdePkg/Library/BaseMemoryLib/BaseMemoryLib.inf | cc255d9de141fccbdfca9ad02e0daa47 | 2018-05-09T17-54-17 Override log generated during pre-build process: Platform : PlatformName Version : 123.456 . 7890 Date : 2018 - 05 - 11 T17 - 56 - 27 Commit : _SHA_2c9def7a4ce84ef26ed6597afcc60cee4e5c92c0 State : 3 / 4 Overrides ---------------------------------------------------------------- OVERRIDER : MdePkg /Library/BaseMemoryLibOptDxe/ BaseMemoryLibOptDxe . inf ORIGINALS : + MdePkg /Library/BaseMemoryLib/ BaseMemoryLib . inf | SUCCESS | 2 days OVERRIDER : PlatformNamePkg /Library/NvmConfigLib/ NvmConfigLib . inf ORIGINALS : + MdeModulePkg /Bus/Pci/NvmExpressDxe/ NvmExpressDxe . inf | MISMATCH | 35 days | Current State : 62929532257365 b261080b7e7b1c4e7a | Last Fingerprint : dc9f5e3af1efbac6cf5485b672291903 + MdePkg /Library/BaseMemoryLibOptDxe/ BaseMemoryLibOptDxe . inf | SUCCESS | 0 days + MdePkg /Library/BaseMemoryLib/ BaseMemoryLib . inf | SUCCESS | 2 days","title":"Example"},{"location":"dyn/mu_basecore/BaseTools/Plugin/OverrideValidation/ReadMe/#copyright-license","text":"Copyright \u00a9 Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright &amp; License"},{"location":"dyn/mu_basecore/BaseTools/Scripts/PackageDocumentTools/Readme/","text":"Prerequisite Tools: 1. Install Python 2.7.3 from https://www.python.org/download/releases/2.7.3/ 2. Install wxPython 2.8.12.1 from https://sourceforge.net/projects/wxpython/files/wxPython/2.8.12.1/ generally the libraries will be installed at python's subfolder, for example in windows: c:\\python27\\Lib\\site-packages\\ 3. Install DoxyGen 1.8.6 from https://sourceforge.net/projects/doxygen/files/rel-1.8.6/ 4. (Windows only) Install Htmlhelp tool from https://msdn.microsoft.com/en-us/library/windows/desktop/ms669985(v=vs.85).aspx Limitation: 1. Current tool doesn't work on latest wxPython and DoxyGen tool. Please use the sepecific version in above. Run the Tool: a) Run with GUI: 1. Enter src folder, double click \"packagedocapp.pyw\" or run command \"python packagedocapp.pyw\" to open the GUI. 2. Make sure all the information in blank are correct. 3. Click \"Generate Package Document!\" b) Run with command line: 1. Open command line window 2. Enter src folder, for example: \"cd C:\\PackageDocumentTools\\src\" 3. Run \"python packagedoc_cli.py --help\" for detail command.","title":"Package Document Tools"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/","text":"Introduction \u00b6 Brotli is a generic-purpose lossless compression algorithm that compresses data using a combination of a modern variant of the LZ77 algorithm, Huffman coding and 2 nd order context modeling, with a compression ratio comparable to the best currently available general-purpose compression methods. It is similar in speed with deflate but offers more dense compression. The specification of the Brotli Compressed Data Format is defined in RFC 7932 . Brotli is open-sourced under the MIT License, see the LICENSE file. Brotli mailing list: https://groups.google.com/forum/#!forum/brotli Benchmarks \u00b6 Squash Compression Benchmark / Unstable Squash Compression Benchmark Large Text Compression Benchmark Lzturbo Benchmark Related projects \u00b6 Independent decoder implementation by Mark Adler, based entirely on format specification. JavaScript port of brotli decoder . Could be used directly via npm install brotli","title":"README"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/#introduction","text":"Brotli is a generic-purpose lossless compression algorithm that compresses data using a combination of a modern variant of the LZ77 algorithm, Huffman coding and 2 nd order context modeling, with a compression ratio comparable to the best currently available general-purpose compression methods. It is similar in speed with deflate but offers more dense compression. The specification of the Brotli Compressed Data Format is defined in RFC 7932 . Brotli is open-sourced under the MIT License, see the LICENSE file. Brotli mailing list: https://groups.google.com/forum/#!forum/brotli","title":"Introduction"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/#benchmarks","text":"Squash Compression Benchmark / Unstable Squash Compression Benchmark Large Text Compression Benchmark Lzturbo Benchmark","title":"Benchmarks"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/#related-projects","text":"Independent decoder implementation by Mark Adler, based entirely on format specification. JavaScript port of brotli decoder . Could be used directly via npm install brotli","title":"Related projects"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/tools/brotli/","text":"brotli(1) -- brotli, unbrotli - compress or decompress files \u00b6 SYNOPSIS \u00b6 brotli [ OPTION|FILE ]... unbrotli is equivalent to brotli --decompress DESCRIPTION \u00b6 brotli is a generic-purpose lossless compression algorithm that compresses data using a combination of a modern variant of the LZ77 algorithm, Huffman coding and 2-nd order context modeling, with a compression ratio comparable to the best currently available general-purpose compression methods. It is similar in speed with deflate but offers more dense compression. brotli command line syntax similar to gzip (1) and zstd (1) . Unlike gzip (1) , source files are preserved by default. It is possible to remove them after processing by using the --rm option . Arguments that look like \" --name \" or \" --name=value \" are options . Every option has a short form \" -x \" or \" -x value \". Multiple short form options could be coalesced: \" --decompress --stdout --suffix=.b \" works the same as \" -d -s -S .b \" and \" -dsS .b \" brotli has 3 operation modes: default mode is compression; --decompress option activates decompression mode; --test option switches to integrity test mode; this option is equivalent to \" --decompress --stdout \" except that the decompressed data is discarded instead of being written to standard output. Every non-option argument is a file entry. If no files are given or file is \" - \", brotli reads from standard input. All arguments after \" -- \" are file entries. Unless --stdout or --output is specified, files are written to a new file whose name is derived from the source file name: when compressing, a suffix is appended to the source filename to get the target filename when decompressing, a suffix is removed from the source filename to get the target filename Default suffix is .br , but it could be specified with --suffix option. Conflicting or duplicate options are not allowed. OPTIONS \u00b6 -# : compression level (0-9); bigger values cause denser, but slower compression -c , --stdout : write on standard output -d , --decompress : decompress mode -f , --force : force output file overwrite -h , --help : display this help and exit -j , --rm : remove source file(s); gzip (1) -like behaviour -k , --keep : keep source file(s); zstd (1) -like behaviour -n , --no-copy-stat : do not copy source file(s) attributes -o FILE , --output=FILE output file; valid only if there is a single input entry -q NUM , --quality=NUM : compression level (0-11); bigger values cause denser, but slower compression -t , --test : test file integrity mode -v , --verbose : increase output verbosity -w NUM , --lgwin=NUM : set LZ77 window size (0, 10-24) (default: 22); window size is (2**NUM - 16) ; 0 lets compressor decide over the optimal value; bigger windows size improve density; decoder might require up to window size memory to operate -S SUF , --suffix=SUF : output file suffix (default: .br ) -V , --version : display version and exit -Z , --best : use best compression level (default); same as \" -q 11 \" SEE ALSO \u00b6 brotli file format is defined in RFC 7932 . brotli is open-sourced under the MIT License . Mailing list: https://groups.google.com/forum/#!forum/brotli BUGS \u00b6 Report bugs at: https://github.com/google/brotli/issues","title":"tools"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/tools/brotli/#brotli1-brotli-unbrotli-compress-or-decompress-files","text":"","title":"brotli(1) -- brotli, unbrotli - compress or decompress files"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/tools/brotli/#synopsis","text":"brotli [ OPTION|FILE ]... unbrotli is equivalent to brotli --decompress","title":"SYNOPSIS"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/tools/brotli/#description","text":"brotli is a generic-purpose lossless compression algorithm that compresses data using a combination of a modern variant of the LZ77 algorithm, Huffman coding and 2-nd order context modeling, with a compression ratio comparable to the best currently available general-purpose compression methods. It is similar in speed with deflate but offers more dense compression. brotli command line syntax similar to gzip (1) and zstd (1) . Unlike gzip (1) , source files are preserved by default. It is possible to remove them after processing by using the --rm option . Arguments that look like \" --name \" or \" --name=value \" are options . Every option has a short form \" -x \" or \" -x value \". Multiple short form options could be coalesced: \" --decompress --stdout --suffix=.b \" works the same as \" -d -s -S .b \" and \" -dsS .b \" brotli has 3 operation modes: default mode is compression; --decompress option activates decompression mode; --test option switches to integrity test mode; this option is equivalent to \" --decompress --stdout \" except that the decompressed data is discarded instead of being written to standard output. Every non-option argument is a file entry. If no files are given or file is \" - \", brotli reads from standard input. All arguments after \" -- \" are file entries. Unless --stdout or --output is specified, files are written to a new file whose name is derived from the source file name: when compressing, a suffix is appended to the source filename to get the target filename when decompressing, a suffix is removed from the source filename to get the target filename Default suffix is .br , but it could be specified with --suffix option. Conflicting or duplicate options are not allowed.","title":"DESCRIPTION"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/tools/brotli/#options","text":"-# : compression level (0-9); bigger values cause denser, but slower compression -c , --stdout : write on standard output -d , --decompress : decompress mode -f , --force : force output file overwrite -h , --help : display this help and exit -j , --rm : remove source file(s); gzip (1) -like behaviour -k , --keep : keep source file(s); zstd (1) -like behaviour -n , --no-copy-stat : do not copy source file(s) attributes -o FILE , --output=FILE output file; valid only if there is a single input entry -q NUM , --quality=NUM : compression level (0-11); bigger values cause denser, but slower compression -t , --test : test file integrity mode -v , --verbose : increase output verbosity -w NUM , --lgwin=NUM : set LZ77 window size (0, 10-24) (default: 22); window size is (2**NUM - 16) ; 0 lets compressor decide over the optimal value; bigger windows size improve density; decoder might require up to window size memory to operate -S SUF , --suffix=SUF : output file suffix (default: .br ) -V , --version : display version and exit -Z , --best : use best compression level (default); same as \" -q 11 \"","title":"OPTIONS"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/tools/brotli/#see-also","text":"brotli file format is defined in RFC 7932 . brotli is open-sourced under the MIT License . Mailing list: https://groups.google.com/forum/#!forum/brotli","title":"SEE ALSO"},{"location":"dyn/mu_basecore/BaseTools/Source/C/BrotliCompress/tools/brotli/#bugs","text":"Report bugs at: https://github.com/google/brotli/issues","title":"BUGS"},{"location":"dyn/mu_basecore/BaseTools/Source/Python/Pkcs7Sign/Readme/","text":"Step by step to generate sample self-signed X.509 certificate chain and sign data with PKCS7 structure \u00b6 This readme demonstrates how to generate 3-layer X.509 certificate chain (RootCA -> IntermediateCA -> SigningCert) with OpenSSL commands, and user MUST set a UNIQUE Subject Name (\"Common Name\") on these three different certificates. How to generate a self-signed X.509 certificate chain via OPENSSL \u00b6 Set OPENSSL environment. NOTE: Below steps are required for Windows. Linux may already have the OPENSSL environment correctly. set OPENSSL_HOME = c : \\ home \\ openssl \\ openssl -[ version ] set OPENSSL_CONF =% OPENSSL_HOME % \\ apps \\ openssl . cnf When a user uses OpenSSL (req or ca command) to generate the certificates, OpenSSL will use the openssl.cnf file as the configuration data (can use \"-config path/to/openssl.cnf\" to describe the specific config file). The user need check the openssl.cnf file, to find your CA path setting, e.g. check if the path exists in [ CA_default ] section. [ CA_default ] dir = ./demoCA # Where everything is kept You may need the following steps for initialization: rd ./demoCA /S/Q mkdir ./demoCA echo.>./demoCA/index.txt echo 01 > ./demoCA/serial mkdir ./demoCA/newcerts OpenSSL will apply the options from the specified sections in openssl.cnf when creating certificates or certificate signing requests. Make sure your configuration in openssl.cnf is correct and rational for certificate constraints. The following sample sections were used when generating test certificates in this readme. ... [ req ] default_bits = 2048 default_keyfile = privkey.pem distinguished_name = req_distinguished_name attributes = req_attributes x509_extensions = v3_ca # The extensions to add to the self signed cert ... [ v3_ca ] # Extensions for a typical Root CA. subjectKeyIdentifier=hash authorityKeyIdentifier=keyid:always,issuer basicConstraints = critical,CA:true keyUsage = critical, digitalSignature, cRLSign, keyCertSign ... [ v3_intermediate_ca ] # Extensions for a typical intermediate CA. subjectKeyIdentifier = hash authorityKeyIdentifier = keyid:always,issuer basicConstraints = critical, CA:true keyUsage = critical, digitalSignature, cRLSign, keyCertSign ... [ usr_cert ] # Extensions for user end certificates. basicConstraints = CA:FALSE nsCertType = client, email subjectKeyIdentifier = hash authorityKeyIdentifier = keyid,issuer keyUsage = critical, nonRepudiation, digitalSignature, keyEncipherment extendedKeyUsage = clientAuth, emailProtection ... Generate the certificate chain: NOTE: User MUST set a UNIQUE \"Common Name\" on the different certificate 1) Generate the Root Pair: Generate a root key: openssl genrsa -aes256 -out TestRoot.key 2048 Generate a self-signed root certificate: openssl req -extensions v3_ca -new -x509 -days 3650 -key TestRoot.key -out TestRoot.crt openssl x509 -in TestRoot.crt -out TestRoot.cer -outform DER openssl x509 -inform DER -in TestRoot.cer -outform PEM -out TestRoot.pub.pem 2) Generate the Intermediate Pair: Generate the intermediate key: openssl genrsa -aes256 -out TestSub.key 2048 Generate the intermediate certificate: openssl req -new -days 3650 -key TestSub.key -out TestSub.csr openssl ca -extensions v3_intermediate_ca -in TestSub.csr -days 3650 -out TestSub.crt -cert TestRoot.crt -keyfile TestRoot.key openssl x509 -in TestSub.crt -out TestSub.cer -outform DER openssl x509 -inform DER -in TestSub.cer -outform PEM -out TestSub.pub.pem 3) Generate User Key Pair for Data Signing: Generate User key: openssl genrsa -aes256 -out TestCert.key 2048 Generate User certificate: openssl req -new -days 3650 -key TestCert.key -out TestCert.csr openssl ca -extensions usr_cert -in TestCert.csr -days 3650 -out TestCert.crt -cert TestSub.crt -keyfile TestSub.key openssl x509 -in TestCert.crt -out TestCert.cer -outform DER openssl x509 -inform DER -in TestCert.cer -outform PEM -out TestCert.pub.pem Convert Key and Certificate for signing. Password is removed with -nodes flag for convenience in this sample. openssl pkcs12 -export -out TestCert.pfx -inkey TestCert.key -in TestCert.crt openssl pkcs12 -in TestCert.pfx -nodes -out TestCert.pem Verify Data Signing & Verification with new X.509 Certificate Chain 1) Sign a Binary File to generate a detached PKCS7 signature: openssl smime -sign -binary -signer TestCert.pem -outform DER -md sha256 -certfile TestSub.pub.pem -out test.bin.p7 -in test.bin 2) Verify PKCS7 Signature of a Binary File: openssl smime -verify -inform DER -in test.bin.p7 -content test.bin -CAfile TestRoot.pub.pem -out test.org.bin Generate DSC PCD include files for Certificate \u00b6 The BinToPcd utility can be used to convert the binary Certificate file to a text file can be included from a DSC file to set a PCD to the contents of the Certificate file. The following 2 PCDs can be set to the PKCS7 Certificate value. The first one supports a single certificate. The second one supports multiple certificate values using the XDR format. * gEfiSecurityPkgTokenSpaceGuid.PcdPkcs7CertBuffer * gFmpDevicePkgTokenSpaceGuid.PcdFmpDevicePkcs7CertBufferXdr Generate DSC PCD include files: BinToPcd.py -i TestRoot.cer -p gEfiSecurityPkgTokenSpaceGuid.PcdPkcs7CertBuffer -o TestRoot.cer.gEfiSecurityPkgTokenSpaceGuid.PcdPkcs7CertBuffer.inc BinToPcd.py -i TestRoot.cer -p gFmpDevicePkgTokenSpaceGuid.PcdFmpDevicePkcs7CertBufferXdr -x -o TestRoot.cer.gFmpDevicePkgTokenSpaceGuid.PcdFmpDevicePkcs7CertBufferXdr.inc These files can be used in !include statements in DSC file PCD sections. For example: Platform scoped fixed at build PCD section [PcdsFixedAtBuild] !include BaseTools/Source/Python/Pkcs7Sign/TestRoot.cer.gEfiSecurityPkgTokenSpaceGuid.PcdPkcs7CertBuffer.inc Platform scoped patchable in module PCD section [PcdsPatchableInModule] !include BaseTools/Source/Python/Pkcs7Sign/TestRoot.cer.gFmpDevicePkgTokenSpaceGuid.PcdFmpDevicePkcs7CertBufferXdr.inc Module scoped fixed at build PCD section [Components] FmpDevicePkg/FmpDxe/FmpDxe.inf { <PcdsFixedAtBuild> !include BaseTools/Source/Python/Pkcs7Sign/TestRoot.cer.gFmpDevicePkgTokenSpaceGuid.PcdFmpDevicePkcs7CertBufferXdr.inc }","title":"Pkcs7Sign"},{"location":"dyn/mu_basecore/BaseTools/Source/Python/Pkcs7Sign/Readme/#step-by-step-to-generate-sample-self-signed-x509-certificate-chain-and-sign-data-with-pkcs7-structure","text":"This readme demonstrates how to generate 3-layer X.509 certificate chain (RootCA -> IntermediateCA -> SigningCert) with OpenSSL commands, and user MUST set a UNIQUE Subject Name (\"Common Name\") on these three different certificates.","title":"Step by step to generate sample self-signed X.509 certificate chain and sign data with PKCS7 structure"},{"location":"dyn/mu_basecore/BaseTools/Source/Python/Pkcs7Sign/Readme/#how-to-generate-a-self-signed-x509-certificate-chain-via-openssl","text":"Set OPENSSL environment. NOTE: Below steps are required for Windows. Linux may already have the OPENSSL environment correctly. set OPENSSL_HOME = c : \\ home \\ openssl \\ openssl -[ version ] set OPENSSL_CONF =% OPENSSL_HOME % \\ apps \\ openssl . cnf When a user uses OpenSSL (req or ca command) to generate the certificates, OpenSSL will use the openssl.cnf file as the configuration data (can use \"-config path/to/openssl.cnf\" to describe the specific config file). The user need check the openssl.cnf file, to find your CA path setting, e.g. check if the path exists in [ CA_default ] section. [ CA_default ] dir = ./demoCA # Where everything is kept You may need the following steps for initialization: rd ./demoCA /S/Q mkdir ./demoCA echo.>./demoCA/index.txt echo 01 > ./demoCA/serial mkdir ./demoCA/newcerts OpenSSL will apply the options from the specified sections in openssl.cnf when creating certificates or certificate signing requests. Make sure your configuration in openssl.cnf is correct and rational for certificate constraints. The following sample sections were used when generating test certificates in this readme. ... [ req ] default_bits = 2048 default_keyfile = privkey.pem distinguished_name = req_distinguished_name attributes = req_attributes x509_extensions = v3_ca # The extensions to add to the self signed cert ... [ v3_ca ] # Extensions for a typical Root CA. subjectKeyIdentifier=hash authorityKeyIdentifier=keyid:always,issuer basicConstraints = critical,CA:true keyUsage = critical, digitalSignature, cRLSign, keyCertSign ... [ v3_intermediate_ca ] # Extensions for a typical intermediate CA. subjectKeyIdentifier = hash authorityKeyIdentifier = keyid:always,issuer basicConstraints = critical, CA:true keyUsage = critical, digitalSignature, cRLSign, keyCertSign ... [ usr_cert ] # Extensions for user end certificates. basicConstraints = CA:FALSE nsCertType = client, email subjectKeyIdentifier = hash authorityKeyIdentifier = keyid,issuer keyUsage = critical, nonRepudiation, digitalSignature, keyEncipherment extendedKeyUsage = clientAuth, emailProtection ... Generate the certificate chain: NOTE: User MUST set a UNIQUE \"Common Name\" on the different certificate 1) Generate the Root Pair: Generate a root key: openssl genrsa -aes256 -out TestRoot.key 2048 Generate a self-signed root certificate: openssl req -extensions v3_ca -new -x509 -days 3650 -key TestRoot.key -out TestRoot.crt openssl x509 -in TestRoot.crt -out TestRoot.cer -outform DER openssl x509 -inform DER -in TestRoot.cer -outform PEM -out TestRoot.pub.pem 2) Generate the Intermediate Pair: Generate the intermediate key: openssl genrsa -aes256 -out TestSub.key 2048 Generate the intermediate certificate: openssl req -new -days 3650 -key TestSub.key -out TestSub.csr openssl ca -extensions v3_intermediate_ca -in TestSub.csr -days 3650 -out TestSub.crt -cert TestRoot.crt -keyfile TestRoot.key openssl x509 -in TestSub.crt -out TestSub.cer -outform DER openssl x509 -inform DER -in TestSub.cer -outform PEM -out TestSub.pub.pem 3) Generate User Key Pair for Data Signing: Generate User key: openssl genrsa -aes256 -out TestCert.key 2048 Generate User certificate: openssl req -new -days 3650 -key TestCert.key -out TestCert.csr openssl ca -extensions usr_cert -in TestCert.csr -days 3650 -out TestCert.crt -cert TestSub.crt -keyfile TestSub.key openssl x509 -in TestCert.crt -out TestCert.cer -outform DER openssl x509 -inform DER -in TestCert.cer -outform PEM -out TestCert.pub.pem Convert Key and Certificate for signing. Password is removed with -nodes flag for convenience in this sample. openssl pkcs12 -export -out TestCert.pfx -inkey TestCert.key -in TestCert.crt openssl pkcs12 -in TestCert.pfx -nodes -out TestCert.pem Verify Data Signing & Verification with new X.509 Certificate Chain 1) Sign a Binary File to generate a detached PKCS7 signature: openssl smime -sign -binary -signer TestCert.pem -outform DER -md sha256 -certfile TestSub.pub.pem -out test.bin.p7 -in test.bin 2) Verify PKCS7 Signature of a Binary File: openssl smime -verify -inform DER -in test.bin.p7 -content test.bin -CAfile TestRoot.pub.pem -out test.org.bin","title":"How to generate a self-signed X.509 certificate chain via OPENSSL"},{"location":"dyn/mu_basecore/BaseTools/Source/Python/Pkcs7Sign/Readme/#generate-dsc-pcd-include-files-for-certificate","text":"The BinToPcd utility can be used to convert the binary Certificate file to a text file can be included from a DSC file to set a PCD to the contents of the Certificate file. The following 2 PCDs can be set to the PKCS7 Certificate value. The first one supports a single certificate. The second one supports multiple certificate values using the XDR format. * gEfiSecurityPkgTokenSpaceGuid.PcdPkcs7CertBuffer * gFmpDevicePkgTokenSpaceGuid.PcdFmpDevicePkcs7CertBufferXdr Generate DSC PCD include files: BinToPcd.py -i TestRoot.cer -p gEfiSecurityPkgTokenSpaceGuid.PcdPkcs7CertBuffer -o TestRoot.cer.gEfiSecurityPkgTokenSpaceGuid.PcdPkcs7CertBuffer.inc BinToPcd.py -i TestRoot.cer -p gFmpDevicePkgTokenSpaceGuid.PcdFmpDevicePkcs7CertBufferXdr -x -o TestRoot.cer.gFmpDevicePkgTokenSpaceGuid.PcdFmpDevicePkcs7CertBufferXdr.inc These files can be used in !include statements in DSC file PCD sections. For example: Platform scoped fixed at build PCD section [PcdsFixedAtBuild] !include BaseTools/Source/Python/Pkcs7Sign/TestRoot.cer.gEfiSecurityPkgTokenSpaceGuid.PcdPkcs7CertBuffer.inc Platform scoped patchable in module PCD section [PcdsPatchableInModule] !include BaseTools/Source/Python/Pkcs7Sign/TestRoot.cer.gFmpDevicePkgTokenSpaceGuid.PcdFmpDevicePkcs7CertBufferXdr.inc Module scoped fixed at build PCD section [Components] FmpDevicePkg/FmpDxe/FmpDxe.inf { <PcdsFixedAtBuild> !include BaseTools/Source/Python/Pkcs7Sign/TestRoot.cer.gFmpDevicePkgTokenSpaceGuid.PcdFmpDevicePkcs7CertBufferXdr.inc }","title":"Generate DSC PCD include files for Certificate"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/","text":"Introduction \u00b6 Brotli is a generic-purpose lossless compression algorithm that compresses data using a combination of a modern variant of the LZ77 algorithm, Huffman coding and 2 nd order context modeling, with a compression ratio comparable to the best currently available general-purpose compression methods. It is similar in speed with deflate but offers more dense compression. The specification of the Brotli Compressed Data Format is defined in RFC 7932 . Brotli is open-sourced under the MIT License, see the LICENSE file. Brotli mailing list: https://groups.google.com/forum/#!forum/brotli Benchmarks \u00b6 Squash Compression Benchmark / Unstable Squash Compression Benchmark Large Text Compression Benchmark Lzturbo Benchmark Related projects \u00b6 Independent decoder implementation by Mark Adler, based entirely on format specification. JavaScript port of brotli decoder . Could be used directly via npm install brotli","title":"Brotli Custom Decompress Lib"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/#introduction","text":"Brotli is a generic-purpose lossless compression algorithm that compresses data using a combination of a modern variant of the LZ77 algorithm, Huffman coding and 2 nd order context modeling, with a compression ratio comparable to the best currently available general-purpose compression methods. It is similar in speed with deflate but offers more dense compression. The specification of the Brotli Compressed Data Format is defined in RFC 7932 . Brotli is open-sourced under the MIT License, see the LICENSE file. Brotli mailing list: https://groups.google.com/forum/#!forum/brotli","title":"Introduction"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/#benchmarks","text":"Squash Compression Benchmark / Unstable Squash Compression Benchmark Large Text Compression Benchmark Lzturbo Benchmark","title":"Benchmarks"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/BrotliCustomDecompressLib/#related-projects","text":"Independent decoder implementation by Mark Adler, based entirely on format specification. JavaScript port of brotli decoder . Could be used directly via npm install brotli","title":"Related projects"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/DeviceStateLib/Readme/","text":"DeviceStateLib \u00b6 About \u00b6 The MsCorePkg provides the necessary functions to store platform specific device states. These device states can then be queried by any element within the boot environment to enable special code paths. In this library implementation a bitmask is stored in a PCD to signify what modes are active. The default bits in the bitmask are set in DeviceStateLib.h - but each platform is expected to implement its own header to define the platform specific device states or to define any of the unused bits: BIT 0: DEVICE_STATE_SECUREBOOT_OFF - UEFI Secure Boot disabled BIT 1: DEVICE_STATE_MANUFACTURING_MODE - Device is in an OEM defined manufacturing mode BIT 2: DEVICE_STATE_DEVELOPMENT_BUILD_ENABLED - Device is a development build. Non-production features might be enabled BIT 3: DEVICE_STATE_SOURCE_DEBUG_ENABLED - Source debug mode is enabled allowing a user to connect and control the device BIT 4: DEVICE_STATE_UNDEFINED - Set by the platform BIT 5: DEVICE_STATE_UNIT_TEST_MODE - Device has a unit test build. Some features are disabled to allow for unit tests in UEFI Shell BIT 24: DEVICE_STATE_PLATFORM_MODE_0 BIT 25: DEVICE_STATE_PLATFORM_MODE_1 BIT 26: DEVICE_STATE_PLATFORM_MODE_2 BIT 27: DEVICE_STATE_PLATFORM_MODE_3 Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Device State Lib"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/DeviceStateLib/Readme/#devicestatelib","text":"","title":"DeviceStateLib"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/DeviceStateLib/Readme/#about","text":"The MsCorePkg provides the necessary functions to store platform specific device states. These device states can then be queried by any element within the boot environment to enable special code paths. In this library implementation a bitmask is stored in a PCD to signify what modes are active. The default bits in the bitmask are set in DeviceStateLib.h - but each platform is expected to implement its own header to define the platform specific device states or to define any of the unused bits: BIT 0: DEVICE_STATE_SECUREBOOT_OFF - UEFI Secure Boot disabled BIT 1: DEVICE_STATE_MANUFACTURING_MODE - Device is in an OEM defined manufacturing mode BIT 2: DEVICE_STATE_DEVELOPMENT_BUILD_ENABLED - Device is a development build. Non-production features might be enabled BIT 3: DEVICE_STATE_SOURCE_DEBUG_ENABLED - Source debug mode is enabled allowing a user to connect and control the device BIT 4: DEVICE_STATE_UNDEFINED - Set by the platform BIT 5: DEVICE_STATE_UNIT_TEST_MODE - Device has a unit test build. Some features are disabled to allow for unit tests in UEFI Shell BIT 24: DEVICE_STATE_PLATFORM_MODE_0 BIT 25: DEVICE_STATE_PLATFORM_MODE_1 BIT 26: DEVICE_STATE_PLATFORM_MODE_2 BIT 27: DEVICE_STATE_PLATFORM_MODE_3","title":"About"},{"location":"dyn/mu_basecore/MdeModulePkg/Library/DeviceStateLib/Readme/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_basecore/MsUnitTestPkg/ReadMe/","text":"Ms Unit Test Support Package \u00b6 About \u00b6 This package adds a unit test framework targeted at the UEFI shell environment. It allows for unit test development to focus on the tests and leave, error logging, result formatting, context persistance, and test running to the framework. The unit test framework works well for low level unit tests as well as system level tests and fits easily in automation frameworks. The code is designed for a unit test application to leverage the framework which is made up of a number of libraries which allow for easy customization of the different elements. A few different instances are created to both show how easy some behaviors can be customized as well as provide different implementations that support different use cases. UnitTestLib \u00b6 The main \"framework\" library. This provides the framework init, suite init, and add test case functionality. It also supports the running of the suites and logging/reporting of results. UnitTestAssetLib \u00b6 The UnitTestAssetLib provides helper macros and functions for checking test conditions and reporting errors. Status and error info will be logged into the test context. There are a number of Assert macros that make the unit test code friendly to view and easy to understand. UnitTestBootUsbLib \u00b6 One of the unique features of the unit test framework is to be able to save text context and reboot the system. Since unit tests are generally run from a bootable usb key the framework has library calls to set boot next for usb. There is numerous ways this could be done on a given platform / BDS implementation and therefore this simple library allows customization if needed. This package supplies two instances: UsbClass Lib: This uses the Usb Class boot option as defined in the UEFI spec and leveraged by industry standard USB applications. UsbMicrosoft Lib: This uses a private boot option found in Microsoft UEFI to boot to usb UnitTestLogLib \u00b6 Library to support logging information during the test execution. This data is logged to the test context and will be available in the test reporting phase. This should be used for logging test details and helpful messages to resolve test failures. UnitTestResultReportLib \u00b6 Library provides function to run at the end of a framework test run and handles formatting the report. This is a common customization point and allows the unit test framework to fit its output reports into other test infrastructure. In this package a simple library instances has been supplied to output test results to the console as plain text. UnitTestPersistenceLib \u00b6 Persistence lib has the main job of saving and restoring test context to a storage medium so that for tests that require exiting the active process and then resuming state can be maintained. This is critical in supporting a system reboot in the middle of a test run. Samples \u00b6 There is a sample unit test provided as both an example of how to write a unit test and leverage many of the features of the framework. This sample can be found in the SampleUnitTestApp directory. Copyright & License \u00b6 Copyright \u00a9 Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Modules"},{"location":"dyn/mu_basecore/MsUnitTestPkg/ReadMe/#ms-unit-test-support-package","text":"","title":"Ms Unit Test Support Package"},{"location":"dyn/mu_basecore/MsUnitTestPkg/ReadMe/#about","text":"This package adds a unit test framework targeted at the UEFI shell environment. It allows for unit test development to focus on the tests and leave, error logging, result formatting, context persistance, and test running to the framework. The unit test framework works well for low level unit tests as well as system level tests and fits easily in automation frameworks. The code is designed for a unit test application to leverage the framework which is made up of a number of libraries which allow for easy customization of the different elements. A few different instances are created to both show how easy some behaviors can be customized as well as provide different implementations that support different use cases.","title":"About"},{"location":"dyn/mu_basecore/MsUnitTestPkg/ReadMe/#unittestlib","text":"The main \"framework\" library. This provides the framework init, suite init, and add test case functionality. It also supports the running of the suites and logging/reporting of results.","title":"UnitTestLib"},{"location":"dyn/mu_basecore/MsUnitTestPkg/ReadMe/#unittestassetlib","text":"The UnitTestAssetLib provides helper macros and functions for checking test conditions and reporting errors. Status and error info will be logged into the test context. There are a number of Assert macros that make the unit test code friendly to view and easy to understand.","title":"UnitTestAssetLib"},{"location":"dyn/mu_basecore/MsUnitTestPkg/ReadMe/#unittestbootusblib","text":"One of the unique features of the unit test framework is to be able to save text context and reboot the system. Since unit tests are generally run from a bootable usb key the framework has library calls to set boot next for usb. There is numerous ways this could be done on a given platform / BDS implementation and therefore this simple library allows customization if needed. This package supplies two instances: UsbClass Lib: This uses the Usb Class boot option as defined in the UEFI spec and leveraged by industry standard USB applications. UsbMicrosoft Lib: This uses a private boot option found in Microsoft UEFI to boot to usb","title":"UnitTestBootUsbLib"},{"location":"dyn/mu_basecore/MsUnitTestPkg/ReadMe/#unittestloglib","text":"Library to support logging information during the test execution. This data is logged to the test context and will be available in the test reporting phase. This should be used for logging test details and helpful messages to resolve test failures.","title":"UnitTestLogLib"},{"location":"dyn/mu_basecore/MsUnitTestPkg/ReadMe/#unittestresultreportlib","text":"Library provides function to run at the end of a framework test run and handles formatting the report. This is a common customization point and allows the unit test framework to fit its output reports into other test infrastructure. In this package a simple library instances has been supplied to output test results to the console as plain text.","title":"UnitTestResultReportLib"},{"location":"dyn/mu_basecore/MsUnitTestPkg/ReadMe/#unittestpersistencelib","text":"Persistence lib has the main job of saving and restoring test context to a storage medium so that for tests that require exiting the active process and then resuming state can be maintained. This is critical in supporting a system reboot in the middle of a test run.","title":"UnitTestPersistenceLib"},{"location":"dyn/mu_basecore/MsUnitTestPkg/ReadMe/#samples","text":"There is a sample unit test provided as both an example of how to write a unit test and leverage many of the features of the framework. This sample can be found in the SampleUnitTestApp directory.","title":"Samples"},{"location":"dyn/mu_basecore/MsUnitTestPkg/ReadMe/#copyright-license","text":"Copyright \u00a9 Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright &amp; License"},{"location":"dyn/mu_basecore/NetworkPkg/SharedNetworking/SharedNetworking/","text":"Shared Networking DXE \u00b6 What is it \u00b6 Shared Networking is a packaged versions of networking components from EDK II. Similar to SharedCrypto (see the SharedCryptoPkg), it precompiles certain components and allows them to be included in a platform without having to build the underlying library. How it works \u00b6 Since many parts of the network simply publish a protocol (like TlsDxe), it was fairly trivial to compile that into an EFI. This EFI is then downloaded via a NuGet External Dependency (see SharedNetworking_ext_dep.json). Versions are modified in a similar way to SharedCrypto. Versioning \u00b6 A typical version consists of 4 numbers. The year, the month of the EDK II release, the revision number, and the build number. An example of this would be 2019.03.02.01 , which would translate to EDK II 1903 release, the second revision and the first build. This means that there were two code changes within 1903 (either in BaseCryptLib or OpenSSL). Release notes will be provided on the NuGet package page and on this repo. Build numbers are reved whenever there needs to be a recompiled binary due to a mistake on our part or a build flag is tweaked. How to use it \u00b6 There are two ways to use SharedNetworking. For first way is to use the FV, which contains all the networking components needed. The second is to replace individual components with INF's. DSC/INF way \u00b6 Including it in your platform is easy peezy lemon squeezy. In fact, you only need three changes. In the example below we show X64, which happens to correspond with DXE but that could easily be changed. Look at your platform for where Networking is already defined. One thing to note is that each binary is released for two targets, RELEASE and DEBUG. Make sure to include the right INF. DSC Changes \u00b6 Parts need to be replaced on a compoenent by component basis. For example, here is how to move over TlsDxe. You need to remove the reference to TLSLib since we no longer need it (the only consumer is TlsDxe). Then switch the component to the Shared version of TLS. It looks like this: [LibraryClasses.X64] #TlsLib|CryptoPkg/Library/TlsLib/TlsLib.inf # remove this line [Components.X64] NetworkPkg/SharedNetworking/TlsDxe.$(TARGET).inf FDF Changes \u00b6 [FV.FVDXE] INF NetworkPkg/SharedNetworking/TlsDxe.$(TARGET).inf # Shared_TLS instead of TlsDxe ... FV way \u00b6 This way is still under development, so it maybe subject to change. In your FDF, add these lines. [FV.FVDXE] FILE FV_IMAGE = {GUID} { SECTION FV_IMAGE = NetworkPkg/SharedNetworking/Mu-SharedNetworking_extdep/$(TARGET)/{ARCH of your platform}/FVDXE.fv # Shared_Networking SECTION UI = \"SharedNetworking\" } With {GUID} being a guid you generated. We use E205F779-07E3-4B64-A2E2-EEDE717B0F59. {Arch of your platform} being the platform you're using. We currently support IA32, X64, and AARCH64. as supposered values You'll also need to remove the networking components that were already in your FDF. Why to Use SharedNetworking \u00b6 Depending on your platform, it could net you some small space savings depending on your linker. The main advantage is that when used with SharedCrypto, you can remove the need to compile OpenSSL, reducing compile times. Questions \u00b6 If you have any questions about anything in this package or the universe in general, feel free to comment on our Github or contact the Project Mu team.","title":"Shared Networking"},{"location":"dyn/mu_basecore/NetworkPkg/SharedNetworking/SharedNetworking/#shared-networking-dxe","text":"","title":"Shared Networking DXE"},{"location":"dyn/mu_basecore/NetworkPkg/SharedNetworking/SharedNetworking/#what-is-it","text":"Shared Networking is a packaged versions of networking components from EDK II. Similar to SharedCrypto (see the SharedCryptoPkg), it precompiles certain components and allows them to be included in a platform without having to build the underlying library.","title":"What is it"},{"location":"dyn/mu_basecore/NetworkPkg/SharedNetworking/SharedNetworking/#how-it-works","text":"Since many parts of the network simply publish a protocol (like TlsDxe), it was fairly trivial to compile that into an EFI. This EFI is then downloaded via a NuGet External Dependency (see SharedNetworking_ext_dep.json). Versions are modified in a similar way to SharedCrypto.","title":"How it works"},{"location":"dyn/mu_basecore/NetworkPkg/SharedNetworking/SharedNetworking/#versioning","text":"A typical version consists of 4 numbers. The year, the month of the EDK II release, the revision number, and the build number. An example of this would be 2019.03.02.01 , which would translate to EDK II 1903 release, the second revision and the first build. This means that there were two code changes within 1903 (either in BaseCryptLib or OpenSSL). Release notes will be provided on the NuGet package page and on this repo. Build numbers are reved whenever there needs to be a recompiled binary due to a mistake on our part or a build flag is tweaked.","title":"Versioning"},{"location":"dyn/mu_basecore/NetworkPkg/SharedNetworking/SharedNetworking/#how-to-use-it","text":"There are two ways to use SharedNetworking. For first way is to use the FV, which contains all the networking components needed. The second is to replace individual components with INF's.","title":"How to use it"},{"location":"dyn/mu_basecore/NetworkPkg/SharedNetworking/SharedNetworking/#dscinf-way","text":"Including it in your platform is easy peezy lemon squeezy. In fact, you only need three changes. In the example below we show X64, which happens to correspond with DXE but that could easily be changed. Look at your platform for where Networking is already defined. One thing to note is that each binary is released for two targets, RELEASE and DEBUG. Make sure to include the right INF.","title":"DSC/INF way"},{"location":"dyn/mu_basecore/NetworkPkg/SharedNetworking/SharedNetworking/#dsc-changes","text":"Parts need to be replaced on a compoenent by component basis. For example, here is how to move over TlsDxe. You need to remove the reference to TLSLib since we no longer need it (the only consumer is TlsDxe). Then switch the component to the Shared version of TLS. It looks like this: [LibraryClasses.X64] #TlsLib|CryptoPkg/Library/TlsLib/TlsLib.inf # remove this line [Components.X64] NetworkPkg/SharedNetworking/TlsDxe.$(TARGET).inf","title":"DSC Changes"},{"location":"dyn/mu_basecore/NetworkPkg/SharedNetworking/SharedNetworking/#fdf-changes","text":"[FV.FVDXE] INF NetworkPkg/SharedNetworking/TlsDxe.$(TARGET).inf # Shared_TLS instead of TlsDxe ...","title":"FDF Changes"},{"location":"dyn/mu_basecore/NetworkPkg/SharedNetworking/SharedNetworking/#fv-way","text":"This way is still under development, so it maybe subject to change. In your FDF, add these lines. [FV.FVDXE] FILE FV_IMAGE = {GUID} { SECTION FV_IMAGE = NetworkPkg/SharedNetworking/Mu-SharedNetworking_extdep/$(TARGET)/{ARCH of your platform}/FVDXE.fv # Shared_Networking SECTION UI = \"SharedNetworking\" } With {GUID} being a guid you generated. We use E205F779-07E3-4B64-A2E2-EEDE717B0F59. {Arch of your platform} being the platform you're using. We currently support IA32, X64, and AARCH64. as supposered values You'll also need to remove the networking components that were already in your FDF.","title":"FV way"},{"location":"dyn/mu_basecore/NetworkPkg/SharedNetworking/SharedNetworking/#why-to-use-sharednetworking","text":"Depending on your platform, it could net you some small space savings depending on your linker. The main advantage is that when used with SharedCrypto, you can remove the need to compile OpenSSL, reducing compile times.","title":"Why to Use SharedNetworking"},{"location":"dyn/mu_basecore/NetworkPkg/SharedNetworking/SharedNetworking/#questions","text":"If you have any questions about anything in this package or the universe in general, feel free to comment on our Github or contact the Project Mu team.","title":"Questions"},{"location":"dyn/mu_basecore/NetworkPkg/SharedNetworking/release_notes/","text":"Shared Network Release Notes \u00b6 This is the packaged version of NetworkPkg","title":"release notes"},{"location":"dyn/mu_basecore/NetworkPkg/SharedNetworking/release_notes/#shared-network-release-notes","text":"This is the packaged version of NetworkPkg","title":"Shared Network Release Notes"},{"location":"dyn/mu_basecore/pytool/Readme/","text":"Edk2 Continuous Integration \u00b6 Basic Status \u00b6 Package Windows VS2019 (IA32/X64) Ubuntu GCC (IA32/X64/ARM/AARCH64) Known Issues ArmPkg ArmPlatformPkg ArmVirtPkg CryptoPkg Spell checking in audit mode DynamicTablesPkg EmbeddedPkg EmulatorPkg FatPkg FmpDevicePkg IntelFsp2Pkg IntelFsp2WrapperPkg MdeModulePkg DxeIpl dependency on ArmPkg, Depends on StandaloneMmPkg, Spell checking in audit mode MdePkg Spell checking in audit mode NetworkPkg Spell checking in audit mode OvmfPkg PcAtChipsetPkg SecurityPkg Spell checking in audit mode ShellPkg Spell checking in audit mode, 3 modules are not being built by DSC SignedCapsulePkg SourceLevelDebugPkg StandaloneMmPkg UefiCpuPkg Spell checking in audit mode, 2 binary modules not being built by DSC UefiPayloadPkg For more detailed status look at the test results of the latest CI run on the repo readme. Background \u00b6 This Continuous integration and testing infrastructure leverages the TianoCore EDKII Tools PIP modules: library and extensions (with repos located here and here ). The primary execution flows can be found in the .azurepipelines / Windows-VS2019.yml and .azurepipelines / Ubuntu-GCC5.yml files. These YAML files are consumed by the Azure Dev Ops Build Pipeline and dictate what server resources should be used, how they should be configured, and what processes should be run on them. An overview of this schema can be found here . Inspection of these files reveals the EDKII Tools commands that make up the primary processes for the CI build: 'stuart_setup', 'stuart_update', and 'stuart_ci_build'. These commands come from the EDKII Tools PIP modules and are configured as described below. More documentation on the tools can be found here and here . Configuration \u00b6 Configuration of the CI process consists of (in order of precedence): command-line arguments passed in via the Pipeline YAML a per-package configuration file (e.g. <package-name>.ci.yaml ) that is detected by the CI system in EDKII Tools. a global configuration Python module (e.g. CISetting.py ) passed in via the command-line The global configuration file is described in this readme from the EDKII Tools documentation. This configuration is written as a Python module so that decisions can be made dynamically based on command line parameters and codebase state. The per-package configuration file can override most settings in the global configuration file, but is not dynamic. This file can be used to skip or customize tests that may be incompatible with a specific package. Each test generally requires per package configuration which comes from this file. Running CI locally \u00b6 The EDKII Tools environment (and by extension the ci) is designed to support easily and consistantly running locally and in a cloud ci environment. To do that a few steps should be followed. Details of EDKII Tools can be found in the docs folder here Prerequisets \u00b6 A supported toolchain (others might work but this is what is tested and validated) Windows 10: VS 2017 or VS 2019 Windows SDK (for rc) Windows WDK (for capsules) Ubuntu 16.04 GCC5 Easy to add more but this is the current state Python 3.7.x or newer on path git on path Recommended to setup and activate a python virtual environment Install the requirements pip install --upgrade pip-requirements.txt Running CI \u00b6 clone your edk2 repo Activate your python virtual environment in cmd window Get code dependencies (done only when submodules change) stuart_setup -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> Update other dependencies (done more often) stuart_update -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> Run CI build (--help will give you options) stuart_ci_build -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> -p : To build only certain packages use a CSV list -a : To run only certain architectures use a CSV list -t : To run only tests related to certain targets use a CSV list By default all tests are opted in. Then given a package.ci.yaml file those tests can be configured for a package. Finally setting the check to the value skip will skip that plugin. Examples: CompilerPlugin=skip skip the build test GuidCheck=skip skip the Guid check SpellCheck=skip skip the spell checker etc Detailed reports and logs per package are captured in the Build directory Current PyTool Test Capabilities \u00b6 All CI tests are instances of EDKII Tools plugins. Documentation on the plugin system can be found here and here . Upon invocation, each plugin will be passed the path to the current package under test and a dictionary containing its targeted configuration, as assembled from the command line, per-package configuration, and global configuration. Note: CI plugins are considered unique from build plugins and helper plugins, even though some CI plugins may execute steps of a build. In the example, these plugins live alongside the code under test (in the .pytool / Plugin directory), but may be moved to the 'edk2-test' repo if that location makes more sense for the community. Module Inclusion Test - DscCompleteCheck \u00b6 This test scans all available modules (via INF files) and compares them to the package-level DSC file for the package each module is contained within. The test considers it an error if any module does not appear in the Components section of at least one package-level DSC (indicating that it would not be built if the package were built). Code Compilation Test - CompilerPlugin \u00b6 Once the Module Inclusion Test has verified that all modules would be built if all package-level DSCs were built, the Code Compilation Test simply runs through and builds every package-level DSC on every toolchain and for every architecture that is supported. Any module that fails to build is considered an error. GUID Uniqueness Test - GuidCheck \u00b6 This test works on the collection of all packages rather than an individual package. It looks at all FILE_GUIDs and GUIDs declared in DEC files and ensures that they are unique for the codebase. This prevents, for example, accidental duplication of GUIDs when using an existing INF as a template for a new module. Cross-Package Dependency Test - DependencyCheck \u00b6 This test compares the list of all packages used in INFs files for a given package against a list of \"allowed dependencies\" in plugin configuration for that package. Any module that depends on a disallowed package will cause a test failure. Library Declaration Test - LibraryClassCheck \u00b6 This test scans at all library header files found in the Library folders in all of the package's declared include directories and ensures that all files have a matching LibraryClass declaration in the DEC file for the package. Any missing declarations will cause a failure. Invalid Character Test - CharEncodingCheck \u00b6 This test scans all files in a package to make sure that there are no invalid Unicode characters that may cause build errors in some character sets/localizations. Spell Checking - cspell \u00b6 This test runs a spell checker on all files within the package. This is done using the NodeJs cspell tool. For details check .pytool / Plugin / SpellCheck . For this plugin to run during ci you must install nodejs and cspell and have both available to the command line when running your CI. Install Install nodejs from https://nodejs.org/en/ Install cspell Open cmd prompt with access to node and npm Run npm install -g cspell More cspell info: https://github.com/streetsidesoftware/cspell PyTool Scopes \u00b6 Scopes are how the PyTool ext_dep, path_env, and plugins are activated. Meaning that if an invocable process has a scope active then those ext_dep and path_env will be active. To allow easy integration of PyTools capabilities there are a few standard scopes. Scope Invocable Description global edk2_invocable++ - should be base_abstract_invocable Running an invocables global-win edk2_invocable++ Running on Microsoft Windows global-nix edk2_invocable++ Running on Linux based OS edk2-build This indicates that an invocable is building EDK2 based UEFI code cibuild set in .pytool/CISettings.py Suggested target for edk2 continuous integration builds. Tools used for CiBuilds can use this scope. Example: asl compiler Future investments \u00b6 PatchCheck tests as plugins MacOS/xcode support Clang/LLVM support Visual Studio AARCH64 and ARM support BaseTools C tools CI/PR and binary release process BaseTools Python tools CI/PR process Host based unit testing Extensible private/closed source platform reporting Platform builds, validation UEFI SCTs Other automation","title":"Readme"},{"location":"dyn/mu_basecore/pytool/Readme/#edk2-continuous-integration","text":"","title":"Edk2 Continuous Integration"},{"location":"dyn/mu_basecore/pytool/Readme/#basic-status","text":"Package Windows VS2019 (IA32/X64) Ubuntu GCC (IA32/X64/ARM/AARCH64) Known Issues ArmPkg ArmPlatformPkg ArmVirtPkg CryptoPkg Spell checking in audit mode DynamicTablesPkg EmbeddedPkg EmulatorPkg FatPkg FmpDevicePkg IntelFsp2Pkg IntelFsp2WrapperPkg MdeModulePkg DxeIpl dependency on ArmPkg, Depends on StandaloneMmPkg, Spell checking in audit mode MdePkg Spell checking in audit mode NetworkPkg Spell checking in audit mode OvmfPkg PcAtChipsetPkg SecurityPkg Spell checking in audit mode ShellPkg Spell checking in audit mode, 3 modules are not being built by DSC SignedCapsulePkg SourceLevelDebugPkg StandaloneMmPkg UefiCpuPkg Spell checking in audit mode, 2 binary modules not being built by DSC UefiPayloadPkg For more detailed status look at the test results of the latest CI run on the repo readme.","title":"Basic Status"},{"location":"dyn/mu_basecore/pytool/Readme/#background","text":"This Continuous integration and testing infrastructure leverages the TianoCore EDKII Tools PIP modules: library and extensions (with repos located here and here ). The primary execution flows can be found in the .azurepipelines / Windows-VS2019.yml and .azurepipelines / Ubuntu-GCC5.yml files. These YAML files are consumed by the Azure Dev Ops Build Pipeline and dictate what server resources should be used, how they should be configured, and what processes should be run on them. An overview of this schema can be found here . Inspection of these files reveals the EDKII Tools commands that make up the primary processes for the CI build: 'stuart_setup', 'stuart_update', and 'stuart_ci_build'. These commands come from the EDKII Tools PIP modules and are configured as described below. More documentation on the tools can be found here and here .","title":"Background"},{"location":"dyn/mu_basecore/pytool/Readme/#configuration","text":"Configuration of the CI process consists of (in order of precedence): command-line arguments passed in via the Pipeline YAML a per-package configuration file (e.g. <package-name>.ci.yaml ) that is detected by the CI system in EDKII Tools. a global configuration Python module (e.g. CISetting.py ) passed in via the command-line The global configuration file is described in this readme from the EDKII Tools documentation. This configuration is written as a Python module so that decisions can be made dynamically based on command line parameters and codebase state. The per-package configuration file can override most settings in the global configuration file, but is not dynamic. This file can be used to skip or customize tests that may be incompatible with a specific package. Each test generally requires per package configuration which comes from this file.","title":"Configuration"},{"location":"dyn/mu_basecore/pytool/Readme/#running-ci-locally","text":"The EDKII Tools environment (and by extension the ci) is designed to support easily and consistantly running locally and in a cloud ci environment. To do that a few steps should be followed. Details of EDKII Tools can be found in the docs folder here","title":"Running CI locally"},{"location":"dyn/mu_basecore/pytool/Readme/#prerequisets","text":"A supported toolchain (others might work but this is what is tested and validated) Windows 10: VS 2017 or VS 2019 Windows SDK (for rc) Windows WDK (for capsules) Ubuntu 16.04 GCC5 Easy to add more but this is the current state Python 3.7.x or newer on path git on path Recommended to setup and activate a python virtual environment Install the requirements pip install --upgrade pip-requirements.txt","title":"Prerequisets"},{"location":"dyn/mu_basecore/pytool/Readme/#running-ci","text":"clone your edk2 repo Activate your python virtual environment in cmd window Get code dependencies (done only when submodules change) stuart_setup -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> Update other dependencies (done more often) stuart_update -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> Run CI build (--help will give you options) stuart_ci_build -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> -p : To build only certain packages use a CSV list -a : To run only certain architectures use a CSV list -t : To run only tests related to certain targets use a CSV list By default all tests are opted in. Then given a package.ci.yaml file those tests can be configured for a package. Finally setting the check to the value skip will skip that plugin. Examples: CompilerPlugin=skip skip the build test GuidCheck=skip skip the Guid check SpellCheck=skip skip the spell checker etc Detailed reports and logs per package are captured in the Build directory","title":"Running CI"},{"location":"dyn/mu_basecore/pytool/Readme/#current-pytool-test-capabilities","text":"All CI tests are instances of EDKII Tools plugins. Documentation on the plugin system can be found here and here . Upon invocation, each plugin will be passed the path to the current package under test and a dictionary containing its targeted configuration, as assembled from the command line, per-package configuration, and global configuration. Note: CI plugins are considered unique from build plugins and helper plugins, even though some CI plugins may execute steps of a build. In the example, these plugins live alongside the code under test (in the .pytool / Plugin directory), but may be moved to the 'edk2-test' repo if that location makes more sense for the community.","title":"Current PyTool Test Capabilities"},{"location":"dyn/mu_basecore/pytool/Readme/#module-inclusion-test-dsccompletecheck","text":"This test scans all available modules (via INF files) and compares them to the package-level DSC file for the package each module is contained within. The test considers it an error if any module does not appear in the Components section of at least one package-level DSC (indicating that it would not be built if the package were built).","title":"Module Inclusion Test - DscCompleteCheck"},{"location":"dyn/mu_basecore/pytool/Readme/#code-compilation-test-compilerplugin","text":"Once the Module Inclusion Test has verified that all modules would be built if all package-level DSCs were built, the Code Compilation Test simply runs through and builds every package-level DSC on every toolchain and for every architecture that is supported. Any module that fails to build is considered an error.","title":"Code Compilation Test - CompilerPlugin"},{"location":"dyn/mu_basecore/pytool/Readme/#guid-uniqueness-test-guidcheck","text":"This test works on the collection of all packages rather than an individual package. It looks at all FILE_GUIDs and GUIDs declared in DEC files and ensures that they are unique for the codebase. This prevents, for example, accidental duplication of GUIDs when using an existing INF as a template for a new module.","title":"GUID Uniqueness Test - GuidCheck"},{"location":"dyn/mu_basecore/pytool/Readme/#cross-package-dependency-test-dependencycheck","text":"This test compares the list of all packages used in INFs files for a given package against a list of \"allowed dependencies\" in plugin configuration for that package. Any module that depends on a disallowed package will cause a test failure.","title":"Cross-Package Dependency Test - DependencyCheck"},{"location":"dyn/mu_basecore/pytool/Readme/#library-declaration-test-libraryclasscheck","text":"This test scans at all library header files found in the Library folders in all of the package's declared include directories and ensures that all files have a matching LibraryClass declaration in the DEC file for the package. Any missing declarations will cause a failure.","title":"Library Declaration Test - LibraryClassCheck"},{"location":"dyn/mu_basecore/pytool/Readme/#invalid-character-test-charencodingcheck","text":"This test scans all files in a package to make sure that there are no invalid Unicode characters that may cause build errors in some character sets/localizations.","title":"Invalid Character Test - CharEncodingCheck"},{"location":"dyn/mu_basecore/pytool/Readme/#spell-checking-cspell","text":"This test runs a spell checker on all files within the package. This is done using the NodeJs cspell tool. For details check .pytool / Plugin / SpellCheck . For this plugin to run during ci you must install nodejs and cspell and have both available to the command line when running your CI. Install Install nodejs from https://nodejs.org/en/ Install cspell Open cmd prompt with access to node and npm Run npm install -g cspell More cspell info: https://github.com/streetsidesoftware/cspell","title":"Spell Checking - cspell"},{"location":"dyn/mu_basecore/pytool/Readme/#pytool-scopes","text":"Scopes are how the PyTool ext_dep, path_env, and plugins are activated. Meaning that if an invocable process has a scope active then those ext_dep and path_env will be active. To allow easy integration of PyTools capabilities there are a few standard scopes. Scope Invocable Description global edk2_invocable++ - should be base_abstract_invocable Running an invocables global-win edk2_invocable++ Running on Microsoft Windows global-nix edk2_invocable++ Running on Linux based OS edk2-build This indicates that an invocable is building EDK2 based UEFI code cibuild set in .pytool/CISettings.py Suggested target for edk2 continuous integration builds. Tools used for CiBuilds can use this scope. Example: asl compiler","title":"PyTool Scopes"},{"location":"dyn/mu_basecore/pytool/Readme/#future-investments","text":"PatchCheck tests as plugins MacOS/xcode support Clang/LLVM support Visual Studio AARCH64 and ARM support BaseTools C tools CI/PR and binary release process BaseTools Python tools CI/PR process Host based unit testing Extensible private/closed source platform reporting Platform builds, validation UEFI SCTs Other automation","title":"Future investments"},{"location":"dyn/mu_basecore/pytool/Plugin/CharEncodingCheck/Readme/","text":"Character Encoding Check Plugin \u00b6 This CiBuildPlugin scans all the files in a package to make sure each file is correctly encoded and all characters can be read. Improper encoding causes tools to fail in some situations especially in different locals. Configuration \u00b6 The plugin can be configured to ignore certain files. \"CharEncodingCheck\" : { \"IgnoreFiles\" : [] } IgnoreFiles \u00b6 OPTIONAL List of file to ignore.","title":"Char Encoding Check"},{"location":"dyn/mu_basecore/pytool/Plugin/CharEncodingCheck/Readme/#character-encoding-check-plugin","text":"This CiBuildPlugin scans all the files in a package to make sure each file is correctly encoded and all characters can be read. Improper encoding causes tools to fail in some situations especially in different locals.","title":"Character Encoding Check Plugin"},{"location":"dyn/mu_basecore/pytool/Plugin/CharEncodingCheck/Readme/#configuration","text":"The plugin can be configured to ignore certain files. \"CharEncodingCheck\" : { \"IgnoreFiles\" : [] }","title":"Configuration"},{"location":"dyn/mu_basecore/pytool/Plugin/CharEncodingCheck/Readme/#ignorefiles","text":"OPTIONAL List of file to ignore.","title":"IgnoreFiles"},{"location":"dyn/mu_basecore/pytool/Plugin/CompilerPlugin/Readme/","text":"Compiler Plugin \u00b6 This CiBuildPlugin compiles the package DSC from the package being tested. Configuration \u00b6 The package relative path of the DSC file to build. \"CompilerPlugin\" : { \"DscPath\" : \"<path to dsc from root of pkg>\" } DscPath \u00b6 Package relative path to the DSC file to build.","title":"Compiler Plugin"},{"location":"dyn/mu_basecore/pytool/Plugin/CompilerPlugin/Readme/#compiler-plugin","text":"This CiBuildPlugin compiles the package DSC from the package being tested.","title":"Compiler Plugin"},{"location":"dyn/mu_basecore/pytool/Plugin/CompilerPlugin/Readme/#configuration","text":"The package relative path of the DSC file to build. \"CompilerPlugin\" : { \"DscPath\" : \"<path to dsc from root of pkg>\" }","title":"Configuration"},{"location":"dyn/mu_basecore/pytool/Plugin/CompilerPlugin/Readme/#dscpath","text":"Package relative path to the DSC file to build.","title":"DscPath"},{"location":"dyn/mu_basecore/pytool/Plugin/DependencyCheck/Readme/","text":"Depdendency Check Plugin \u00b6 A CiBuildPlugin that finds all modules (inf files) in a package and reviews the packages used to confirm they are acceptable. This is to help enforce layering and identify improper dependencies between packages. Configuration \u00b6 The plugin must be configured with the acceptabe package dependencies for the package. \"DependencyCheck\" : { \"AcceptableDependencies\" : [], \"AcceptableDependencies-<MODULE_TYPE>\" : [], \"IgnoreInf\" : [] } AcceptableDependencies \u00b6 Package dec files that are allowed in all INFs. Example: MdePkg/MdePkg.dec AcceptableDependencies- \u00b6 OPTIONAL Package dependencies for INFs that have module type . Example: AcceptableDependencies-HOST_APPLICATION. IgnoreInf \u00b6 OPTIONAL list of INFs to ignore for this dependency check.","title":"Dependency Check"},{"location":"dyn/mu_basecore/pytool/Plugin/DependencyCheck/Readme/#depdendency-check-plugin","text":"A CiBuildPlugin that finds all modules (inf files) in a package and reviews the packages used to confirm they are acceptable. This is to help enforce layering and identify improper dependencies between packages.","title":"Depdendency Check Plugin"},{"location":"dyn/mu_basecore/pytool/Plugin/DependencyCheck/Readme/#configuration","text":"The plugin must be configured with the acceptabe package dependencies for the package. \"DependencyCheck\" : { \"AcceptableDependencies\" : [], \"AcceptableDependencies-<MODULE_TYPE>\" : [], \"IgnoreInf\" : [] }","title":"Configuration"},{"location":"dyn/mu_basecore/pytool/Plugin/DependencyCheck/Readme/#acceptabledependencies","text":"Package dec files that are allowed in all INFs. Example: MdePkg/MdePkg.dec","title":"AcceptableDependencies"},{"location":"dyn/mu_basecore/pytool/Plugin/DependencyCheck/Readme/#acceptabledependencies-","text":"OPTIONAL Package dependencies for INFs that have module type . Example: AcceptableDependencies-HOST_APPLICATION.","title":"AcceptableDependencies-"},{"location":"dyn/mu_basecore/pytool/Plugin/DependencyCheck/Readme/#ignoreinf","text":"OPTIONAL list of INFs to ignore for this dependency check.","title":"IgnoreInf"},{"location":"dyn/mu_basecore/pytool/Plugin/DscCompleteCheck/Readme/","text":"Dsc Complete Check Plugin \u00b6 This CiBuildPlugin scans all INF files from a package and confirms they are listed in the package level DSC file. The test considers it an error if any INF does not appear in the Components section of the package-level DSC (indicating that it would not be built if the package were built). This is critical because much of the CI infrastructure assumes that all modules will be listed in the DSC and compiled. Configuration \u00b6 The plugin has a few configuration options to support the UEFI codebase. \"DscCompleteCheck\" : { \"DscPath\" : \"\" , # Path to dsc from root of package \"IgnoreInf\" : [] # Ignore INF if found in filesystem by not dsc } DscPath \u00b6 Path to DSC to consider platform dsc IgnoreInf \u00b6 Ignore error if Inf file is not listed in DSC file","title":"Dsc Complete Check"},{"location":"dyn/mu_basecore/pytool/Plugin/DscCompleteCheck/Readme/#dsc-complete-check-plugin","text":"This CiBuildPlugin scans all INF files from a package and confirms they are listed in the package level DSC file. The test considers it an error if any INF does not appear in the Components section of the package-level DSC (indicating that it would not be built if the package were built). This is critical because much of the CI infrastructure assumes that all modules will be listed in the DSC and compiled.","title":"Dsc Complete Check Plugin"},{"location":"dyn/mu_basecore/pytool/Plugin/DscCompleteCheck/Readme/#configuration","text":"The plugin has a few configuration options to support the UEFI codebase. \"DscCompleteCheck\" : { \"DscPath\" : \"\" , # Path to dsc from root of package \"IgnoreInf\" : [] # Ignore INF if found in filesystem by not dsc }","title":"Configuration"},{"location":"dyn/mu_basecore/pytool/Plugin/DscCompleteCheck/Readme/#dscpath","text":"Path to DSC to consider platform dsc","title":"DscPath"},{"location":"dyn/mu_basecore/pytool/Plugin/DscCompleteCheck/Readme/#ignoreinf","text":"Ignore error if Inf file is not listed in DSC file","title":"IgnoreInf"},{"location":"dyn/mu_basecore/pytool/Plugin/GuidCheck/Readme/","text":"Guid Check Plugin \u00b6 This CiBuildPlugin scans all the files in a code tree to find all the GUID definitions. After collection it will then look for duplication in the package under test. Uniqueness of all GUIDs are critical within the UEFI environment. Duplication can cause numerous issues including locating the wrong data structure, calling the wrong function, or decoding the wrong data members. Currently Scanned: INF files are scanned for there Module guid DEC files are scanned for all of their Protocols, PPIs, and Guids as well as the one package GUID. Any GUID value being equal to two names or even just defined in two files is considered an error unless in the ignore list. Any GUID name that is found more than once is an error unless all occurrences are Module GUIDs. Since the Module GUID is assigned to the Module name it is common to have numerous versions of the same module named the same. Configuration \u00b6 The plugin has numerous configuration options to support the UEFI codebase. \"GuidCheck\" : { \"IgnoreGuidName\" : [], \"IgnoreGuidValue\" : [], \"IgnoreFoldersAndFiles\" : [], \"IgnoreDuplicates\" : [] } IgnoreGuidName \u00b6 This list allows strings in two formats. GuidName This will remove any entry with this GuidName from the list of GUIDs therefore ignoring any error associated with this name. GuidName=GuidValue This will also ignore the GUID by name but only if the value equals the GuidValue. GuidValue should be in registry format. This is the suggested format to use as it will limit the ignore to only the defined case. IgnoreGuidValue \u00b6 This list allows strings in guid registry format GuidValue . This will remove any entry with this GuidValue from the list of GUIDs therefore ignoring any error associated with this value. GuidValue must be in registry format xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx IgnoreFoldersAndFiles \u00b6 This supports .gitignore file and folder matching strings including wildcards Any folder or file ignored will not be parsed and therefore any GUID defined will be ignored. The plugin will always ignores the following [\"/Build\", \"/Conf\"] IgnoreDuplicates \u00b6 This supports strings in the format of GuidName = GuidName = GuidName For the error with the GuidNames to be ignored the list must match completely with what is found during the code scan. For example if there are two GUIDs that are by design equal within the code tree then it should be GuidName = GuidName If instead there are three GUIDs then it must be GuidName = GuidName = GuidName This is the best ignore list to use because it is the most strict and will catch new problems when new conflicts are introduced. There are numerous places in the UEFI specification in which two GUID names are assigned the same value. These names should be set in this ignore list so that they don't cause an error but any additional duplication would still be caught.","title":"Guid Check"},{"location":"dyn/mu_basecore/pytool/Plugin/GuidCheck/Readme/#guid-check-plugin","text":"This CiBuildPlugin scans all the files in a code tree to find all the GUID definitions. After collection it will then look for duplication in the package under test. Uniqueness of all GUIDs are critical within the UEFI environment. Duplication can cause numerous issues including locating the wrong data structure, calling the wrong function, or decoding the wrong data members. Currently Scanned: INF files are scanned for there Module guid DEC files are scanned for all of their Protocols, PPIs, and Guids as well as the one package GUID. Any GUID value being equal to two names or even just defined in two files is considered an error unless in the ignore list. Any GUID name that is found more than once is an error unless all occurrences are Module GUIDs. Since the Module GUID is assigned to the Module name it is common to have numerous versions of the same module named the same.","title":"Guid Check Plugin"},{"location":"dyn/mu_basecore/pytool/Plugin/GuidCheck/Readme/#configuration","text":"The plugin has numerous configuration options to support the UEFI codebase. \"GuidCheck\" : { \"IgnoreGuidName\" : [], \"IgnoreGuidValue\" : [], \"IgnoreFoldersAndFiles\" : [], \"IgnoreDuplicates\" : [] }","title":"Configuration"},{"location":"dyn/mu_basecore/pytool/Plugin/GuidCheck/Readme/#ignoreguidname","text":"This list allows strings in two formats. GuidName This will remove any entry with this GuidName from the list of GUIDs therefore ignoring any error associated with this name. GuidName=GuidValue This will also ignore the GUID by name but only if the value equals the GuidValue. GuidValue should be in registry format. This is the suggested format to use as it will limit the ignore to only the defined case.","title":"IgnoreGuidName"},{"location":"dyn/mu_basecore/pytool/Plugin/GuidCheck/Readme/#ignoreguidvalue","text":"This list allows strings in guid registry format GuidValue . This will remove any entry with this GuidValue from the list of GUIDs therefore ignoring any error associated with this value. GuidValue must be in registry format xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx","title":"IgnoreGuidValue"},{"location":"dyn/mu_basecore/pytool/Plugin/GuidCheck/Readme/#ignorefoldersandfiles","text":"This supports .gitignore file and folder matching strings including wildcards Any folder or file ignored will not be parsed and therefore any GUID defined will be ignored. The plugin will always ignores the following [\"/Build\", \"/Conf\"]","title":"IgnoreFoldersAndFiles"},{"location":"dyn/mu_basecore/pytool/Plugin/GuidCheck/Readme/#ignoreduplicates","text":"This supports strings in the format of GuidName = GuidName = GuidName For the error with the GuidNames to be ignored the list must match completely with what is found during the code scan. For example if there are two GUIDs that are by design equal within the code tree then it should be GuidName = GuidName If instead there are three GUIDs then it must be GuidName = GuidName = GuidName This is the best ignore list to use because it is the most strict and will catch new problems when new conflicts are introduced. There are numerous places in the UEFI specification in which two GUID names are assigned the same value. These names should be set in this ignore list so that they don't cause an error but any additional duplication would still be caught.","title":"IgnoreDuplicates"},{"location":"dyn/mu_basecore/pytool/Plugin/LibraryClassCheck/Readme/","text":"Library Class Check Plugin \u00b6 This CiBuildPlugin scans at all library header files found in the Library folders in all of the package's declared include directories and ensures that all files have a matching LibraryClass declaration in the DEC file for the package. Any missing declarations will cause a failure. Configuration \u00b6 The plugin has a few configuration options to support the UEFI codebase. \"LibraryClassCheck\" : { IgnoreHeaderFile : [], # Ignore a file found on disk IgnoreLibraryClass : [] # Ignore a declaration found in dec file } IgnoreHeaderFile \u00b6 Ignore a file found on disk IgnoreLibraryClass \u00b6 Ignore a declaration found in dec file","title":"Library Class Check"},{"location":"dyn/mu_basecore/pytool/Plugin/LibraryClassCheck/Readme/#library-class-check-plugin","text":"This CiBuildPlugin scans at all library header files found in the Library folders in all of the package's declared include directories and ensures that all files have a matching LibraryClass declaration in the DEC file for the package. Any missing declarations will cause a failure.","title":"Library Class Check Plugin"},{"location":"dyn/mu_basecore/pytool/Plugin/LibraryClassCheck/Readme/#configuration","text":"The plugin has a few configuration options to support the UEFI codebase. \"LibraryClassCheck\" : { IgnoreHeaderFile : [], # Ignore a file found on disk IgnoreLibraryClass : [] # Ignore a declaration found in dec file }","title":"Configuration"},{"location":"dyn/mu_basecore/pytool/Plugin/LibraryClassCheck/Readme/#ignoreheaderfile","text":"Ignore a file found on disk","title":"IgnoreHeaderFile"},{"location":"dyn/mu_basecore/pytool/Plugin/LibraryClassCheck/Readme/#ignorelibraryclass","text":"Ignore a declaration found in dec file","title":"IgnoreLibraryClass"},{"location":"dyn/mu_basecore/pytool/Plugin/SpellCheck/Readme/","text":"Spell Check Plugin \u00b6 This CiBuildPlugin scans all the files in a given package and checks for spelling errors. This plugin requires NodeJs and cspell. If the plugin doesn't find its required tools then it will mark the test as skipped. NodeJS: https://nodejs.org/en/ cspell: https://www.npmjs.com/package/cspell Src and doc available: https://github.com/streetsidesoftware/cspell Configuration \u00b6 The plugin has a few configuration options to support the UEFI codebase. \"SpellCheck\" : { \"AuditOnly\" : False , # If True, log all errors and then mark as skipped \"IgnoreFiles\" : [], # use gitignore syntax to ignore errors in matching files \"ExtendWords\" : [], # words to extend to the dictionary for this package \"IgnoreStandardPaths\" : [], # Standard Plugin defined paths that should be ignore \"AdditionalIncludePaths\" : [] # Additional paths to spell check (wildcards supported) } AuditOnly \u00b6 Boolean - Default is False. If True run the test in an Audit only mode which will log all errors but instead of failing the build it will set the test as skipped. This allows visibility into the failures without breaking the build. IgnoreFiles \u00b6 This supports .gitignore file and folder matching strings including wildcards All files will be parsed regardless but then any spelling errors found within ignored files will not be reported as an error. Errors in ignored files will still be output to the test results as informational comments. ExtendWords \u00b6 This list allows words to be added to the dictionary for the spell checker when this package is tested. These follow the rules of the cspell config words field. IgnoreStandardPaths \u00b6 This plugin by default will check the below standard paths. If the package would like to ignore any of them list that here. [ # C source \"*.c\" , \"*.h\" , # Assembly files \"*.nasm\" , \"*.asm\" , \"*.masm\" , \"*.s\" , # ACPI source language \"*.asl\" , # Edk2 build files \"*.dsc\" , \"*.dec\" , \"*.fdf\" , \"*.inf\" , # Documentation files \"*.md\" , \"*.txt\" ] AdditionalIncludePaths \u00b6 If the package would to add additional path patterns to be included in spellchecking they can be defined here. Other configuration \u00b6 In the cspell.base.json there are numerous other settings configured. There is no support to override these on a per package basis but future features could make this available. One interesting configuration option is minWordLength . Currently it is set to 5 which means all 2,3, and 4 letter words will be ignored. This helps minimize the number of technical acronyms, register names, and other UEFI specific values that must be ignored. False positives \u00b6 The cspell dictionary is not perfect and there are cases where technical words or acronyms are not found in the dictionary. There are three ways to resolve false positives and the choice for which method should be based on how broadly the word should be accepted. CSpell Base Config file \u00b6 If the change should apply to all UEFI code and documentation then it should be added to the base config file words section. The base config file is adjacent to this file and titled cspell.base.json . This is a list of accepted words for all spell checking operations on all packages. Package Config \u00b6 In the package *.ci.yaml file there is a SpellCheck config section. This section allows files to be ignored as well as words that should be considered valid for all files within this package. Add the desired words to the \"ExtendedWords\" member. In-line File \u00b6 CSpell supports numerous methods to annotate your files to ignore words, sections, etc. This can be found in CSpell documentation. Suggestion here is to use a c-style comment at the top of the file to add words that should be ignored just for this file. Obviously this has the highest maintenance cost so it should only be used for file unique words. // spell-checker:ignore unenroll, word2, word3 or # spell-checker:ignore unenroll, word2, word3","title":"Spell Check"},{"location":"dyn/mu_basecore/pytool/Plugin/SpellCheck/Readme/#spell-check-plugin","text":"This CiBuildPlugin scans all the files in a given package and checks for spelling errors. This plugin requires NodeJs and cspell. If the plugin doesn't find its required tools then it will mark the test as skipped. NodeJS: https://nodejs.org/en/ cspell: https://www.npmjs.com/package/cspell Src and doc available: https://github.com/streetsidesoftware/cspell","title":"Spell Check Plugin"},{"location":"dyn/mu_basecore/pytool/Plugin/SpellCheck/Readme/#configuration","text":"The plugin has a few configuration options to support the UEFI codebase. \"SpellCheck\" : { \"AuditOnly\" : False , # If True, log all errors and then mark as skipped \"IgnoreFiles\" : [], # use gitignore syntax to ignore errors in matching files \"ExtendWords\" : [], # words to extend to the dictionary for this package \"IgnoreStandardPaths\" : [], # Standard Plugin defined paths that should be ignore \"AdditionalIncludePaths\" : [] # Additional paths to spell check (wildcards supported) }","title":"Configuration"},{"location":"dyn/mu_basecore/pytool/Plugin/SpellCheck/Readme/#auditonly","text":"Boolean - Default is False. If True run the test in an Audit only mode which will log all errors but instead of failing the build it will set the test as skipped. This allows visibility into the failures without breaking the build.","title":"AuditOnly"},{"location":"dyn/mu_basecore/pytool/Plugin/SpellCheck/Readme/#ignorefiles","text":"This supports .gitignore file and folder matching strings including wildcards All files will be parsed regardless but then any spelling errors found within ignored files will not be reported as an error. Errors in ignored files will still be output to the test results as informational comments.","title":"IgnoreFiles"},{"location":"dyn/mu_basecore/pytool/Plugin/SpellCheck/Readme/#extendwords","text":"This list allows words to be added to the dictionary for the spell checker when this package is tested. These follow the rules of the cspell config words field.","title":"ExtendWords"},{"location":"dyn/mu_basecore/pytool/Plugin/SpellCheck/Readme/#ignorestandardpaths","text":"This plugin by default will check the below standard paths. If the package would like to ignore any of them list that here. [ # C source \"*.c\" , \"*.h\" , # Assembly files \"*.nasm\" , \"*.asm\" , \"*.masm\" , \"*.s\" , # ACPI source language \"*.asl\" , # Edk2 build files \"*.dsc\" , \"*.dec\" , \"*.fdf\" , \"*.inf\" , # Documentation files \"*.md\" , \"*.txt\" ]","title":"IgnoreStandardPaths"},{"location":"dyn/mu_basecore/pytool/Plugin/SpellCheck/Readme/#additionalincludepaths","text":"If the package would to add additional path patterns to be included in spellchecking they can be defined here.","title":"AdditionalIncludePaths"},{"location":"dyn/mu_basecore/pytool/Plugin/SpellCheck/Readme/#other-configuration","text":"In the cspell.base.json there are numerous other settings configured. There is no support to override these on a per package basis but future features could make this available. One interesting configuration option is minWordLength . Currently it is set to 5 which means all 2,3, and 4 letter words will be ignored. This helps minimize the number of technical acronyms, register names, and other UEFI specific values that must be ignored.","title":"Other configuration"},{"location":"dyn/mu_basecore/pytool/Plugin/SpellCheck/Readme/#false-positives","text":"The cspell dictionary is not perfect and there are cases where technical words or acronyms are not found in the dictionary. There are three ways to resolve false positives and the choice for which method should be based on how broadly the word should be accepted.","title":"False positives"},{"location":"dyn/mu_basecore/pytool/Plugin/SpellCheck/Readme/#cspell-base-config-file","text":"If the change should apply to all UEFI code and documentation then it should be added to the base config file words section. The base config file is adjacent to this file and titled cspell.base.json . This is a list of accepted words for all spell checking operations on all packages.","title":"CSpell Base Config file"},{"location":"dyn/mu_basecore/pytool/Plugin/SpellCheck/Readme/#package-config","text":"In the package *.ci.yaml file there is a SpellCheck config section. This section allows files to be ignored as well as words that should be considered valid for all files within this package. Add the desired words to the \"ExtendedWords\" member.","title":"Package Config"},{"location":"dyn/mu_basecore/pytool/Plugin/SpellCheck/Readme/#in-line-file","text":"CSpell supports numerous methods to annotate your files to ignore words, sections, etc. This can be found in CSpell documentation. Suggestion here is to use a c-style comment at the top of the file to add words that should be ignored just for this file. Obviously this has the highest maintenance cost so it should only be used for file unique words. // spell-checker:ignore unenroll, word2, word3 or # spell-checker:ignore unenroll, word2, word3","title":"In-line File"},{"location":"dyn/mu_oem_sample/RepoDetails/","text":"Project Mu Oem Sample Repository \u00b6 Git Details Repository Url: https://github.com/Microsoft/mu_oem_sample.git Branch: release/201911 Commit: f623b64605a8e82e54739408a119dc4574d6b063 Commit Date: 2019-12-07 18:43:37 +0000 This repository is considered sample code for any entity building devices using Project Mu. It is likely that any device manufacturer will want to customize the device behavior by changing the modules in this package. Numerous libraries to support UEFI Boot Device Selection phase (BDS) Firmware Version information UI App / \"Frontpage\" application support as well as example More Info \u00b6 FrontpageDsc and FrontpageFdf that can be included so you don't have to unravel all of the libraries and protocols that are required to get started with FrontPage. A brief description of each MS component was added to FrontPageDsc to help explain how each piece of the puzzle fits in. Please see the Project Mu docs ( https://github.com/Microsoft/mu ) for more information. This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments. Per Platform Libraries \u00b6 MsPlatformDevicesLib, DfciDeviceIdSupportLib, PlatformThemeLib. These three libraries need to be implemented per platform. An example can be found in the NXP iMX8 platform . Issues \u00b6 Please open any issues in the Project Mu GitHub tracker. More Details Contributing Code or Docs \u00b6 Please follow the general Project Mu Pull Request process. More Details Code Requirements Doc Requirements PR-Gate Builds \u00b6 pip install --upgrade -r requirements.txt mu_build -c corebuild.mu.json Copyright & License \u00b6 Copyright \u00a9 2016-2018, Microsoft Corporation All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"Repo Details"},{"location":"dyn/mu_oem_sample/RepoDetails/#project-mu-oem-sample-repository","text":"Git Details Repository Url: https://github.com/Microsoft/mu_oem_sample.git Branch: release/201911 Commit: f623b64605a8e82e54739408a119dc4574d6b063 Commit Date: 2019-12-07 18:43:37 +0000 This repository is considered sample code for any entity building devices using Project Mu. It is likely that any device manufacturer will want to customize the device behavior by changing the modules in this package. Numerous libraries to support UEFI Boot Device Selection phase (BDS) Firmware Version information UI App / \"Frontpage\" application support as well as example","title":"Project Mu Oem Sample Repository"},{"location":"dyn/mu_oem_sample/RepoDetails/#more-info","text":"FrontpageDsc and FrontpageFdf that can be included so you don't have to unravel all of the libraries and protocols that are required to get started with FrontPage. A brief description of each MS component was added to FrontPageDsc to help explain how each piece of the puzzle fits in. Please see the Project Mu docs ( https://github.com/Microsoft/mu ) for more information. This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.","title":"More Info"},{"location":"dyn/mu_oem_sample/RepoDetails/#per-platform-libraries","text":"MsPlatformDevicesLib, DfciDeviceIdSupportLib, PlatformThemeLib. These three libraries need to be implemented per platform. An example can be found in the NXP iMX8 platform .","title":"Per Platform Libraries"},{"location":"dyn/mu_oem_sample/RepoDetails/#issues","text":"Please open any issues in the Project Mu GitHub tracker. More Details","title":"Issues"},{"location":"dyn/mu_oem_sample/RepoDetails/#contributing-code-or-docs","text":"Please follow the general Project Mu Pull Request process. More Details Code Requirements Doc Requirements","title":"Contributing Code or Docs"},{"location":"dyn/mu_oem_sample/RepoDetails/#pr-gate-builds","text":"pip install --upgrade -r requirements.txt mu_build -c corebuild.mu.json","title":"PR-Gate Builds"},{"location":"dyn/mu_oem_sample/RepoDetails/#copyright-license","text":"Copyright \u00a9 2016-2018, Microsoft Corporation All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"Copyright &amp; License"},{"location":"dyn/mu_oem_sample/OemPkg/FrontPage/FrontPagePasswordSupport/","text":"FrontPage Password Support \u00b6 Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent About \u00b6 OemPkg FrontPage application is a sample UEFI UI app that among other features supports password authentication. Password Architecture \u00b6 Below architecture diagram shows how the password support is integrated into the FrontPage application. Setting a Password \u00b6 FrontPage links to PasswordPolicyLib in order to validate the user-provided password and then to create a hash out of the password. Then, the Settings Access Protocol is used to save the hash. DfciPasswordProvider is linked to DfciPkg SettingsManagerDxe to register the password setting with the Settings Access Protocol. DfciPasswordProvider links to PasswordStoreLib to set the password via PasswordStoreSetPassword. Authenticating a Password \u00b6 FrontPage uses the DFCI Authentication Protocol to authenticate a password via the AuthWithPW interface and acquire an authentication token. DFCI Authentication Protocol is installed by DFCI Identity and Auth Manager, which uses PasswordStoreLib to check whether a password is set, and if set, then to authenticate the password.","title":"Front Page"},{"location":"dyn/mu_oem_sample/OemPkg/FrontPage/FrontPagePasswordSupport/#frontpage-password-support","text":"","title":"FrontPage Password Support"},{"location":"dyn/mu_oem_sample/OemPkg/FrontPage/FrontPagePasswordSupport/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_oem_sample/OemPkg/FrontPage/FrontPagePasswordSupport/#about","text":"OemPkg FrontPage application is a sample UEFI UI app that among other features supports password authentication.","title":"About"},{"location":"dyn/mu_oem_sample/OemPkg/FrontPage/FrontPagePasswordSupport/#password-architecture","text":"Below architecture diagram shows how the password support is integrated into the FrontPage application.","title":"Password Architecture"},{"location":"dyn/mu_oem_sample/OemPkg/FrontPage/FrontPagePasswordSupport/#setting-a-password","text":"FrontPage links to PasswordPolicyLib in order to validate the user-provided password and then to create a hash out of the password. Then, the Settings Access Protocol is used to save the hash. DfciPasswordProvider is linked to DfciPkg SettingsManagerDxe to register the password setting with the Settings Access Protocol. DfciPasswordProvider links to PasswordStoreLib to set the password via PasswordStoreSetPassword.","title":"Setting a Password"},{"location":"dyn/mu_oem_sample/OemPkg/FrontPage/FrontPagePasswordSupport/#authenticating-a-password","text":"FrontPage uses the DFCI Authentication Protocol to authenticate a password via the AuthWithPW interface and acquire an authentication token. DFCI Authentication Protocol is installed by DFCI Identity and Auth Manager, which uses PasswordStoreLib to check whether a password is set, and if set, then to authenticate the password.","title":"Authenticating a Password"},{"location":"dyn/mu_pip_build/RepoDetails/","text":"Project Mu Pip Build \u00b6 Git Details Repository Url: https://github.com/Microsoft/mu_pip_build.git Branch: master Commit: 7a42e81ef6f1a689dd7ac093e6a5db8e4aea95e2 Commit Date: 2019-12-10 20:01:30 -0800 THIS PROJECT IS NO LONGER ACTIVE - ACTIVE WORK HAS MOVED TO https://github.com/tianocore/edk2-pytool-extensions \u00b6 Provided with config file, mu_build fetches/clones dependencies then compiles every module in each package. This is the entrypoint into the CI / Pull Request build and test infrastructure. More Info \u00b6 Please see the Project Mu docs ( https://github.com/Microsoft/mu ) for more information. This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments. Issues \u00b6 Please open any issues in the Project Mu GitHub tracker. More Details Contributing Code or Docs \u00b6 Please follow the general Project Mu Pull Request process. More Details Additionally make sure all testing described in the \"Development\" section passes. Using \u00b6 Usage Details Development \u00b6 Development Details Publish \u00b6 Publish Details Copyright & License \u00b6 Copyright \u00a9 2016-2018, Microsoft Corporation All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"Repo Details"},{"location":"dyn/mu_pip_build/RepoDetails/#project-mu-pip-build","text":"Git Details Repository Url: https://github.com/Microsoft/mu_pip_build.git Branch: master Commit: 7a42e81ef6f1a689dd7ac093e6a5db8e4aea95e2 Commit Date: 2019-12-10 20:01:30 -0800","title":"Project Mu Pip Build"},{"location":"dyn/mu_pip_build/RepoDetails/#this-project-is-no-longer-active-active-work-has-moved-to-httpsgithubcomtianocoreedk2-pytool-extensions","text":"Provided with config file, mu_build fetches/clones dependencies then compiles every module in each package. This is the entrypoint into the CI / Pull Request build and test infrastructure.","title":"THIS PROJECT IS NO LONGER ACTIVE - ACTIVE WORK HAS MOVED TO https://github.com/tianocore/edk2-pytool-extensions"},{"location":"dyn/mu_pip_build/RepoDetails/#more-info","text":"Please see the Project Mu docs ( https://github.com/Microsoft/mu ) for more information. This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.","title":"More Info"},{"location":"dyn/mu_pip_build/RepoDetails/#issues","text":"Please open any issues in the Project Mu GitHub tracker. More Details","title":"Issues"},{"location":"dyn/mu_pip_build/RepoDetails/#contributing-code-or-docs","text":"Please follow the general Project Mu Pull Request process. More Details Additionally make sure all testing described in the \"Development\" section passes.","title":"Contributing Code or Docs"},{"location":"dyn/mu_pip_build/RepoDetails/#using","text":"Usage Details","title":"Using"},{"location":"dyn/mu_pip_build/RepoDetails/#development","text":"Development Details","title":"Development"},{"location":"dyn/mu_pip_build/RepoDetails/#publish","text":"Publish Details","title":"Publish"},{"location":"dyn/mu_pip_build/RepoDetails/#copyright-license","text":"Copyright \u00a9 2016-2018, Microsoft Corporation All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"Copyright &amp; License"},{"location":"dyn/mu_pip_build/developing/","text":"Developing Project Mu Pip Build \u00b6 Pre-Requisites \u00b6 Get the code git clone https://github.com/Microsoft/mu_pip_build.git Install development dependencies pip install --upgrade -r requirements.txt Uninstall any copy of mu_build pip uninstall mu_build Install from local source (run command from root of repo) pip install -e . Testing \u00b6 Run a Basic Syntax/Lint Check (using flake8) and resolve any issues flake8 MuBuild Info Newer editors are very helpful in resolving source formatting errors (whitespace, indentation, etc). In VSCode open the py file and use ++alt+shift+f++ to auto format. Run pytest with coverage data collected pytest -v --junitxml=test.junit.xml --html=pytest_MuBuild_report.html --self-contained-html --cov=MuBuild --cov-report html:cov_html --cov-report xml:cov.xml --cov-config .coveragerc Look at the reports pytest_MuBuild_report.html cov_html/index.html","title":"developing"},{"location":"dyn/mu_pip_build/developing/#developing-project-mu-pip-build","text":"","title":"Developing Project Mu Pip Build"},{"location":"dyn/mu_pip_build/developing/#pre-requisites","text":"Get the code git clone https://github.com/Microsoft/mu_pip_build.git Install development dependencies pip install --upgrade -r requirements.txt Uninstall any copy of mu_build pip uninstall mu_build Install from local source (run command from root of repo) pip install -e .","title":"Pre-Requisites"},{"location":"dyn/mu_pip_build/developing/#testing","text":"Run a Basic Syntax/Lint Check (using flake8) and resolve any issues flake8 MuBuild Info Newer editors are very helpful in resolving source formatting errors (whitespace, indentation, etc). In VSCode open the py file and use ++alt+shift+f++ to auto format. Run pytest with coverage data collected pytest -v --junitxml=test.junit.xml --html=pytest_MuBuild_report.html --self-contained-html --cov=MuBuild --cov-report html:cov_html --cov-report xml:cov.xml --cov-config .coveragerc Look at the reports pytest_MuBuild_report.html cov_html/index.html","title":"Testing"},{"location":"dyn/mu_pip_build/publishing/","text":"Publishing Project Mu Pip Build \u00b6 The MuBuild is published as a pypi (pip) module. The pip module is named mu_build . Pypi allows for easy version management, dependency management, and sharing. Publishing/releasing a new version is generally handled thru a server based build process but for completeness the process is documented here. Steps \u00b6 Info These directions assume you have already configured your workspace for developing. If not please first do that. Directions on the developing page. Pass all development tests and check. Update the readme with info on changes for this version. Get your changes into master branch (official releases should only be done from the master branch) Make a git tag for the version that will be released. Tag format is v . . Do the release process (Use the server process for this but for documentation sake these are the steps) Install tools pip install --upgrade -r requirements.publisher.txt Build a wheel python setup.py sdist bdist_wheel Confirm wheel version is aligned with git tag ConfirmVersionAndTag.py Publish the wheel/distribution to pypi twine upload dist/*","title":"publishing"},{"location":"dyn/mu_pip_build/publishing/#publishing-project-mu-pip-build","text":"The MuBuild is published as a pypi (pip) module. The pip module is named mu_build . Pypi allows for easy version management, dependency management, and sharing. Publishing/releasing a new version is generally handled thru a server based build process but for completeness the process is documented here.","title":"Publishing Project Mu Pip Build"},{"location":"dyn/mu_pip_build/publishing/#steps","text":"Info These directions assume you have already configured your workspace for developing. If not please first do that. Directions on the developing page. Pass all development tests and check. Update the readme with info on changes for this version. Get your changes into master branch (official releases should only be done from the master branch) Make a git tag for the version that will be released. Tag format is v . . Do the release process (Use the server process for this but for documentation sake these are the steps) Install tools pip install --upgrade -r requirements.publisher.txt Build a wheel python setup.py sdist bdist_wheel Confirm wheel version is aligned with git tag ConfirmVersionAndTag.py Publish the wheel/distribution to pypi twine upload dist/*","title":"Steps"},{"location":"dyn/mu_pip_build/using/","text":"Using Project Mu Pip Build \u00b6 Install from pip pip install mu_build Usage Docs \u00b6 TBD","title":"using"},{"location":"dyn/mu_pip_build/using/#using-project-mu-pip-build","text":"Install from pip pip install mu_build","title":"Using Project Mu Pip Build"},{"location":"dyn/mu_pip_build/using/#usage-docs","text":"TBD","title":"Usage Docs"},{"location":"dyn/mu_pip_environment/RepoDetails/","text":"Project Mu Pip Environment \u00b6 Git Details Repository Url: https://github.com/Microsoft/mu_pip_environment.git Branch: master Commit: d62df3b1d86bf375e82a7ca09740fa0aa3504fcc Commit Date: 2019-12-10 20:02:51 -0800 THIS PROJECT IS NO LONGER ACTIVE - ACTIVE WORK HAS MOVED TO https://github.com/tianocore/edk2-pytool-extensions \u00b6 Entry point into Self Describing Environment (SDE). Sets up and parses state of workspace before calling into build. More Info \u00b6 Please see the Project Mu docs ( https://github.com/Microsoft/mu ) for more information. This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments. Issues \u00b6 Please open any issues in the Project Mu GitHub tracker. More Details Contributing Code or Docs \u00b6 Please follow the general Project Mu Pull Request process. More Details Additionally make sure all testing described in the \"Development\" section passes. Using \u00b6 Usage Details Development \u00b6 Development Details Publish \u00b6 Publish Details Copyright & License \u00b6 Copyright \u00a9 2018, Microsoft Corporation All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"Repo Details"},{"location":"dyn/mu_pip_environment/RepoDetails/#project-mu-pip-environment","text":"Git Details Repository Url: https://github.com/Microsoft/mu_pip_environment.git Branch: master Commit: d62df3b1d86bf375e82a7ca09740fa0aa3504fcc Commit Date: 2019-12-10 20:02:51 -0800","title":"Project Mu Pip Environment"},{"location":"dyn/mu_pip_environment/RepoDetails/#this-project-is-no-longer-active-active-work-has-moved-to-httpsgithubcomtianocoreedk2-pytool-extensions","text":"Entry point into Self Describing Environment (SDE). Sets up and parses state of workspace before calling into build.","title":"THIS PROJECT IS NO LONGER ACTIVE - ACTIVE WORK HAS MOVED TO https://github.com/tianocore/edk2-pytool-extensions"},{"location":"dyn/mu_pip_environment/RepoDetails/#more-info","text":"Please see the Project Mu docs ( https://github.com/Microsoft/mu ) for more information. This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.","title":"More Info"},{"location":"dyn/mu_pip_environment/RepoDetails/#issues","text":"Please open any issues in the Project Mu GitHub tracker. More Details","title":"Issues"},{"location":"dyn/mu_pip_environment/RepoDetails/#contributing-code-or-docs","text":"Please follow the general Project Mu Pull Request process. More Details Additionally make sure all testing described in the \"Development\" section passes.","title":"Contributing Code or Docs"},{"location":"dyn/mu_pip_environment/RepoDetails/#using","text":"Usage Details","title":"Using"},{"location":"dyn/mu_pip_environment/RepoDetails/#development","text":"Development Details","title":"Development"},{"location":"dyn/mu_pip_environment/RepoDetails/#publish","text":"Publish Details","title":"Publish"},{"location":"dyn/mu_pip_environment/RepoDetails/#copyright-license","text":"Copyright \u00a9 2018, Microsoft Corporation All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"Copyright &amp; License"},{"location":"dyn/mu_pip_environment/developing/","text":"Developing Project Mu Pip Environment \u00b6 Pre-Requisites \u00b6 Get the code git clone https://github.com/Microsoft/mu_pip_environment.git Install development dependencies pip install --upgrade -r requirements.txt Uninstall any copy of mu_environment pip uninstall mu_environment Install from local source (run command from root of repo) pip install -e . Testing \u00b6 Run a Basic Syntax/Lint Check (using flake8) and resolve any issues flake8 MuEnvironment Info Newer editors are very helpful in resolving source formatting errors (whitespace, indentation, etc). In VSCode open the py file and use ++alt+shift+f++ to auto format. Run pytest with coverage data collected pytest -v --junitxml=test.junit.xml --html=pytest_MuEnvironment_report.html --self-contained-html --cov=MuEnvironment --cov-report html:cov_html --cov-report xml:cov.xml --cov-config .coveragerc Look at the reports pytest_MuEnvironment_report.html cov_html/index.html","title":"developing"},{"location":"dyn/mu_pip_environment/developing/#developing-project-mu-pip-environment","text":"","title":"Developing Project Mu Pip Environment"},{"location":"dyn/mu_pip_environment/developing/#pre-requisites","text":"Get the code git clone https://github.com/Microsoft/mu_pip_environment.git Install development dependencies pip install --upgrade -r requirements.txt Uninstall any copy of mu_environment pip uninstall mu_environment Install from local source (run command from root of repo) pip install -e .","title":"Pre-Requisites"},{"location":"dyn/mu_pip_environment/developing/#testing","text":"Run a Basic Syntax/Lint Check (using flake8) and resolve any issues flake8 MuEnvironment Info Newer editors are very helpful in resolving source formatting errors (whitespace, indentation, etc). In VSCode open the py file and use ++alt+shift+f++ to auto format. Run pytest with coverage data collected pytest -v --junitxml=test.junit.xml --html=pytest_MuEnvironment_report.html --self-contained-html --cov=MuEnvironment --cov-report html:cov_html --cov-report xml:cov.xml --cov-config .coveragerc Look at the reports pytest_MuEnvironment_report.html cov_html/index.html","title":"Testing"},{"location":"dyn/mu_pip_environment/publishing/","text":"Publishing Project Mu Pip Environment \u00b6 The MuEnvironment is published as a pypi (pip) module. The pip module is named mu_environment . Pypi allows for easy version management, dependency management, and sharing. Publishing/releasing a new version is generally handled thru a server based build process but for completeness the process is documented here. Steps \u00b6 Info These directions assume you have already configured your workspace for developing. If not please first do that. Directions on the developing page. Pass all development tests and check. Update the readme with info on changes for this version. Get your changes into master branch (official releases should only be done from the master branch) Make a git tag for the version that will be released. Tag format is v . . Do the release process Install tools pip install --upgrade -r requirements.publisher.txt Build a wheel python setup.py sdist bdist_wheel Confirm wheel version is aligned with git tag ConfirmVersionAndTag.py Publish the wheel/distribution to pypi twine upload dist/*","title":"publishing"},{"location":"dyn/mu_pip_environment/publishing/#publishing-project-mu-pip-environment","text":"The MuEnvironment is published as a pypi (pip) module. The pip module is named mu_environment . Pypi allows for easy version management, dependency management, and sharing. Publishing/releasing a new version is generally handled thru a server based build process but for completeness the process is documented here.","title":"Publishing Project Mu Pip Environment"},{"location":"dyn/mu_pip_environment/publishing/#steps","text":"Info These directions assume you have already configured your workspace for developing. If not please first do that. Directions on the developing page. Pass all development tests and check. Update the readme with info on changes for this version. Get your changes into master branch (official releases should only be done from the master branch) Make a git tag for the version that will be released. Tag format is v . . Do the release process Install tools pip install --upgrade -r requirements.publisher.txt Build a wheel python setup.py sdist bdist_wheel Confirm wheel version is aligned with git tag ConfirmVersionAndTag.py Publish the wheel/distribution to pypi twine upload dist/*","title":"Steps"},{"location":"dyn/mu_pip_environment/using/","text":"Using Project Mu Pip Environment \u00b6 Install from pip pip install mu_environment Usage Docs \u00b6 TBD","title":"using"},{"location":"dyn/mu_pip_environment/using/#using-project-mu-pip-environment","text":"Install from pip pip install mu_environment","title":"Using Project Mu Pip Environment"},{"location":"dyn/mu_pip_environment/using/#usage-docs","text":"TBD","title":"Usage Docs"},{"location":"dyn/mu_pip_environment/MuEnvironment/bin/readme/","text":"The binary files that will be included with this package \u00b6","title":"bin"},{"location":"dyn/mu_pip_environment/MuEnvironment/bin/readme/#the-binary-files-that-will-be-included-with-this-package","text":"","title":"The binary files that will be included with this package"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_MuLogging/","text":"Mu Logging \u00b6 MuLogging is a collection of utilities to manage logging in Project Mu. There are four different ways to create handlers. 1. setup_txt_logger - a handler that outputs a txt file 2. setup_markdown_logger - a handler that outputs a markdown file with an output file 3. setup_console_logging - a handler that logs to the console with optional colors 4. create_output_stream - a handler that has an in-memory stream that you can later read from setup_logging is a helper function that creates 1-3 of the handlers. The output_stream is used for plugins in mu_build so they can keep track of compiler output General Practice \u00b6 All modules that are not PlatformBuilder or MuBuild should request a named logger like this: logging . getLogger ( \"MuGit\" ) Modules that are not the root module get downgraded a level (ie. critical -> warning)","title":"feature Mu Logging"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_MuLogging/#mu-logging","text":"MuLogging is a collection of utilities to manage logging in Project Mu. There are four different ways to create handlers. 1. setup_txt_logger - a handler that outputs a txt file 2. setup_markdown_logger - a handler that outputs a markdown file with an output file 3. setup_console_logging - a handler that logs to the console with optional colors 4. create_output_stream - a handler that has an in-memory stream that you can later read from setup_logging is a helper function that creates 1-3 of the handlers. The output_stream is used for plugins in mu_build so they can keep track of compiler output","title":"Mu Logging"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_MuLogging/#general-practice","text":"All modules that are not PlatformBuilder or MuBuild should request a named logger like this: logging . getLogger ( \"MuGit\" ) Modules that are not the root module get downgraded a level (ie. critical -> warning)","title":"General Practice"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_extdep/","text":"External Dependencies \u00b6 Overview \u00b6 External dependencies are a way within the build environment to describe external dependencies and have the build system fetch them when doing the setup or update operations. Ext_dep state will also be verified when doing a build to ensure the environment is in the required state prior to building. Ext_deps have solved three major issues for Project Mu. Binaries causing bloat of git repositories Conditional inclusion of a dependency (only for certain usages) Reproducability and tracking of dependencies Why \u00b6 Git Bloat \u00b6 Best practices advise against checking in binaries to git repositories as the overall size of git repos will balloon quickly causing slow clones and slow operations. Building firmware often requires custom tools, firmware blobs, or other binaries and it is critical these are maintained and versioned with the repository. Package management tools can solve the hosting of these binaries but edk2 has no built in tool to track them, extract them, etc. Ext_deps provide that mechanism. Conditional Inclusion (scopes) \u00b6 Ext_deps leverage the environment scope concept so that a repository can carry ext_deps that are only used in some conditions. Scopes are a string that a environment envoking tool uses to indicate what ext_deps should be used. These scopes are loosly based on functionality. Reproducability and Tracking \u00b6 Ext_deps are common infrastructure so that all external dependencies can be handled consistantly. Versions are added to the version report so that for any given operation (like build) a complete list of what was used is available. This makes tracking versions consistant and \"free\". Ext_deps when fetched will update their state. If the repository is updated to include a new ext_dep version the tool will be told the environment state is not valid and can then enforce thet user updates their environment. Examples of Usage \u00b6 Here are a few examples where ext_deps have been found useful: An ext_dep describing a test repository that is only needed when running unit tests. By leveraging scopes this ext_dep is only fetched when the unittest scope is active. Similiar to the unit test dependency, support for CI builds often require unique dependencies. When doing a CI build of a core repository it might have critical dependencies that need to be fetched but when the core repository is included within a platform repository as a dependency, then the core would defer to the platform as to how to include the dependency. An ext_dep describing the compiler toolchain. This ext_dep is only needed when a builder is using that toolchain for that target type. An ext_dep describing some platform binary. This is only needed when building that given platform and since git is not optiized to handle binaries this saves a lot of unnecessary bloat in the repository. Supported Types \u00b6 NuGet Dependency \u00b6 Nuget dependency is used to fetch files from a nuget feed. This feed can be either unauthenticated or authenticated. Support is done by using the nuget command line tool. When the ext_dep type is set to nuget the descriptor will be intrepreted as a nuget dependency. Nuget has a few nice features such as caching, authentication, versioning, and is platform and language agnostic. Web Dependency \u00b6 Web dependency is used to describe a dependency on an asset that can be downloaded via a URL and a web request. It will download whatever is located at the source URL and can support single files, compressed files, and folders. When the ext_dep type is set to web the ext_dep will be intrepreted as a web dependency. Git Dependency \u00b6 Git dependency is used to describe a dependency on a git repository. This repository will be cloned to the ext_dep location and the version will be checked out. For this ext_dep descriptor the type is git . A git dependency should be treated as read-only because the verify and clean phase will do destructive operations where local changes would be destroyed. Developer Note \u00b6 To create a new Dependency type it requires a new subclass of the ExternalDependency class. The subclass needs to have a type field and then factory method in ExternalDependency.py needs to be updated to create new instances of the new type. How they work \u00b6 Ext_deps are found by the SDE (self-describing environment). If you have any questions about that, go review the document for that. Once the ext_dep is found it can be interacted with depending on use case/tool. Objects created with the data from ext_dep descriptors and are subclassed according to the \"type\" field in the descriptor. These objects contain the code for fetching, validating, updating, and cleaning dependency objects and metadata. When referenced from the SDE itself, they can also update paths and other build/shell vars in the build environment. How to create/use an ext_dep \u00b6 An ext_dep is defined by a json file that ends in _ext_dep.json It must follow the schema outlined below. It will be unpacked in a new folder in the same directory as the .json file in a folder named {name}_extdep. We strongly recommend adding any folder that ends in _extdep to your repositories gitignore. It would look like this: *_extdep/ Ext_Dep Example json file { \"scope\" : \"cibuild\" , \"type\" : \"nuget\" , \"name\" : \"iasl\" , \"source\" : \"https://api.nuget.org/v3/index.json\" , \"version\" : \"20190215.0.0\" , \"flags\" : [ \"set_path\" , \"host_specific\" ] } The base schema \u00b6 Required \u00b6 scope: (required) (string) - name of scope when this ext_dep should be evaluated - type: (required) (string from list of known types) - See above for types - name: This is the name of the ext_dep and will be part of the path where the ext_dep is unpacked - source: see per type - version: see per type - flags: Optional conditions that can be applied. Can be empty list Optional \u00b6 id: (string) - Identifier allowing override feature - Must be unique override_id: (string) - Identifier of the ext_dep this should replace (allows for changing an ext_dep in another source by id) var_name: TODO Nuget Type Schema differences \u00b6 source: This should be the nuget feed URL version: nuget version. Generally xx.yy.zz For this type there are zero additional ext_dep fields. Web Type Schema differences \u00b6 source: url to download version: only used for folder naming For this type there are three additional ext_dep fields: internal_path (required) This describes the internal structure of whatever we are downloading . If you are just downloading a file , include the name you would like the file to be . If you are downloading a directory , indicate so with a / before the path . The folder the path points to will have it ' s contents copied into the final name_ext_dep folder . compression_type (optional) Including this field is indicating that the file being downloaded is compressed and that you would like the contents of internal_path to be extracted . If you have a compressed file and would not like it to be decompressed , omit this field . Currently tar and zip files are supported . If the file is not compressed , omit this field . sha256 (optional) If desired, you can provide the hash of your file. This hash will be checked against what is being downloaded to ensure it is valid. It is strongly recommended to use this to ensure the contents are as expected. Git Type Schema Differences \u00b6 source: url of git repo version: commit hash to checkout Experimental Option: url_creds_var \u00b6 If this field is found in the descriptor file when initializing this extdep, the string value listed will be checked against the environment's shell_vars. If a matching var is found, this string in the shell_var will be prepended to the URL host for the source URL. NOTE: This is intended for server builds and may be subject to change as we figure out how it fits into build flows. Also note that any creds passed may end up in build logs and other server-side artifacts. Use with caution! Example: TEST_DESCRIPTOR = { \"scope\" : \"global\" , \"type\" : \"git\" , \"name\" : \"ExampleRepo\" , \"source\" : \"http://example.com/path/to/repo.git\" , \"version\" : \"7fd1a60b01f91b314f59955a4e4d4e80d8edf11d\" , \"url_creds_var\" : 'my_url_creds' \"flags\" : [] } # Populate shell var. env . set_shell_var ( 'my_url_creds' , 'my_user:my_pass' ) # URL cloned by the GitDependency object will look like... final_url = 'http://my_user:my_pass@example.com/path/to/repo.git' The Flags \u00b6 There are specific flags that do different things. Flags are defined by MuEnviroment and cannot be modified without updating the pip module. More information on the flags can be found in the SDE documentation.","title":"feature extdep"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_extdep/#external-dependencies","text":"","title":"External Dependencies"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_extdep/#overview","text":"External dependencies are a way within the build environment to describe external dependencies and have the build system fetch them when doing the setup or update operations. Ext_dep state will also be verified when doing a build to ensure the environment is in the required state prior to building. Ext_deps have solved three major issues for Project Mu. Binaries causing bloat of git repositories Conditional inclusion of a dependency (only for certain usages) Reproducability and tracking of dependencies","title":"Overview"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_extdep/#why","text":"","title":"Why"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_extdep/#git-bloat","text":"Best practices advise against checking in binaries to git repositories as the overall size of git repos will balloon quickly causing slow clones and slow operations. Building firmware often requires custom tools, firmware blobs, or other binaries and it is critical these are maintained and versioned with the repository. Package management tools can solve the hosting of these binaries but edk2 has no built in tool to track them, extract them, etc. Ext_deps provide that mechanism.","title":"Git Bloat"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_extdep/#conditional-inclusion-scopes","text":"Ext_deps leverage the environment scope concept so that a repository can carry ext_deps that are only used in some conditions. Scopes are a string that a environment envoking tool uses to indicate what ext_deps should be used. These scopes are loosly based on functionality.","title":"Conditional Inclusion (scopes)"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_extdep/#reproducability-and-tracking","text":"Ext_deps are common infrastructure so that all external dependencies can be handled consistantly. Versions are added to the version report so that for any given operation (like build) a complete list of what was used is available. This makes tracking versions consistant and \"free\". Ext_deps when fetched will update their state. If the repository is updated to include a new ext_dep version the tool will be told the environment state is not valid and can then enforce thet user updates their environment.","title":"Reproducability and Tracking"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_extdep/#examples-of-usage","text":"Here are a few examples where ext_deps have been found useful: An ext_dep describing a test repository that is only needed when running unit tests. By leveraging scopes this ext_dep is only fetched when the unittest scope is active. Similiar to the unit test dependency, support for CI builds often require unique dependencies. When doing a CI build of a core repository it might have critical dependencies that need to be fetched but when the core repository is included within a platform repository as a dependency, then the core would defer to the platform as to how to include the dependency. An ext_dep describing the compiler toolchain. This ext_dep is only needed when a builder is using that toolchain for that target type. An ext_dep describing some platform binary. This is only needed when building that given platform and since git is not optiized to handle binaries this saves a lot of unnecessary bloat in the repository.","title":"Examples of Usage"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_extdep/#supported-types","text":"","title":"Supported Types"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_extdep/#nuget-dependency","text":"Nuget dependency is used to fetch files from a nuget feed. This feed can be either unauthenticated or authenticated. Support is done by using the nuget command line tool. When the ext_dep type is set to nuget the descriptor will be intrepreted as a nuget dependency. Nuget has a few nice features such as caching, authentication, versioning, and is platform and language agnostic.","title":"NuGet Dependency"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_extdep/#web-dependency","text":"Web dependency is used to describe a dependency on an asset that can be downloaded via a URL and a web request. It will download whatever is located at the source URL and can support single files, compressed files, and folders. When the ext_dep type is set to web the ext_dep will be intrepreted as a web dependency.","title":"Web Dependency"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_extdep/#git-dependency","text":"Git dependency is used to describe a dependency on a git repository. This repository will be cloned to the ext_dep location and the version will be checked out. For this ext_dep descriptor the type is git . A git dependency should be treated as read-only because the verify and clean phase will do destructive operations where local changes would be destroyed.","title":"Git Dependency"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_extdep/#developer-note","text":"To create a new Dependency type it requires a new subclass of the ExternalDependency class. The subclass needs to have a type field and then factory method in ExternalDependency.py needs to be updated to create new instances of the new type.","title":"Developer Note"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_extdep/#how-they-work","text":"Ext_deps are found by the SDE (self-describing environment). If you have any questions about that, go review the document for that. Once the ext_dep is found it can be interacted with depending on use case/tool. Objects created with the data from ext_dep descriptors and are subclassed according to the \"type\" field in the descriptor. These objects contain the code for fetching, validating, updating, and cleaning dependency objects and metadata. When referenced from the SDE itself, they can also update paths and other build/shell vars in the build environment.","title":"How they work"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_extdep/#how-to-createuse-an-ext_dep","text":"An ext_dep is defined by a json file that ends in _ext_dep.json It must follow the schema outlined below. It will be unpacked in a new folder in the same directory as the .json file in a folder named {name}_extdep. We strongly recommend adding any folder that ends in _extdep to your repositories gitignore. It would look like this: *_extdep/ Ext_Dep Example json file { \"scope\" : \"cibuild\" , \"type\" : \"nuget\" , \"name\" : \"iasl\" , \"source\" : \"https://api.nuget.org/v3/index.json\" , \"version\" : \"20190215.0.0\" , \"flags\" : [ \"set_path\" , \"host_specific\" ] }","title":"How to create/use an ext_dep"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_extdep/#the-base-schema","text":"","title":"The base schema"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_extdep/#required","text":"scope: (required) (string) - name of scope when this ext_dep should be evaluated - type: (required) (string from list of known types) - See above for types - name: This is the name of the ext_dep and will be part of the path where the ext_dep is unpacked - source: see per type - version: see per type - flags: Optional conditions that can be applied. Can be empty list","title":"Required"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_extdep/#optional","text":"id: (string) - Identifier allowing override feature - Must be unique override_id: (string) - Identifier of the ext_dep this should replace (allows for changing an ext_dep in another source by id) var_name: TODO","title":"Optional"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_extdep/#nuget-type-schema-differences","text":"source: This should be the nuget feed URL version: nuget version. Generally xx.yy.zz For this type there are zero additional ext_dep fields.","title":"Nuget Type Schema differences"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_extdep/#web-type-schema-differences","text":"source: url to download version: only used for folder naming For this type there are three additional ext_dep fields: internal_path (required) This describes the internal structure of whatever we are downloading . If you are just downloading a file , include the name you would like the file to be . If you are downloading a directory , indicate so with a / before the path . The folder the path points to will have it ' s contents copied into the final name_ext_dep folder . compression_type (optional) Including this field is indicating that the file being downloaded is compressed and that you would like the contents of internal_path to be extracted . If you have a compressed file and would not like it to be decompressed , omit this field . Currently tar and zip files are supported . If the file is not compressed , omit this field . sha256 (optional) If desired, you can provide the hash of your file. This hash will be checked against what is being downloaded to ensure it is valid. It is strongly recommended to use this to ensure the contents are as expected.","title":"Web Type Schema differences"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_extdep/#git-type-schema-differences","text":"source: url of git repo version: commit hash to checkout","title":"Git Type Schema Differences"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_extdep/#experimental-option-url_creds_var","text":"If this field is found in the descriptor file when initializing this extdep, the string value listed will be checked against the environment's shell_vars. If a matching var is found, this string in the shell_var will be prepended to the URL host for the source URL. NOTE: This is intended for server builds and may be subject to change as we figure out how it fits into build flows. Also note that any creds passed may end up in build logs and other server-side artifacts. Use with caution! Example: TEST_DESCRIPTOR = { \"scope\" : \"global\" , \"type\" : \"git\" , \"name\" : \"ExampleRepo\" , \"source\" : \"http://example.com/path/to/repo.git\" , \"version\" : \"7fd1a60b01f91b314f59955a4e4d4e80d8edf11d\" , \"url_creds_var\" : 'my_url_creds' \"flags\" : [] } # Populate shell var. env . set_shell_var ( 'my_url_creds' , 'my_user:my_pass' ) # URL cloned by the GitDependency object will look like... final_url = 'http://my_user:my_pass@example.com/path/to/repo.git'","title":"Experimental Option: url_creds_var"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_extdep/#the-flags","text":"There are specific flags that do different things. Flags are defined by MuEnviroment and cannot be modified without updating the pip module. More information on the flags can be found in the SDE documentation.","title":"The Flags"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_nugetpublishing/","text":"NugetPublishing \u00b6 Tool to help create and publish nuget packages for Project Mu resources Usage \u00b6 See NugetPublishing -h OPTIONAL: host_specific folders \u00b6 The possible different setups for the host are: OS: Linux, Windows, Java Architecture: x86 or ARM Highest Order Bit: 32 or 64 Before the path to the NuGet package contents is published, the Python environment can look inside at several sub-folders and decide which one to use based on the Host OS, highest order bit available, and the architecture of the processor. To do so, add \"host_specific\" to your flags like so: \"flags\": [\"host_specific\"], If this flag is present, the environment will make a list possible sub-folders that would be acceptable for the host machine. For this example, a 64 bit Windows machine with an x86 processor was used: Windows-x86-64 Windows-x86 Windows-64 x86-64 Windows x86 64 The environment will look for these folders, following this order, and select the first one it finds. If none are found, the flag will be ignored. Authentication \u00b6 For publishing most service providers require authentication. The --ApiKey parameter allows the caller to supply a unique key for authorization. There are numerous ways to authenticate. For example Azure Dev Ops: VSTS credential manager. In an interactive session a dialog will popup for the user to login Tokens can also be used as the API key. Go to your account page to generate a token that can push packages NuGet.org Must use an API key. Go to your account page and generate a key. Pushing to an Authenticated Stream \u00b6 Previously the VsCredentialProvider was packaged right next to Nuget.exe and it was automatically found. If you have a specific credential provider executable needed to push to your stream, you'll need to follow the instructions here to make the executable available to find. You can add it to %LocalAppData%\\NuGet\\CredentialProvider or you can add an environmental variable NUGET_CREDENTIALPROVIDERS_PATH with the location of your provider. If you have multiple, they can be semicolon seperated. Example: Creating new config file for first use \u00b6 This will create the config files and place them in the current directory: NugetPublishing --Operation New --Name iasl --Author ProjectMu --ConfigFileFolderPath . --Description \"Description of item.\" --FeedUrl https://api.nuget.org/v3/index.json --ProjectUrl http://aka.ms/projectmu --LicenseType BSD2 For help run: NugetPublishing --Operation New --help Example: Publishing new version of tool \u00b6 Using an existing config file publish a new iasl.exe. See the example file iasl.config.json Download version from acpica.org Unzip Make a new folder (for my example I will call it \"new\") Copy the assets to publish into this new folder (in this case just iasl.exe) Run the iasl.exe -v command to see the version. Open cmd prompt in the NugetPublishing dir Pack and push (here is my example command. ) NugetPublishing --Operation PackAndPush --ConfigFilePath iasl.config.json --Version 20180209.0.0 --InputFolderPath \"C:\\temp\\iasl-win-20180209\\new\" --ApiKey <your key here>","title":"feature nugetpublishing"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_nugetpublishing/#nugetpublishing","text":"Tool to help create and publish nuget packages for Project Mu resources","title":"NugetPublishing"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_nugetpublishing/#usage","text":"See NugetPublishing -h","title":"Usage"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_nugetpublishing/#optional-host_specific-folders","text":"The possible different setups for the host are: OS: Linux, Windows, Java Architecture: x86 or ARM Highest Order Bit: 32 or 64 Before the path to the NuGet package contents is published, the Python environment can look inside at several sub-folders and decide which one to use based on the Host OS, highest order bit available, and the architecture of the processor. To do so, add \"host_specific\" to your flags like so: \"flags\": [\"host_specific\"], If this flag is present, the environment will make a list possible sub-folders that would be acceptable for the host machine. For this example, a 64 bit Windows machine with an x86 processor was used: Windows-x86-64 Windows-x86 Windows-64 x86-64 Windows x86 64 The environment will look for these folders, following this order, and select the first one it finds. If none are found, the flag will be ignored.","title":"OPTIONAL: host_specific folders"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_nugetpublishing/#authentication","text":"For publishing most service providers require authentication. The --ApiKey parameter allows the caller to supply a unique key for authorization. There are numerous ways to authenticate. For example Azure Dev Ops: VSTS credential manager. In an interactive session a dialog will popup for the user to login Tokens can also be used as the API key. Go to your account page to generate a token that can push packages NuGet.org Must use an API key. Go to your account page and generate a key.","title":"Authentication"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_nugetpublishing/#pushing-to-an-authenticated-stream","text":"Previously the VsCredentialProvider was packaged right next to Nuget.exe and it was automatically found. If you have a specific credential provider executable needed to push to your stream, you'll need to follow the instructions here to make the executable available to find. You can add it to %LocalAppData%\\NuGet\\CredentialProvider or you can add an environmental variable NUGET_CREDENTIALPROVIDERS_PATH with the location of your provider. If you have multiple, they can be semicolon seperated.","title":"Pushing to an Authenticated Stream"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_nugetpublishing/#example-creating-new-config-file-for-first-use","text":"This will create the config files and place them in the current directory: NugetPublishing --Operation New --Name iasl --Author ProjectMu --ConfigFileFolderPath . --Description \"Description of item.\" --FeedUrl https://api.nuget.org/v3/index.json --ProjectUrl http://aka.ms/projectmu --LicenseType BSD2 For help run: NugetPublishing --Operation New --help","title":"Example: Creating new config file for first use"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_nugetpublishing/#example-publishing-new-version-of-tool","text":"Using an existing config file publish a new iasl.exe. See the example file iasl.config.json Download version from acpica.org Unzip Make a new folder (for my example I will call it \"new\") Copy the assets to publish into this new folder (in this case just iasl.exe) Run the iasl.exe -v command to see the version. Open cmd prompt in the NugetPublishing dir Pack and push (here is my example command. ) NugetPublishing --Operation PackAndPush --ConfigFilePath iasl.config.json --Version 20180209.0.0 --InputFolderPath \"C:\\temp\\iasl-win-20180209\\new\" --ApiKey <your key here>","title":"Example: Publishing new version of tool"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_omnicache/","text":"Omnicache \u00b6 Omnicache, the tool, is a command line tool that helps setup and update a Project Mu Omnicache. An Omnicache is just a bare repo with lots of remotes fetched so that if configured the Project Mu tools will use it as a reference when updating or cloning a repo. This saves a lot of network bandwidth, disk space, and time if you develop with many workspaces on a single PC and can also be used to speed up CI. Creating your Omnicache \u00b6 You can setup your Omnicache many ways. You can add config entries from numerous files or thru command line. Try Omnicache -h for help. Here are the steps for a simple empty installation. Make sure you have installed mu_environment using Pip Open cmd prompt Create one omnicache --init <path> At the end of the creation it will suggest setting the OMNICACHE_PATH environment variable. For best results do this. Adding Config Entries \u00b6 Config entries can be added when first creating the cache as well as any time by using the tool. Config entries can be added 1-by-1 from the command line or thru a config file. Example of adding config entry \u00b6 omnicache -a tianocore_edk2 https://github.com/tianocore/edk2.git --init %OMNICACHE_PATH% omnicache -a openssl https://github.com/openssl/openssl.git True --init %OMNICACHE_PATH% Example Config for Project Mu repos \u00b6 Copy the below sample and save it as abc.yml remotes: - name: mu_basecore url: https://github.com/Microsoft/mu_basecore.git - name: common_mu url: https://github.com/Microsoft/mu_plus.git - name: common_mu_tiano2 url: https://github.com/Microsoft/mu_tiano_plus.git - name: mu_silicon_intel_tiano url: https://github.com/Microsoft/mu_silicon_intel_tiano.git - name: mu_silicon_arm_tiano url: https://github.com/Microsoft/mu_silicon_arm_tiano.git - name: mu_oem_sample url: https://github.com/Microsoft/mu_oem_sample.git - name: openssl url: https://github.com/openssl/openssl.git tag: true - name: tianocore_edk2 url: https://github.com/tianocore/edk2.git Then run omnicache command to add the new entries. omnicache -c abc.yml --init %OMNICACHE_PATH% Keeping your Omnicache Current \u00b6 The Omnicache doesn't have to always be current. If it gets stale it will still help but there will be more \"cache misses\". Since the Omnicache is just a git repo it can easily be updated by running git commands and since it is a bare repo it is trouble free to update. The Omicache tool attempts to make this even easier. Windows Scheduled Task \u00b6 If you want to use a scheduled task here is one way to do it on Windows. Set the OMNICACHE_PATH environment variable to your path Create an omnicache_update.bat file in your omnicache directory that contains omnicache --init --fetch %OMNICACHE_PATH% Create a temporary XML file on your desktop with the contents below named \"O_U.xml\" <?xml version=\"1.0\" encoding=\"UTF-16\"?> <Task version= \"1.4\" xmlns= \"http://schemas.microsoft.com/windows/2004/02/mit/task\" > <Triggers> <CalendarTrigger> <StartBoundary> 2019-01-04T8:00:00 </StartBoundary> <ExecutionTimeLimit> PT2H </ExecutionTimeLimit> <Enabled> true </Enabled> <ScheduleByDay> <DaysInterval> 1 </DaysInterval> </ScheduleByDay> </CalendarTrigger> </Triggers> <Settings> <MultipleInstancesPolicy> IgnoreNew </MultipleInstancesPolicy> <DisallowStartIfOnBatteries> false </DisallowStartIfOnBatteries> <StopIfGoingOnBatteries> false </StopIfGoingOnBatteries> <AllowHardTerminate> true </AllowHardTerminate> <StartWhenAvailable> false </StartWhenAvailable> <RunOnlyIfNetworkAvailable> false </RunOnlyIfNetworkAvailable> <IdleSettings> <StopOnIdleEnd> true </StopOnIdleEnd> <RestartOnIdle> false </RestartOnIdle> </IdleSettings> <AllowStartOnDemand> true </AllowStartOnDemand> <Enabled> true </Enabled> <Hidden> false </Hidden> <RunOnlyIfIdle> false </RunOnlyIfIdle> <DisallowStartOnRemoteAppSession> false </DisallowStartOnRemoteAppSession> <UseUnifiedSchedulingEngine> true </UseUnifiedSchedulingEngine> <WakeToRun> false </WakeToRun> <ExecutionTimeLimit> PT72H </ExecutionTimeLimit> <Priority> 7 </Priority> </Settings> <Actions Context= \"Author\" > <Exec> <Command> cmd.exe </Command> <Arguments> /c omnicache_update.bat </Arguments> <WorkingDirectory> %OMNICACHE_PATH% </WorkingDirectory> </Exec> </Actions> </Task> Open Cmd prompt and use SCHTASKS to create a task. SCHTASKS /Create /XML \"O_U.xml\" /TN \"Omnicache Updater\" Using Omnicache for update \u00b6 Set the environment variable OMNICACHE_PATH for automatic usage. Project Mu tools when running platformbuild.py --setup or platformbuild.py --update will use the cache. Using Omnicache for git clone \u00b6 Current best practice is to setup a bashrc alias if using git for windows in gitbash. alias gcl = ' git clone --reference ${ OMNICACHE_PATH } \u2019 Then every git clone you want to do you can call gcl <url> <folder> instead of git clone <url> <folder> Warnings \u00b6 Removing the omnicache from your PC can cause problems in your repos. Read up on --reference in git for methods to resolve this before deleting the omnicache. Bug in git submodule update --recursive --reference <path> . This doesn't work as git appends the recursive submodule path to the reference path. Contacting git maintainers for clarity. Tags: tags are not namespaced by remote therefore conflicts could occur. Suggestion is to not pull tags unless required. Stack exchange has a few other ideas but nothing implemented yet. Older versions of the omnicache tool used -u true to update. Newer versions just require -u or --fetch . Since -a is a varable length argument list it is best to always add the --init parameter as the last parameter before the cache_dir positional argument. This way python argparse knows positional args from the -a optional args. A second tutorial of Omnicache \u00b6 The Omnicache or how I learned to stop worrying and love the allrepo \u00b6 The Genesis \u00b6 Many repos in the Project Mu tree have common roots and share a very similar codebase. In order to speed up clone times for our CI builds as well as for personal use, we realized you can clone a repo using a reference repository. git clone --reference ../some-directory Another feature that came to light is that you can use git to create an omnirepostitory. You can have all the objects stored into one place and git will query this repo for any objects it wishes to fetch and if they aren't found, it will then request them from upstream. We created some helper functions to wrap around this. It can be called by omnicache. Creating a new omnicache \u00b6 omnicache --init ../omnicache You can optionally use omnicache --new ../omnicache The difference between the two is that new will fail if something exists there, init does not. Feeding- I mean, Adding to the omnicache \u00b6 omnicache -a <name> <url> <Sync tags optional default = False> ../omnicache omnicache --add <name> <url> <Sync tags optional default = False> ../omnicache (Either of these will work) Updating the omnicache \u00b6 Now that you're a proud owner of an omnicache, you need to take care to update it semi-regularly. omnicache --update ../omnicache omnicache -u ../omnicache (Either of these will work) Know what's in the cache \u00b6 You can find out what is in your cache by listing it's contents. omnicache --list ../omnicache Assimilation into the Omnicache \u00b6 Sometimes you have a folder where all the repos are already cloned (either as submodules or separate folders). You can scan them all into the omnicache by using the scan feature. omnicache --scan ../folder ../omnicache This will add unique repos/submodules that it finds in the top level folders in ../folder. Unique is determined by URL. Fighting back against the Omnicache \u00b6 If your omnicache has grown a touch too powerful, you can take control back in your life by removing items from the cache. omnicache --remove ../omnicache omnicache -r ../omnicache Using the Omnicache \u00b6 Many of the tools in Project Mu are equipped to handle the omnicache and details on how to use them can be found in their respective documentations or help menus.","title":"feature omnicache"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_omnicache/#omnicache","text":"Omnicache, the tool, is a command line tool that helps setup and update a Project Mu Omnicache. An Omnicache is just a bare repo with lots of remotes fetched so that if configured the Project Mu tools will use it as a reference when updating or cloning a repo. This saves a lot of network bandwidth, disk space, and time if you develop with many workspaces on a single PC and can also be used to speed up CI.","title":"Omnicache"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_omnicache/#creating-your-omnicache","text":"You can setup your Omnicache many ways. You can add config entries from numerous files or thru command line. Try Omnicache -h for help. Here are the steps for a simple empty installation. Make sure you have installed mu_environment using Pip Open cmd prompt Create one omnicache --init <path> At the end of the creation it will suggest setting the OMNICACHE_PATH environment variable. For best results do this.","title":"Creating your Omnicache"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_omnicache/#adding-config-entries","text":"Config entries can be added when first creating the cache as well as any time by using the tool. Config entries can be added 1-by-1 from the command line or thru a config file.","title":"Adding Config Entries"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_omnicache/#example-of-adding-config-entry","text":"omnicache -a tianocore_edk2 https://github.com/tianocore/edk2.git --init %OMNICACHE_PATH% omnicache -a openssl https://github.com/openssl/openssl.git True --init %OMNICACHE_PATH%","title":"Example of adding config entry"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_omnicache/#example-config-for-project-mu-repos","text":"Copy the below sample and save it as abc.yml remotes: - name: mu_basecore url: https://github.com/Microsoft/mu_basecore.git - name: common_mu url: https://github.com/Microsoft/mu_plus.git - name: common_mu_tiano2 url: https://github.com/Microsoft/mu_tiano_plus.git - name: mu_silicon_intel_tiano url: https://github.com/Microsoft/mu_silicon_intel_tiano.git - name: mu_silicon_arm_tiano url: https://github.com/Microsoft/mu_silicon_arm_tiano.git - name: mu_oem_sample url: https://github.com/Microsoft/mu_oem_sample.git - name: openssl url: https://github.com/openssl/openssl.git tag: true - name: tianocore_edk2 url: https://github.com/tianocore/edk2.git Then run omnicache command to add the new entries. omnicache -c abc.yml --init %OMNICACHE_PATH%","title":"Example Config for Project Mu repos"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_omnicache/#keeping-your-omnicache-current","text":"The Omnicache doesn't have to always be current. If it gets stale it will still help but there will be more \"cache misses\". Since the Omnicache is just a git repo it can easily be updated by running git commands and since it is a bare repo it is trouble free to update. The Omicache tool attempts to make this even easier.","title":"Keeping your Omnicache Current"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_omnicache/#windows-scheduled-task","text":"If you want to use a scheduled task here is one way to do it on Windows. Set the OMNICACHE_PATH environment variable to your path Create an omnicache_update.bat file in your omnicache directory that contains omnicache --init --fetch %OMNICACHE_PATH% Create a temporary XML file on your desktop with the contents below named \"O_U.xml\" <?xml version=\"1.0\" encoding=\"UTF-16\"?> <Task version= \"1.4\" xmlns= \"http://schemas.microsoft.com/windows/2004/02/mit/task\" > <Triggers> <CalendarTrigger> <StartBoundary> 2019-01-04T8:00:00 </StartBoundary> <ExecutionTimeLimit> PT2H </ExecutionTimeLimit> <Enabled> true </Enabled> <ScheduleByDay> <DaysInterval> 1 </DaysInterval> </ScheduleByDay> </CalendarTrigger> </Triggers> <Settings> <MultipleInstancesPolicy> IgnoreNew </MultipleInstancesPolicy> <DisallowStartIfOnBatteries> false </DisallowStartIfOnBatteries> <StopIfGoingOnBatteries> false </StopIfGoingOnBatteries> <AllowHardTerminate> true </AllowHardTerminate> <StartWhenAvailable> false </StartWhenAvailable> <RunOnlyIfNetworkAvailable> false </RunOnlyIfNetworkAvailable> <IdleSettings> <StopOnIdleEnd> true </StopOnIdleEnd> <RestartOnIdle> false </RestartOnIdle> </IdleSettings> <AllowStartOnDemand> true </AllowStartOnDemand> <Enabled> true </Enabled> <Hidden> false </Hidden> <RunOnlyIfIdle> false </RunOnlyIfIdle> <DisallowStartOnRemoteAppSession> false </DisallowStartOnRemoteAppSession> <UseUnifiedSchedulingEngine> true </UseUnifiedSchedulingEngine> <WakeToRun> false </WakeToRun> <ExecutionTimeLimit> PT72H </ExecutionTimeLimit> <Priority> 7 </Priority> </Settings> <Actions Context= \"Author\" > <Exec> <Command> cmd.exe </Command> <Arguments> /c omnicache_update.bat </Arguments> <WorkingDirectory> %OMNICACHE_PATH% </WorkingDirectory> </Exec> </Actions> </Task> Open Cmd prompt and use SCHTASKS to create a task. SCHTASKS /Create /XML \"O_U.xml\" /TN \"Omnicache Updater\"","title":"Windows Scheduled Task"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_omnicache/#using-omnicache-for-update","text":"Set the environment variable OMNICACHE_PATH for automatic usage. Project Mu tools when running platformbuild.py --setup or platformbuild.py --update will use the cache.","title":"Using Omnicache for update"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_omnicache/#using-omnicache-for-git-clone","text":"Current best practice is to setup a bashrc alias if using git for windows in gitbash. alias gcl = ' git clone --reference ${ OMNICACHE_PATH } \u2019 Then every git clone you want to do you can call gcl <url> <folder> instead of git clone <url> <folder>","title":"Using Omnicache for git clone"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_omnicache/#warnings","text":"Removing the omnicache from your PC can cause problems in your repos. Read up on --reference in git for methods to resolve this before deleting the omnicache. Bug in git submodule update --recursive --reference <path> . This doesn't work as git appends the recursive submodule path to the reference path. Contacting git maintainers for clarity. Tags: tags are not namespaced by remote therefore conflicts could occur. Suggestion is to not pull tags unless required. Stack exchange has a few other ideas but nothing implemented yet. Older versions of the omnicache tool used -u true to update. Newer versions just require -u or --fetch . Since -a is a varable length argument list it is best to always add the --init parameter as the last parameter before the cache_dir positional argument. This way python argparse knows positional args from the -a optional args.","title":"Warnings"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_omnicache/#a-second-tutorial-of-omnicache","text":"","title":"A second tutorial of Omnicache"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_omnicache/#the-omnicache-or-how-i-learned-to-stop-worrying-and-love-the-allrepo","text":"","title":"The Omnicache or how I learned to stop worrying and love the allrepo"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_omnicache/#the-genesis","text":"Many repos in the Project Mu tree have common roots and share a very similar codebase. In order to speed up clone times for our CI builds as well as for personal use, we realized you can clone a repo using a reference repository. git clone --reference ../some-directory Another feature that came to light is that you can use git to create an omnirepostitory. You can have all the objects stored into one place and git will query this repo for any objects it wishes to fetch and if they aren't found, it will then request them from upstream. We created some helper functions to wrap around this. It can be called by omnicache.","title":"The Genesis"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_omnicache/#creating-a-new-omnicache","text":"omnicache --init ../omnicache You can optionally use omnicache --new ../omnicache The difference between the two is that new will fail if something exists there, init does not.","title":"Creating a new omnicache"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_omnicache/#feeding-i-mean-adding-to-the-omnicache","text":"omnicache -a <name> <url> <Sync tags optional default = False> ../omnicache omnicache --add <name> <url> <Sync tags optional default = False> ../omnicache (Either of these will work)","title":"Feeding- I mean, Adding to the omnicache"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_omnicache/#updating-the-omnicache","text":"Now that you're a proud owner of an omnicache, you need to take care to update it semi-regularly. omnicache --update ../omnicache omnicache -u ../omnicache (Either of these will work)","title":"Updating the omnicache"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_omnicache/#know-whats-in-the-cache","text":"You can find out what is in your cache by listing it's contents. omnicache --list ../omnicache","title":"Know what's in the cache"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_omnicache/#assimilation-into-the-omnicache","text":"Sometimes you have a folder where all the repos are already cloned (either as submodules or separate folders). You can scan them all into the omnicache by using the scan feature. omnicache --scan ../folder ../omnicache This will add unique repos/submodules that it finds in the top level folders in ../folder. Unique is determined by URL.","title":"Assimilation into the Omnicache"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_omnicache/#fighting-back-against-the-omnicache","text":"If your omnicache has grown a touch too powerful, you can take control back in your life by removing items from the cache. omnicache --remove ../omnicache omnicache -r ../omnicache","title":"Fighting back against the Omnicache"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_omnicache/#using-the-omnicache","text":"Many of the tools in Project Mu are equipped to handle the omnicache and details on how to use them can be found in their respective documentations or help menus.","title":"Using the Omnicache"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_pluginmanager/","text":"Plugin Manager \u00b6 The Genesis \u00b6 Plugins are similar to external dependencies, in that they are defined by a Json file and they are discovered by the SDE. If you wish to learn more about the SDE, please go read the document about the self describing enviroment. They are defined by the EnvironmentDescriptorFiles which also describe external dependencies and path descriptors. Types of plugins \u00b6 Types of plugins are defined by the class they inherit from UefiBuildPlugin Contains two methods, Pre and Post Build. These methods are called on Pre and Post Build steps in UefiBuild (not MuBuild). There is no guarantee on ordering between different plugins (Pre will always come before Post). Post is will not run if there is a critical error in the build process. The idea here is to allow for custom, self-contained build functionality to be added without required UEFI build changes or inline code modifications. DscProcessorPlugin (in-progress) This is a plugin type that can apply transformations to the active DSC that will then be used to build the system. This is not production ready and not enabled in any builds currently. UefiHelperPlugin This is a helper plugin that publishes a function that can be used by other parts of the system. An example of this would be the Capsule signing system. This really is less about plugin design and more about keeping the UEFI build and platform builder python files minimal and getting the desired code reuse. MuBuildPlugin A plugin that runs during the main stage of MuBuild. The build step is actually a plugin so as ordering is not guranteed so you don't have any assurance that the build is successful or that the build has started How it works \u00b6 You might be asking yourself how does the sausage get made. In the name of sating curiosity, here it is. The SDE discovers the plugin .json environment descriptors in the file system tree. Once they're disocvered, they're passed to the Plugin Manager which loads each of them and puts them into the appropriate structure. Once they're in there, they are requested by UefiBuild or MuBuild and dispatched. Helper functions are requested from the PluginManager and then executed. Writing your own \u00b6 Writing your own plugin is fairly simple. See MuEnvironment\\PluginManager.py for the interface definition and required functions for each type of plugin. For IUefiBuildPlugin type the plugin will simply be called during the pre and post build steps after the platform builder object runs its step. The UefiBuilder object will be passed during the call and therefore the environment dictionary is available within the plugin. These plugins should be authored to be independent and the platform build or UEFI build should not have any dependency on the plugin. The plugin can depend on variables within the environment dictionary but should be otherwise independent / isolated code. For IUefiHelperPlugin type the plugin will simply register functions with the helper object so that other parts of the platform build can use the functions. It is acceptable for platform build to know/need the helper functions but it is not acceptable for UEFI build super class to depend upon it. I expect most of these plugins will be at a layer lower than the UDK as this is really to isolate business unit logic while still allowing code reuse. Look at the HelperFunctions object to see how a plugin registers its functions. For IMuBuildPlugin type the plugin will be allowed to verify it's configuration and be called by the MuBuild system. It will have the current state of the build and access to the environment. MuBuild checkpoints the environment prior to calling out to each plugin, so the environment can be dirtied by the plugin. As an example of a Mu Build Plugin, we will look at one of the plugins we use, Character Encoding Check MuBuildPlugin. This runs as part of the MuBuild CI build. The schema \u00b6 From MU_BASECORE\\BaseTools\\Plugin\\CharEncodingCheck\\CharEncodingCheck_plug_in.json { \"scope\" : \"project_mu\" , \"name\" : \"Char Encoding Check Test\" , \"module\" : \"CharEncodingCheck\" } Scope: See the SDE doc about scopes Name: This is the name of the plugin and will be part of the path where the nuget is unpacked Module: the python file to load The Python \u00b6 File is from: MU_BASECORE\\BaseTools\\Plugin\\CharEncodingCheck\\CharEncodingCheck.py It's important that the filename matches the Module name in the json file. import os import logging from MuEnvironment.PluginManager import IMuBuildPlugin class CharEncodingCheck ( IMuBuildPlugin ): def GetTestName ( self , packagename , environment ): return ( \"MuBuild CharEncodingCheck \" + packagename , \"MuBuild.CharEncodingCheck.\" + packagename ) # - package is the edk2 path to package. This means workspace/packagepath relative. # - edk2path object configured with workspace and packages path # - any additional command line args # - RepoConfig Object (dict) for the build # - PkgConfig Object (dict) for the pkg # - EnvConfig Object # - Plugin Manager Instance # - Plugin Helper Obj Instance # - testclass Object used for outputing junit results # - output_stream the StringIO output stream from this plugin def RunBuildPlugin ( self , packagename , Edk2pathObj , args , repoconfig , pkgconfig , environment , PLM , PLMHelper , tc , output_stream = None ): overall_status = 0 files_tested = 0 if overall_status is not 0 : tc . SetFailed ( \"CharEncoding {0} Failed. Errors {1} \" . format ( packagename , overall_status ), \"CHAR_ENCODING_CHECK_FAILED\" ) else : tc . SetSuccess () return overall_status def ValidateConfig ( self , config , name ): validOptions = [ \"IgnoreFiles\" , \"skip\" ] for key in config : if key not in validOptions : raise Exception ( \"Invalid config option {0} in {1} \" . format ( key , name )) Some things to notice are the class that this is inheriting from: IMuBuildPlugin. Validate Config is the way that a plugin can validate the configuration they will receive. The repo config and the package config is later passed into the RunBuildPlugin method. The validate step is run before build happens so we don't waste a user's time only to bail halfway through the build process. There is also this idea of the tc, which is the test unit class. You can set this particular MuBuild step as failed, skipped, or successful. Logging standard out or error out gets placed in the JUnit report that is later picked up by the CI system. Using a plugin \u00b6 Using plugins is straightforward but it exact usage depends on what type of plugin you use. For the IUefiBuildPlugin (pre/post build) and IMuBuildPlugin type there is nothing the UEFI build must do besides make sure the plugin is in your workspace and scoped to an active scope. For Helper plugins basically the UEFI builder Helper member will contain the registered functions as methods on the object. Therefore calling any function is as simple as using self.Helper.[your func name here]. It is by design that the parameters and calling contract are not defined. It is expected that the caller and plugin know about each other and are really just using the plugin system to make inclusion and code sharing easy.","title":"feature pluginmanager"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_pluginmanager/#plugin-manager","text":"","title":"Plugin Manager"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_pluginmanager/#the-genesis","text":"Plugins are similar to external dependencies, in that they are defined by a Json file and they are discovered by the SDE. If you wish to learn more about the SDE, please go read the document about the self describing enviroment. They are defined by the EnvironmentDescriptorFiles which also describe external dependencies and path descriptors.","title":"The Genesis"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_pluginmanager/#types-of-plugins","text":"Types of plugins are defined by the class they inherit from UefiBuildPlugin Contains two methods, Pre and Post Build. These methods are called on Pre and Post Build steps in UefiBuild (not MuBuild). There is no guarantee on ordering between different plugins (Pre will always come before Post). Post is will not run if there is a critical error in the build process. The idea here is to allow for custom, self-contained build functionality to be added without required UEFI build changes or inline code modifications. DscProcessorPlugin (in-progress) This is a plugin type that can apply transformations to the active DSC that will then be used to build the system. This is not production ready and not enabled in any builds currently. UefiHelperPlugin This is a helper plugin that publishes a function that can be used by other parts of the system. An example of this would be the Capsule signing system. This really is less about plugin design and more about keeping the UEFI build and platform builder python files minimal and getting the desired code reuse. MuBuildPlugin A plugin that runs during the main stage of MuBuild. The build step is actually a plugin so as ordering is not guranteed so you don't have any assurance that the build is successful or that the build has started","title":"Types of plugins"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_pluginmanager/#how-it-works","text":"You might be asking yourself how does the sausage get made. In the name of sating curiosity, here it is. The SDE discovers the plugin .json environment descriptors in the file system tree. Once they're disocvered, they're passed to the Plugin Manager which loads each of them and puts them into the appropriate structure. Once they're in there, they are requested by UefiBuild or MuBuild and dispatched. Helper functions are requested from the PluginManager and then executed.","title":"How it works"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_pluginmanager/#writing-your-own","text":"Writing your own plugin is fairly simple. See MuEnvironment\\PluginManager.py for the interface definition and required functions for each type of plugin. For IUefiBuildPlugin type the plugin will simply be called during the pre and post build steps after the platform builder object runs its step. The UefiBuilder object will be passed during the call and therefore the environment dictionary is available within the plugin. These plugins should be authored to be independent and the platform build or UEFI build should not have any dependency on the plugin. The plugin can depend on variables within the environment dictionary but should be otherwise independent / isolated code. For IUefiHelperPlugin type the plugin will simply register functions with the helper object so that other parts of the platform build can use the functions. It is acceptable for platform build to know/need the helper functions but it is not acceptable for UEFI build super class to depend upon it. I expect most of these plugins will be at a layer lower than the UDK as this is really to isolate business unit logic while still allowing code reuse. Look at the HelperFunctions object to see how a plugin registers its functions. For IMuBuildPlugin type the plugin will be allowed to verify it's configuration and be called by the MuBuild system. It will have the current state of the build and access to the environment. MuBuild checkpoints the environment prior to calling out to each plugin, so the environment can be dirtied by the plugin. As an example of a Mu Build Plugin, we will look at one of the plugins we use, Character Encoding Check MuBuildPlugin. This runs as part of the MuBuild CI build.","title":"Writing your own"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_pluginmanager/#the-schema","text":"From MU_BASECORE\\BaseTools\\Plugin\\CharEncodingCheck\\CharEncodingCheck_plug_in.json { \"scope\" : \"project_mu\" , \"name\" : \"Char Encoding Check Test\" , \"module\" : \"CharEncodingCheck\" } Scope: See the SDE doc about scopes Name: This is the name of the plugin and will be part of the path where the nuget is unpacked Module: the python file to load","title":"The schema"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_pluginmanager/#the-python","text":"File is from: MU_BASECORE\\BaseTools\\Plugin\\CharEncodingCheck\\CharEncodingCheck.py It's important that the filename matches the Module name in the json file. import os import logging from MuEnvironment.PluginManager import IMuBuildPlugin class CharEncodingCheck ( IMuBuildPlugin ): def GetTestName ( self , packagename , environment ): return ( \"MuBuild CharEncodingCheck \" + packagename , \"MuBuild.CharEncodingCheck.\" + packagename ) # - package is the edk2 path to package. This means workspace/packagepath relative. # - edk2path object configured with workspace and packages path # - any additional command line args # - RepoConfig Object (dict) for the build # - PkgConfig Object (dict) for the pkg # - EnvConfig Object # - Plugin Manager Instance # - Plugin Helper Obj Instance # - testclass Object used for outputing junit results # - output_stream the StringIO output stream from this plugin def RunBuildPlugin ( self , packagename , Edk2pathObj , args , repoconfig , pkgconfig , environment , PLM , PLMHelper , tc , output_stream = None ): overall_status = 0 files_tested = 0 if overall_status is not 0 : tc . SetFailed ( \"CharEncoding {0} Failed. Errors {1} \" . format ( packagename , overall_status ), \"CHAR_ENCODING_CHECK_FAILED\" ) else : tc . SetSuccess () return overall_status def ValidateConfig ( self , config , name ): validOptions = [ \"IgnoreFiles\" , \"skip\" ] for key in config : if key not in validOptions : raise Exception ( \"Invalid config option {0} in {1} \" . format ( key , name )) Some things to notice are the class that this is inheriting from: IMuBuildPlugin. Validate Config is the way that a plugin can validate the configuration they will receive. The repo config and the package config is later passed into the RunBuildPlugin method. The validate step is run before build happens so we don't waste a user's time only to bail halfway through the build process. There is also this idea of the tc, which is the test unit class. You can set this particular MuBuild step as failed, skipped, or successful. Logging standard out or error out gets placed in the JUnit report that is later picked up by the CI system.","title":"The Python"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_pluginmanager/#using-a-plugin","text":"Using plugins is straightforward but it exact usage depends on what type of plugin you use. For the IUefiBuildPlugin (pre/post build) and IMuBuildPlugin type there is nothing the UEFI build must do besides make sure the plugin is in your workspace and scoped to an active scope. For Helper plugins basically the UEFI builder Helper member will contain the registered functions as methods on the object. Therefore calling any function is as simple as using self.Helper.[your func name here]. It is by design that the parameters and calling contract are not defined. It is expected that the caller and plugin know about each other and are really just using the plugin system to make inclusion and code sharing easy.","title":"Using a plugin"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_sde/","text":"The Self Describing Environment and You \u00b6 The Genesis \u00b6 As Project Mu grew, the centralized systems that have been in place to this point have gotten more and more brittle. Previously, the paths to critical files and build tools have been hard-coded into the primary build scripts (such as PlatformBuild.py). If code was to be added or moved, all build scripts for all projects had to be updated to find the new code and consume it. Furthermore, the old build system required that all binaries, executables, artifacts, and other miscellaneous files be carried in the source tree somewhere. Since moving to Git, this cost has become increasingly burdensome to the point where some of the larger repos are almost unwieldly. The new Self Describing Environment system, along with the new Plugin behavior, aims to remedy some of these problems, while preserving flexibility and agility for further project growth. What is it? \u00b6 The Self-Describing Environment is assembled by a combination of scripts and descriptor files. The scripts locate the descriptor files and configure the environment in a number of different ways (eg. PATH, PYTHONPATH, Shell Variables, Build Variables, external dependencies, etc.). Currently, there are two kinds of descriptor files that can be found in the Core UEFI tree: Path Environment descriptors (path_env) and External Dependency descriptors (ext_dep). Both of these files are simple JSON files containing fields that are used to configure the SDE. They have some overlapping features, but are used for very different purposes. Many of these features have their own documentation, and you are encouraged to go check them out. path_env Descriptors \u00b6 The path_env descriptor is used, primarily, to update the path. This way the build system can locate required tools and scripts. It can also update build vars that can be referenced from the primary build script (PlatformBuild.py) to locate things like binary artifacts that will be included in certain build steps (eg. OPROM binaries). The path_env descriptor works by taking the path containing the descriptor and applying it to the environment as specified by the fields of the descriptor. For example, if there were a path_env file located at \"\\MyBuild\\SubDir\\Tools\\my_sample_path_env.json\" and the descriptor flags included \"set_path\", \"\\MyBuild\\SubDir\\Tools\" would be added to the environment path. path_env descriptors are located by the environment configuration scripts by searching the Workspace for files ending in \"*_path_env.json\". It does not matter what the first part of the file is called, so long as the end is correct. By convention, the first part of the file name should be descriptive enough to differentiate a given descriptor from another descriptor, should it show up in a \"find in files\" list or something. The following path_env fields are required: scope Identifies which build environments this descriptor contributes to, and what level of precedence it should take within that environment. flags We'll see that flags are common to both path_env and ext_dep descriptors, but they are required for path_env (and only optional for ext_dep). This is because it doesn\u2019t make any sense to create a path_env descriptor without specifying what part of the environment should be updated. Currently supported flags are: host_specific Allows a nuget package to specify that the contents of the package are organized by host OS or architecture. The SDE will determine what folder is relevant for the host OS and product being built and add that to the path. set_path Adds the NuGet unpacked folder to the front of PATH set_pypath Adds the NuGet unpacked folder to the front of PYTHONPATH. Also adds it to sys.path. set_build_var Sets a build variable with the key being the name of the ext_dep and the value being the path of the nuget unpacked folder If you include this attribute you must include a var_name (a var that exists internally to the build system that is retrieved with with env.GetValue()) set_shell_var Sets a shell variable with the key being the name of the ext_dep and the value being the path of the nuget unpacked folder If you include this attribute you must include a var_name (a var that exists in the command-line environment via \"set\" or \"os.environ\" or \"env\") include_separator Includes a path seperated at the end of the path we set in variables The following path_env fields are optional or conditional: var_name If either the \"set_shell_var\" or \"set_build_var\" are in the flags, this field will be required. It defines the name of the var being set. id Part of the Override System, this field defines the name that this file will be referred to as by any override_id fields. override_id This file will override any descriptor files found in lower lexical order or scope order. Files are traversed in directory order (depth first) and then scope order (highest to lowest). Overrides are only applied forward in the traversal, not backwards. The Belly of the Beast \u00b6 SelfDescribingEnvironment.py \u00b6 This is the proverbial \"heart of the beast\". It contains most of the business logic for locating, compiling, sorting, filtering, and assembling the SDE files and the environment itself. There are class methods and helper functions to do things like: - Locate all the relevant files in the workspace. - Sort the files lexically and by scope. - Filter the files based on overrides. - Assemble the environment (eg. PATH, PYTHONPATH, vars, etc.). - Validate all dependencies. - Update all dependencies. Many of these routines will leverage logic specific to individual sub-modules (Python, not Git), but the collective logic is located here. EnvironmentDescriptorFiles.py \u00b6 This module contains business logic and validation code for dealing with the descriptor files as JSON objects. It contains code (and error checking) for loading the files, reading their contents into a standard internal representation, and running a limited set of sanitization and validation functions to identify any mistakes as early as possible and provide as much information as possible. For convenience, this module also contains the class code for PathEnv descriptor objects, but that's because the class code is so small felt silly to create another file. ExternalDependencies.py \u00b6 This module contains code for managing external dependencies. ExternalDependency objects are created with the data from ext_dep descriptors and are subclassed according to the \"type\" field in the descriptor. Currently, the only valid subclass is \"nuget\". These objects contain the code for fetching, validating, updating, and cleaning dependency objects and metadata. When referenced from the SDE itself, they can also update paths and other build/shell vars in the build environment. Taming the SDE \u00b6 Understanding Scope \u00b6 A critical concept in the SDE system is that of \"scope\". Each project can define its own scope, and scope is integral to the distributed and shared nature of the SDE. Project scopes are linearly hierarchical and can have an arbitrary number of entries. Only descriptors matching one or more of the scope entries will be included in the SDE during initialization. Furthermore, higher scopes will take precedence when setting paths and assigning values to vars. An example project scope might be: (\"my_platform\", \"tablet_family\", \"silicon_reference\") In this example, \"my_platform\" is the highest priority in the scope. Any descriptor files found in the entire workspace that have this scope will not only be included in the SDE, they will take precedence over any of the lesser scopes. \"tablet_family\" and \"silicon_reference\" scopes will also be used, in that order. Additionally, all projects inherit the \"global\" scope, but it takes the lowest precedence. Setting Up for PlatformBuild \u00b6 The SDE includes modifications to the PlatformBuild.py script that make it easier to start working with any platform. Since the SDE knows how to fetch its own dependencies, and since all these dependencies are described by the platform itself, the build scripts can now perform the minimal steps to enable building any given platform, including: Synchronizing all required submodules. Downloading all source (and only the source actually used by the platform). Configuring all paths. Downloading all binaries. To leverage this setup behavior, simply run the PlatformBuild.py script corresponding to the platform you want to build with the \"--SETUP\" argument. This argument will cause the platform to configure itself, and display any errors encountered. NOTE: --SETUP should only be required once per build machine, per platform being built. It is not necessary to run it regularly. Only when setting up a new personal workstation or starting to work with a platform that you haven't used yet. The --SETUP feature does not actually build the platform. A normal PlatformBuild.py must still be performed. The --SETUP feature will NOT change branches in any submodule that already exists locally, or that has local changes. This is to prevent accidental loss of work. If you would like the script to try making changes even in these cases, use the \"--FORCE\" argument. The --SETUP feature does not yet install dev singing certs. Those steps must still be performed manually. Setting Up for MuBuild \u00b6 MuBuild works on a similar mechanism to PlatformBuild but it invokes the SDE directly and does an update. Git Modules are monitored and handled via the RepoResolver framework, which has more logic to it, and doesn't handle submodules.c Building \u00b6 Building still works as it always has and all prior arguments can still be passed to the PlatformBuild.py script. The only special arguments are \"--SETUP\" and \"--UPDATE\" (described below), which will trigger new behaviors. Note that the current state of the SDE is always printed in the DEBUG level of the build log. Updating Dependencies \u00b6 Prior to any build, the SDE will attempt to validate the external dependencies that currently exist on the local machine against the versions that are specified in the code. If the code is updated (perhaps by a pull request to the branch you're working on), it is possible that the dependencies will have to be refreshed. If this is the case, you will see a message prompting you to do so when you run PlatformBuild.py to build your platform. To perform this update, simply run the PlatformBuild.py script with the --UPDATE argument. Any dependencies that match their current versions will be skipped and only out-of-date dependencies will be refreshed.","title":"feature sde"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_sde/#the-self-describing-environment-and-you","text":"","title":"The Self Describing Environment and You"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_sde/#the-genesis","text":"As Project Mu grew, the centralized systems that have been in place to this point have gotten more and more brittle. Previously, the paths to critical files and build tools have been hard-coded into the primary build scripts (such as PlatformBuild.py). If code was to be added or moved, all build scripts for all projects had to be updated to find the new code and consume it. Furthermore, the old build system required that all binaries, executables, artifacts, and other miscellaneous files be carried in the source tree somewhere. Since moving to Git, this cost has become increasingly burdensome to the point where some of the larger repos are almost unwieldly. The new Self Describing Environment system, along with the new Plugin behavior, aims to remedy some of these problems, while preserving flexibility and agility for further project growth.","title":"The Genesis"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_sde/#what-is-it","text":"The Self-Describing Environment is assembled by a combination of scripts and descriptor files. The scripts locate the descriptor files and configure the environment in a number of different ways (eg. PATH, PYTHONPATH, Shell Variables, Build Variables, external dependencies, etc.). Currently, there are two kinds of descriptor files that can be found in the Core UEFI tree: Path Environment descriptors (path_env) and External Dependency descriptors (ext_dep). Both of these files are simple JSON files containing fields that are used to configure the SDE. They have some overlapping features, but are used for very different purposes. Many of these features have their own documentation, and you are encouraged to go check them out.","title":"What is it?"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_sde/#path_env-descriptors","text":"The path_env descriptor is used, primarily, to update the path. This way the build system can locate required tools and scripts. It can also update build vars that can be referenced from the primary build script (PlatformBuild.py) to locate things like binary artifacts that will be included in certain build steps (eg. OPROM binaries). The path_env descriptor works by taking the path containing the descriptor and applying it to the environment as specified by the fields of the descriptor. For example, if there were a path_env file located at \"\\MyBuild\\SubDir\\Tools\\my_sample_path_env.json\" and the descriptor flags included \"set_path\", \"\\MyBuild\\SubDir\\Tools\" would be added to the environment path. path_env descriptors are located by the environment configuration scripts by searching the Workspace for files ending in \"*_path_env.json\". It does not matter what the first part of the file is called, so long as the end is correct. By convention, the first part of the file name should be descriptive enough to differentiate a given descriptor from another descriptor, should it show up in a \"find in files\" list or something. The following path_env fields are required: scope Identifies which build environments this descriptor contributes to, and what level of precedence it should take within that environment. flags We'll see that flags are common to both path_env and ext_dep descriptors, but they are required for path_env (and only optional for ext_dep). This is because it doesn\u2019t make any sense to create a path_env descriptor without specifying what part of the environment should be updated. Currently supported flags are: host_specific Allows a nuget package to specify that the contents of the package are organized by host OS or architecture. The SDE will determine what folder is relevant for the host OS and product being built and add that to the path. set_path Adds the NuGet unpacked folder to the front of PATH set_pypath Adds the NuGet unpacked folder to the front of PYTHONPATH. Also adds it to sys.path. set_build_var Sets a build variable with the key being the name of the ext_dep and the value being the path of the nuget unpacked folder If you include this attribute you must include a var_name (a var that exists internally to the build system that is retrieved with with env.GetValue()) set_shell_var Sets a shell variable with the key being the name of the ext_dep and the value being the path of the nuget unpacked folder If you include this attribute you must include a var_name (a var that exists in the command-line environment via \"set\" or \"os.environ\" or \"env\") include_separator Includes a path seperated at the end of the path we set in variables The following path_env fields are optional or conditional: var_name If either the \"set_shell_var\" or \"set_build_var\" are in the flags, this field will be required. It defines the name of the var being set. id Part of the Override System, this field defines the name that this file will be referred to as by any override_id fields. override_id This file will override any descriptor files found in lower lexical order or scope order. Files are traversed in directory order (depth first) and then scope order (highest to lowest). Overrides are only applied forward in the traversal, not backwards.","title":"path_env Descriptors"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_sde/#the-belly-of-the-beast","text":"","title":"The Belly of the Beast"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_sde/#selfdescribingenvironmentpy","text":"This is the proverbial \"heart of the beast\". It contains most of the business logic for locating, compiling, sorting, filtering, and assembling the SDE files and the environment itself. There are class methods and helper functions to do things like: - Locate all the relevant files in the workspace. - Sort the files lexically and by scope. - Filter the files based on overrides. - Assemble the environment (eg. PATH, PYTHONPATH, vars, etc.). - Validate all dependencies. - Update all dependencies. Many of these routines will leverage logic specific to individual sub-modules (Python, not Git), but the collective logic is located here.","title":"SelfDescribingEnvironment.py"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_sde/#environmentdescriptorfilespy","text":"This module contains business logic and validation code for dealing with the descriptor files as JSON objects. It contains code (and error checking) for loading the files, reading their contents into a standard internal representation, and running a limited set of sanitization and validation functions to identify any mistakes as early as possible and provide as much information as possible. For convenience, this module also contains the class code for PathEnv descriptor objects, but that's because the class code is so small felt silly to create another file.","title":"EnvironmentDescriptorFiles.py"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_sde/#externaldependenciespy","text":"This module contains code for managing external dependencies. ExternalDependency objects are created with the data from ext_dep descriptors and are subclassed according to the \"type\" field in the descriptor. Currently, the only valid subclass is \"nuget\". These objects contain the code for fetching, validating, updating, and cleaning dependency objects and metadata. When referenced from the SDE itself, they can also update paths and other build/shell vars in the build environment.","title":"ExternalDependencies.py"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_sde/#taming-the-sde","text":"","title":"Taming the SDE"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_sde/#understanding-scope","text":"A critical concept in the SDE system is that of \"scope\". Each project can define its own scope, and scope is integral to the distributed and shared nature of the SDE. Project scopes are linearly hierarchical and can have an arbitrary number of entries. Only descriptors matching one or more of the scope entries will be included in the SDE during initialization. Furthermore, higher scopes will take precedence when setting paths and assigning values to vars. An example project scope might be: (\"my_platform\", \"tablet_family\", \"silicon_reference\") In this example, \"my_platform\" is the highest priority in the scope. Any descriptor files found in the entire workspace that have this scope will not only be included in the SDE, they will take precedence over any of the lesser scopes. \"tablet_family\" and \"silicon_reference\" scopes will also be used, in that order. Additionally, all projects inherit the \"global\" scope, but it takes the lowest precedence.","title":"Understanding Scope"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_sde/#setting-up-for-platformbuild","text":"The SDE includes modifications to the PlatformBuild.py script that make it easier to start working with any platform. Since the SDE knows how to fetch its own dependencies, and since all these dependencies are described by the platform itself, the build scripts can now perform the minimal steps to enable building any given platform, including: Synchronizing all required submodules. Downloading all source (and only the source actually used by the platform). Configuring all paths. Downloading all binaries. To leverage this setup behavior, simply run the PlatformBuild.py script corresponding to the platform you want to build with the \"--SETUP\" argument. This argument will cause the platform to configure itself, and display any errors encountered. NOTE: --SETUP should only be required once per build machine, per platform being built. It is not necessary to run it regularly. Only when setting up a new personal workstation or starting to work with a platform that you haven't used yet. The --SETUP feature does not actually build the platform. A normal PlatformBuild.py must still be performed. The --SETUP feature will NOT change branches in any submodule that already exists locally, or that has local changes. This is to prevent accidental loss of work. If you would like the script to try making changes even in these cases, use the \"--FORCE\" argument. The --SETUP feature does not yet install dev singing certs. Those steps must still be performed manually.","title":"Setting Up for PlatformBuild"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_sde/#setting-up-for-mubuild","text":"MuBuild works on a similar mechanism to PlatformBuild but it invokes the SDE directly and does an update. Git Modules are monitored and handled via the RepoResolver framework, which has more logic to it, and doesn't handle submodules.c","title":"Setting Up for MuBuild"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_sde/#building","text":"Building still works as it always has and all prior arguments can still be passed to the PlatformBuild.py script. The only special arguments are \"--SETUP\" and \"--UPDATE\" (described below), which will trigger new behaviors. Note that the current state of the SDE is always printed in the DEBUG level of the build log.","title":"Building"},{"location":"dyn/mu_pip_environment/MuEnvironment/docs/feature_sde/#updating-dependencies","text":"Prior to any build, the SDE will attempt to validate the external dependencies that currently exist on the local machine against the versions that are specified in the code. If the code is updated (perhaps by a pull request to the branch you're working on), it is possible that the dependencies will have to be refreshed. If this is the case, you will see a message prompting you to do so when you run PlatformBuild.py to build your platform. To perform this update, simply run the PlatformBuild.py script with the --UPDATE argument. Any dependencies that match their current versions will be skipped and only out-of-date dependencies will be refreshed.","title":"Updating Dependencies"},{"location":"dyn/mu_pip_python_library/RepoDetails/","text":"Project Mu Pip Python Library \u00b6 Git Details Repository Url: https://github.com/Microsoft/mu_pip_python_library.git Branch: master Commit: c65ba472344ebe154b409c7c30dc9149d45675a6 Commit Date: 2019-12-10 19:59:44 -0800 THIS PROJECT IS NO LONGER ACTIVE - ACTIVE WORK HAS MOVED TO https://github.com/tianocore/edk2-pytool-library \u00b6 Python files describing various miscellaneous components from the TPM and EDKII specs. More Info \u00b6 Please see the Project Mu docs ( https://github.com/Microsoft/mu ) for more information. This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments. Issues \u00b6 Please open any issues in the Project Mu GitHub tracker. More Details Contributing Code or Docs \u00b6 Please follow the general Project Mu Pull Request process. More Details Additionally make sure all testing described in the \"Development\" section passes. Using \u00b6 Usage Details Development \u00b6 Development Details Publish \u00b6 Publish Details Copyright & License \u00b6 Copyright \u00a9 2016-2018, Microsoft Corporation All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"Repo Details"},{"location":"dyn/mu_pip_python_library/RepoDetails/#project-mu-pip-python-library","text":"Git Details Repository Url: https://github.com/Microsoft/mu_pip_python_library.git Branch: master Commit: c65ba472344ebe154b409c7c30dc9149d45675a6 Commit Date: 2019-12-10 19:59:44 -0800","title":"Project Mu Pip Python Library"},{"location":"dyn/mu_pip_python_library/RepoDetails/#this-project-is-no-longer-active-active-work-has-moved-to-httpsgithubcomtianocoreedk2-pytool-library","text":"Python files describing various miscellaneous components from the TPM and EDKII specs.","title":"THIS PROJECT IS NO LONGER ACTIVE - ACTIVE WORK HAS MOVED TO https://github.com/tianocore/edk2-pytool-library"},{"location":"dyn/mu_pip_python_library/RepoDetails/#more-info","text":"Please see the Project Mu docs ( https://github.com/Microsoft/mu ) for more information. This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.","title":"More Info"},{"location":"dyn/mu_pip_python_library/RepoDetails/#issues","text":"Please open any issues in the Project Mu GitHub tracker. More Details","title":"Issues"},{"location":"dyn/mu_pip_python_library/RepoDetails/#contributing-code-or-docs","text":"Please follow the general Project Mu Pull Request process. More Details Additionally make sure all testing described in the \"Development\" section passes.","title":"Contributing Code or Docs"},{"location":"dyn/mu_pip_python_library/RepoDetails/#using","text":"Usage Details","title":"Using"},{"location":"dyn/mu_pip_python_library/RepoDetails/#development","text":"Development Details","title":"Development"},{"location":"dyn/mu_pip_python_library/RepoDetails/#publish","text":"Publish Details","title":"Publish"},{"location":"dyn/mu_pip_python_library/RepoDetails/#copyright-license","text":"Copyright \u00a9 2016-2018, Microsoft Corporation All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"Copyright &amp; License"},{"location":"dyn/mu_pip_python_library/developing/","text":"Developing Project Mu Pip Python Library \u00b6 Pre-Requisites \u00b6 Get the code git clone https://github.com/Microsoft/mu_pip_python_library.git Install development dependencies pip install --upgrade -r requirements.txt Uninstall any copy of mu_python_library pip uninstall mu_python_library Install from local source (run command from root of repo) pip install -e . Testing \u00b6 Run a Basic Syntax/Lint Check (using flake8) and resolve any issues flake8 MuPythonLibrary Info Newer editors are very helpful in resolving source formatting errors (whitespace, indentation, etc). In VSCode open the py file and use ++alt+shift+f++ to auto format. Run pytest with coverage data collected pytest -v --junitxml=test.junit.xml --html=pytest_MuPythonLibrary_report.html --self-contained-html --cov=MuPythonLibrary --cov-report html:cov_html --cov-report xml:cov.xml --cov-config .coveragerc Look at the reports pytest_MuPythonLibrary_report.html cov_html/index.html","title":"developing"},{"location":"dyn/mu_pip_python_library/developing/#developing-project-mu-pip-python-library","text":"","title":"Developing Project Mu Pip Python Library"},{"location":"dyn/mu_pip_python_library/developing/#pre-requisites","text":"Get the code git clone https://github.com/Microsoft/mu_pip_python_library.git Install development dependencies pip install --upgrade -r requirements.txt Uninstall any copy of mu_python_library pip uninstall mu_python_library Install from local source (run command from root of repo) pip install -e .","title":"Pre-Requisites"},{"location":"dyn/mu_pip_python_library/developing/#testing","text":"Run a Basic Syntax/Lint Check (using flake8) and resolve any issues flake8 MuPythonLibrary Info Newer editors are very helpful in resolving source formatting errors (whitespace, indentation, etc). In VSCode open the py file and use ++alt+shift+f++ to auto format. Run pytest with coverage data collected pytest -v --junitxml=test.junit.xml --html=pytest_MuPythonLibrary_report.html --self-contained-html --cov=MuPythonLibrary --cov-report html:cov_html --cov-report xml:cov.xml --cov-config .coveragerc Look at the reports pytest_MuPythonLibrary_report.html cov_html/index.html","title":"Testing"},{"location":"dyn/mu_pip_python_library/publishing/","text":"Publishing Project Mu Pip Python Library \u00b6 The MuPythonLibrary is published as a pypi (pip) module. The pip module is named mu_python_library . Pypi allows for easy version management, dependency management, and sharing. Publishing/releasing a new version is generally handled thru a server based build process but for completeness the process is documented here. Steps \u00b6 Info These directions assume you have already configured your workspace for developing. If not please first do that. Directions on the developing page. Pass all development tests and check. Update the readme with info on changes for this version. Get your changes into master branch (official releases should only be done from the master branch) Make a git tag for the version that will be released. Tag format is v . . Do the release process Install tools pip install --upgrade -r requirements.publisher.txt Build a wheel python setup.py sdist bdist_wheel Confirm wheel version is aligned with git tag ConfirmVersionAndTag.py Publish the wheel/distribution to pypi twine upload dist/*","title":"publishing"},{"location":"dyn/mu_pip_python_library/publishing/#publishing-project-mu-pip-python-library","text":"The MuPythonLibrary is published as a pypi (pip) module. The pip module is named mu_python_library . Pypi allows for easy version management, dependency management, and sharing. Publishing/releasing a new version is generally handled thru a server based build process but for completeness the process is documented here.","title":"Publishing Project Mu Pip Python Library"},{"location":"dyn/mu_pip_python_library/publishing/#steps","text":"Info These directions assume you have already configured your workspace for developing. If not please first do that. Directions on the developing page. Pass all development tests and check. Update the readme with info on changes for this version. Get your changes into master branch (official releases should only be done from the master branch) Make a git tag for the version that will be released. Tag format is v . . Do the release process Install tools pip install --upgrade -r requirements.publisher.txt Build a wheel python setup.py sdist bdist_wheel Confirm wheel version is aligned with git tag ConfirmVersionAndTag.py Publish the wheel/distribution to pypi twine upload dist/*","title":"Steps"},{"location":"dyn/mu_pip_python_library/using/","text":"Using Project Mu Pip Python Library \u00b6 Install from pip pip install mu_python_library Usage Docs \u00b6 TBD","title":"using"},{"location":"dyn/mu_pip_python_library/using/#using-project-mu-pip-python-library","text":"Install from pip pip install mu_python_library","title":"Using Project Mu Pip Python Library"},{"location":"dyn/mu_pip_python_library/using/#usage-docs","text":"TBD","title":"Usage Docs"},{"location":"dyn/mu_pip_python_library/MuPythonLibrary/feature_GetHostInfo/","text":"GetHostInfo \u00b6 This document details the utility function called GetHostInfo. This function was written because NuGet needed a way to determine attributes about the host system to determine what parts of a dependency to use. How to Use \u00b6 from MuPythonLibrary.UtilityFunctions import GetHostInfo host_info = GetHostInfo () Usage info \u00b6 GetHostInfo() will return a namedtuple with 3 attributes describing the host machine. Below for each is the name of the field, description of the field and possible contents therein. 1. os - OS Name \u00b6 Windows, Linux, or Java 2. arch - Processor architecture \u00b6 ARM or x86 3. bit - Highest order bit \u00b6 32 or 64 Purpose \u00b6 Since there are multiple different ways one could derive these values, it is necessary provide a common implementation of that logic to ensure it is uniform.","title":"feature Get Host Info"},{"location":"dyn/mu_pip_python_library/MuPythonLibrary/feature_GetHostInfo/#gethostinfo","text":"This document details the utility function called GetHostInfo. This function was written because NuGet needed a way to determine attributes about the host system to determine what parts of a dependency to use.","title":"GetHostInfo"},{"location":"dyn/mu_pip_python_library/MuPythonLibrary/feature_GetHostInfo/#how-to-use","text":"from MuPythonLibrary.UtilityFunctions import GetHostInfo host_info = GetHostInfo ()","title":"How to Use"},{"location":"dyn/mu_pip_python_library/MuPythonLibrary/feature_GetHostInfo/#usage-info","text":"GetHostInfo() will return a namedtuple with 3 attributes describing the host machine. Below for each is the name of the field, description of the field and possible contents therein.","title":"Usage info"},{"location":"dyn/mu_pip_python_library/MuPythonLibrary/feature_GetHostInfo/#1-os-os-name","text":"Windows, Linux, or Java","title":"1. os - OS Name"},{"location":"dyn/mu_pip_python_library/MuPythonLibrary/feature_GetHostInfo/#2-arch-processor-architecture","text":"ARM or x86","title":"2. arch - Processor architecture"},{"location":"dyn/mu_pip_python_library/MuPythonLibrary/feature_GetHostInfo/#3-bit-highest-order-bit","text":"32 or 64","title":"3. bit - Highest order bit"},{"location":"dyn/mu_pip_python_library/MuPythonLibrary/feature_GetHostInfo/#purpose","text":"Since there are multiple different ways one could derive these values, it is necessary provide a common implementation of that logic to ensure it is uniform.","title":"Purpose"},{"location":"dyn/mu_pip_python_library/MuPythonLibrary/feature_MuAnsiHandler/","text":"GetHostInfo \u00b6 This document details the Ansi Handler How to Use \u00b6 from MuPythonLibrary.MuAnsiHandler import ColoredStreamHandler handler = ColoredStreamHandler ( stream , strip = True , convert = False ) formatter = ColoredFormatter () Usage info \u00b6 ColoredStreamHandler() will create a handler from the logging package. It accepts a stream (such as a file) and will display the colors in that particular stream as needed to the console. There are two options, strip and convert. ColoredFormatter() will create a formater from the logging package that will insert ANSI codes according to the logging level into the output stream. ColoredStreamHandler Arguments \u00b6 1. strip \u00b6 Strip will strip ANSI codes if the terminal does not support them (such as windows). 2. convert \u00b6 Convert will convert ANSI codes on windows platforms into windows platform calls. ColoredFormatter Arguments \u00b6 1. msg \u00b6 The best documentation for this is from Python itself. It's the same message that's passed into the Formatted baseclass. 2. use_azure \u00b6 Azure Dev ops can support colors with certain keywords. This turns that on instead of using ANSI. Purpose \u00b6 To put color into your life and your terminal, we needed to support coloring based on logging levels. ANSI seemed like a universal choice. The StreamHandler is just a workaround for windows based systems that don't support ANSI natively.","title":"feature Mu Ansi Handler"},{"location":"dyn/mu_pip_python_library/MuPythonLibrary/feature_MuAnsiHandler/#gethostinfo","text":"This document details the Ansi Handler","title":"GetHostInfo"},{"location":"dyn/mu_pip_python_library/MuPythonLibrary/feature_MuAnsiHandler/#how-to-use","text":"from MuPythonLibrary.MuAnsiHandler import ColoredStreamHandler handler = ColoredStreamHandler ( stream , strip = True , convert = False ) formatter = ColoredFormatter ()","title":"How to Use"},{"location":"dyn/mu_pip_python_library/MuPythonLibrary/feature_MuAnsiHandler/#usage-info","text":"ColoredStreamHandler() will create a handler from the logging package. It accepts a stream (such as a file) and will display the colors in that particular stream as needed to the console. There are two options, strip and convert. ColoredFormatter() will create a formater from the logging package that will insert ANSI codes according to the logging level into the output stream.","title":"Usage info"},{"location":"dyn/mu_pip_python_library/MuPythonLibrary/feature_MuAnsiHandler/#coloredstreamhandler-arguments","text":"","title":"ColoredStreamHandler Arguments"},{"location":"dyn/mu_pip_python_library/MuPythonLibrary/feature_MuAnsiHandler/#1-strip","text":"Strip will strip ANSI codes if the terminal does not support them (such as windows).","title":"1. strip"},{"location":"dyn/mu_pip_python_library/MuPythonLibrary/feature_MuAnsiHandler/#2-convert","text":"Convert will convert ANSI codes on windows platforms into windows platform calls.","title":"2. convert"},{"location":"dyn/mu_pip_python_library/MuPythonLibrary/feature_MuAnsiHandler/#coloredformatter-arguments","text":"","title":"ColoredFormatter Arguments"},{"location":"dyn/mu_pip_python_library/MuPythonLibrary/feature_MuAnsiHandler/#1-msg","text":"The best documentation for this is from Python itself. It's the same message that's passed into the Formatted baseclass.","title":"1. msg"},{"location":"dyn/mu_pip_python_library/MuPythonLibrary/feature_MuAnsiHandler/#2-use_azure","text":"Azure Dev ops can support colors with certain keywords. This turns that on instead of using ANSI.","title":"2. use_azure"},{"location":"dyn/mu_pip_python_library/MuPythonLibrary/feature_MuAnsiHandler/#purpose","text":"To put color into your life and your terminal, we needed to support coloring based on logging levels. ANSI seemed like a universal choice. The StreamHandler is just a workaround for windows based systems that don't support ANSI natively.","title":"Purpose"},{"location":"dyn/mu_pip_python_library/MuPythonLibrary/bin/vswhere/","text":"This is where VSwhere will go? \u00b6","title":"bin"},{"location":"dyn/mu_pip_python_library/MuPythonLibrary/bin/vswhere/#this-is-where-vswhere-will-go","text":"","title":"This is where VSwhere will go?"},{"location":"dyn/mu_plus/RepoDetails/","text":"Project Mu Common Plus \u00b6 Git Details Repository Url: https://github.com/Microsoft/mu_plus.git Branch: release/201911 Commit: 87950d72ba585ae1b6b7ee13c2f284493eb91707 Commit Date: 2019-12-10 19:40:40 +0000 About \u00b6 This repo contains Project Mu common code that should only take Basecore as a dependency and be applicable to almost any FW project. For full documentation. Please see the Project Mu docs ( https://github.com/Microsoft/mu ) for more information. This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments. This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments. Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Repo Details"},{"location":"dyn/mu_plus/RepoDetails/#project-mu-common-plus","text":"Git Details Repository Url: https://github.com/Microsoft/mu_plus.git Branch: release/201911 Commit: 87950d72ba585ae1b6b7ee13c2f284493eb91707 Commit Date: 2019-12-10 19:40:40 +0000","title":"Project Mu Common Plus"},{"location":"dyn/mu_plus/RepoDetails/#about","text":"This repo contains Project Mu common code that should only take Basecore as a dependency and be applicable to almost any FW project. For full documentation. Please see the Project Mu docs ( https://github.com/Microsoft/mu ) for more information. This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments. This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.","title":"About"},{"location":"dyn/mu_plus/RepoDetails/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_plus/DfciPkg/AuthManagerNull/","text":"AuthManagerNull \u00b6 Purposes \u00b6 Do not use in production! FrontPage during device bringup \u00b6 This driver can be a stand in for IdentityAndAuthManager, which requires RngLib, to allow FrontPage development if RngLib is not yet functional. Unit Testing \u00b6 With further development, this \"Null\" driver could be an effective stub for IdentityAndAuthManager, allowing detailed unit testing of DFCI.","title":"Auth Manager Null"},{"location":"dyn/mu_plus/DfciPkg/AuthManagerNull/#authmanagernull","text":"","title":"AuthManagerNull"},{"location":"dyn/mu_plus/DfciPkg/AuthManagerNull/#purposes","text":"Do not use in production!","title":"Purposes"},{"location":"dyn/mu_plus/DfciPkg/AuthManagerNull/#frontpage-during-device-bringup","text":"This driver can be a stand in for IdentityAndAuthManager, which requires RngLib, to allow FrontPage development if RngLib is not yet functional.","title":"FrontPage during device bringup"},{"location":"dyn/mu_plus/DfciPkg/AuthManagerNull/#unit-testing","text":"With further development, this \"Null\" driver could be an effective stub for IdentityAndAuthManager, allowing detailed unit testing of DFCI.","title":"Unit Testing"},{"location":"dyn/mu_plus/DfciPkg/Docs/Dfci_Feature/","text":"Device Firmware Configuration Interface (DFCI) Introduction \u00b6 Overview \u00b6 The Device Firmware Configuration Interface (DFCI) brings new levels of security and usability to PC configuration management. It is a new feature of UEFI that enables secure programmatic configuration of hardware settings that are typically configured within a BIOS menu by a human. High value configuration can be moved to UEFI BIOS where it is resilient against malware, rootkits, and non-persistent physical tampering. Whereas traditional UEFI security implementations required a physical touch, DFCI securely enables zero-touch remote configuration of these settings built upon Microsoft Intune and authorized by Windows Autopilot . DFCI can provide additional assurance by configuring and locking hardware security features before launching the OS (e.g. disabling microphones or radios). Note that for management of servers in a datacenter, DFCI does not presume to be the solution. Redfish may be a more suitable solution for the datacenter. Why Zero Touch \u00b6 Traditional UEFI management solutions were either not secure, allowing malware to control them, or not scalable, requiring a physical touch by IT or OEM for authentication. DFCI is zero touch, leveraging the existing Windows Autopilot device registration for DFCI authorization. Why should I configure my UEFI BIOS \u00b6 PC configuration is typically performed via Active Directory Group Policy, System Center Configuration Manager (SCCM), or Modern Device Management (MDM) such as Microsoft Intune . All of these solutions store their managed configuration in the OS disk partition. Unfortunately, this configuration can be bypassed by the PCs default ability to boot other operating system instances via external media (e.g. USB), network (e.g. PXE), & alternate disk partitions, or by simply re-installing the OS. Device Firmware Configuration Interface (DFCI) places high value configuration settings into PCs UEFI BIOS. UEFI DFCI storage is both visible to all OS instances, persistent, surviving OS reinstalls and disk reformats, and tamper-resistant, defending itself from malware and rootkits. UEFI executes before the OS and can disallow booting of specified devices, for example USB or network PXE. Further, DFCI can leverage hardware security to enforce some policies with higher assurance than typical OS configuration. For example, it could disable power to cameras or radios in a way that they could not be re-enabled by an OS, malware, or rootkit. Popular Usages \u00b6 Disabling cameras, microphones, and/or radios in manufacturing and other secure facilities Disabling boot to USB and network for single purpose and KIOSK devices Disabling local user access to all UEFI settings to maintain the out of box configuration OEM Enablement Summary \u00b6 DFCI enablement is comprised of: UEFI BIOS implementation Windows Autopilot participation If an OEM or its Partners already participate in the Windows Autopilot program, no additional Autopilot work is required, the only remaining work should be the UEFI BIOS implementation. Windows Autopilot Implementation \u00b6 The pre-existing Windows Autopilot device registration workflows remain unchanged for DFCI, no additional work is required. It should be noted that Autopilot self-registrations are not trusted for the purpose of DFCI management (e.g. from Intune, Microsoft Store for Business, & Business 365). UEFI BIOS Implementation \u00b6 DFCI enablement in UEFI BIOS requires implementation of DFCI interfaces and semantics, and inclusion of a public Microsoft certificate. There is precisely one (1) Microsoft zero-touch certificate that is shared by all DFCI-enabled systems to authenticate zero-touch provisioning requests. Thus there is no requirement to inject the certificate at manufacturing, it may simply be included in the UEFI BIOS image. The DFCI source code and public certificate are available on GitHub under a permissive open source license (SPDX-License-Identifier: BSD-2-Clause-Patent). https://github.com/microsoft/mu_plus/tree/dev/201908/DfciPkg https://github.com/microsoft/mu_plus/tree/dev/201908/ZeroTouchPkg There is also an example UEFI BIOS menu that demonstrates how to integrate DFCI: https://github.com/microsoft/mu_oem_sample/search?q=dfci&unscoped_q=dfci UEFI Implementation Details \u00b6 Scenarios: Building the Microsoft Scenarios with DFCI Integration: Integrating DFCI code into your platforms Architecture: DFCI Code Internals","title":"Dfci Feature"},{"location":"dyn/mu_plus/DfciPkg/Docs/Dfci_Feature/#device-firmware-configuration-interface-dfci-introduction","text":"","title":"Device Firmware Configuration Interface (DFCI) Introduction"},{"location":"dyn/mu_plus/DfciPkg/Docs/Dfci_Feature/#overview","text":"The Device Firmware Configuration Interface (DFCI) brings new levels of security and usability to PC configuration management. It is a new feature of UEFI that enables secure programmatic configuration of hardware settings that are typically configured within a BIOS menu by a human. High value configuration can be moved to UEFI BIOS where it is resilient against malware, rootkits, and non-persistent physical tampering. Whereas traditional UEFI security implementations required a physical touch, DFCI securely enables zero-touch remote configuration of these settings built upon Microsoft Intune and authorized by Windows Autopilot . DFCI can provide additional assurance by configuring and locking hardware security features before launching the OS (e.g. disabling microphones or radios). Note that for management of servers in a datacenter, DFCI does not presume to be the solution. Redfish may be a more suitable solution for the datacenter.","title":"Overview"},{"location":"dyn/mu_plus/DfciPkg/Docs/Dfci_Feature/#why-zero-touch","text":"Traditional UEFI management solutions were either not secure, allowing malware to control them, or not scalable, requiring a physical touch by IT or OEM for authentication. DFCI is zero touch, leveraging the existing Windows Autopilot device registration for DFCI authorization.","title":"Why Zero Touch"},{"location":"dyn/mu_plus/DfciPkg/Docs/Dfci_Feature/#why-should-i-configure-my-uefi-bios","text":"PC configuration is typically performed via Active Directory Group Policy, System Center Configuration Manager (SCCM), or Modern Device Management (MDM) such as Microsoft Intune . All of these solutions store their managed configuration in the OS disk partition. Unfortunately, this configuration can be bypassed by the PCs default ability to boot other operating system instances via external media (e.g. USB), network (e.g. PXE), & alternate disk partitions, or by simply re-installing the OS. Device Firmware Configuration Interface (DFCI) places high value configuration settings into PCs UEFI BIOS. UEFI DFCI storage is both visible to all OS instances, persistent, surviving OS reinstalls and disk reformats, and tamper-resistant, defending itself from malware and rootkits. UEFI executes before the OS and can disallow booting of specified devices, for example USB or network PXE. Further, DFCI can leverage hardware security to enforce some policies with higher assurance than typical OS configuration. For example, it could disable power to cameras or radios in a way that they could not be re-enabled by an OS, malware, or rootkit.","title":"Why should I configure my UEFI BIOS"},{"location":"dyn/mu_plus/DfciPkg/Docs/Dfci_Feature/#popular-usages","text":"Disabling cameras, microphones, and/or radios in manufacturing and other secure facilities Disabling boot to USB and network for single purpose and KIOSK devices Disabling local user access to all UEFI settings to maintain the out of box configuration","title":"Popular Usages"},{"location":"dyn/mu_plus/DfciPkg/Docs/Dfci_Feature/#oem-enablement-summary","text":"DFCI enablement is comprised of: UEFI BIOS implementation Windows Autopilot participation If an OEM or its Partners already participate in the Windows Autopilot program, no additional Autopilot work is required, the only remaining work should be the UEFI BIOS implementation.","title":"OEM Enablement Summary"},{"location":"dyn/mu_plus/DfciPkg/Docs/Dfci_Feature/#windows-autopilot-implementation","text":"The pre-existing Windows Autopilot device registration workflows remain unchanged for DFCI, no additional work is required. It should be noted that Autopilot self-registrations are not trusted for the purpose of DFCI management (e.g. from Intune, Microsoft Store for Business, & Business 365).","title":"Windows Autopilot Implementation"},{"location":"dyn/mu_plus/DfciPkg/Docs/Dfci_Feature/#uefi-bios-implementation","text":"DFCI enablement in UEFI BIOS requires implementation of DFCI interfaces and semantics, and inclusion of a public Microsoft certificate. There is precisely one (1) Microsoft zero-touch certificate that is shared by all DFCI-enabled systems to authenticate zero-touch provisioning requests. Thus there is no requirement to inject the certificate at manufacturing, it may simply be included in the UEFI BIOS image. The DFCI source code and public certificate are available on GitHub under a permissive open source license (SPDX-License-Identifier: BSD-2-Clause-Patent). https://github.com/microsoft/mu_plus/tree/dev/201908/DfciPkg https://github.com/microsoft/mu_plus/tree/dev/201908/ZeroTouchPkg There is also an example UEFI BIOS menu that demonstrates how to integrate DFCI: https://github.com/microsoft/mu_oem_sample/search?q=dfci&unscoped_q=dfci","title":"UEFI BIOS Implementation"},{"location":"dyn/mu_plus/DfciPkg/Docs/Dfci_Feature/#uefi-implementation-details","text":"Scenarios: Building the Microsoft Scenarios with DFCI Integration: Integrating DFCI code into your platforms Architecture: DFCI Code Internals","title":"UEFI Implementation Details"},{"location":"dyn/mu_plus/DfciPkg/Docs/Internals/DfciInternals/","text":"DFCI Internals \u00b6 This section describes the internal operations of DFCI. Communications with Provider \u00b6 DFCI communicates with a controlling identity. One of the controlling identities could be Microsoft Intune. The communications path from the controlling identity is though the use of UEFI variables. DFCI processes the mailbox variables during a system restart. Identity Manager \u00b6 In the source code, the Identity manager is implemented in IdentityAndAuthManager is defined in the DfciPkg located in the mu_plus repository https://github.com/microsoft/mu_plus/ . Identity and Auth Manager is responsible for managing the Identities. The initial state of the system has the Local User with full authentication to make changes to any of the available settings. There are six Identities known by DFCI: Identity Use of the Identity Owner The system owner. Used by a controlling agent - that authorizes Use to control some settings User A delegated user. Used by Microsoft Intune. User1 Not currently used User2 Not currently used Local User Not a certificate - just a known, default, user Zero Touch Limited use Identity to allow an Enroll from a controlling agent. The system has the Zero Touch Certificate installed during manufacturing. Zero Touch cannot be enrolled through the normal enroll operation. Zero Touch has no use when a system is enrolled. The Identity Manager reads the incoming mailbox to process a Identity enroll, Identity certificate update, and Identity unenroll operations. Except for the Local User, when an Identity is enrolled, it means adding a Certificate that will be used to validate incoming settings. The Identity Manager verifies that the incoming identity mailbox packet: Is signed by one of the Identities The signed identity has permission to update the target identity. Target information in the packet matches the system information The one exception is when an new Owner is being enrolled, no Identities validate the mailbox packet, and the Local User has permission to enroll an owner, DFCI will pause booting to prompt the Local User for permission to do the enroll. The user will be asked to validate the enrollment by entering the last two characters of the new owners certificate hash. When installed by the manufacturer of the system, the Zero Touch certificate will have permission to allow the Zero Touch owner packet to be enrolled without user intervention. Hence, the term Zero Touch enrollment. Permissions Manager \u00b6 The permission manager processes incoming permission mailbox packets. Permission packets must be signed by one of Owner, User, User1 or User2. When processing the incoming permissions XML, the signer permissions are used to enable adding or changing a permission. Settings Manager \u00b6 The settings manager processes incoming settings mailbox packets. Settings packets must be signed by one of Owner, User, User1 or User2. When processing the incoming settings XML, the signer permissions are used to change a setting. Identity Packet Formats \u00b6 An Identity packet consists of a binary header, a DER encoded certificate file, a test signature validating the signing capability, and the signature of the packet: The Test Signature is the detached signature of signing the public key certificate by the private key of the public key certificate. The Signature field of the packet is the detached signature of signing Header-PublicCert-TestSignature by: Operation Signing Key Enroll The private key of the matching Public Key Certificate Roll The private key matching the public cert of the Identity being rolled. Unenroll The private key matching the public cert of the Identity being unenrolled. Permission Packet Formats \u00b6 A Permission packet consists of a binary header, an XML payload, and a signature: Sample permission packet: <?xml version=\"1.0\" encoding=\"utf-8\"?> <PermissionsPacket xmlns= \"urn:UefiSettings-Schema\" > <CreatedBy> Cloud Controller </CreatedBy> <CreatedOn> 2018-03-28 </CreatedOn> <Version> 1 </Version> <LowestSupportedVersion> 1 </LowestSupportedVersion> <Permissions Default= \"129\" Delegated= \"192\" Append= \"False\" > <!-- Sample DDS initial enroll permissions Permission Mask - 128 = Owner 64 = User 32 = User1 16 = User2 8 = ZTD 1 = Local User Owner keeps the following settings for itself --> <Permission> <Id> Dfci.OwnerKey.Enum </Id> <PMask> 128 </PMask> <DMask> 128 </DMask> </Permission> <Permission> <Id> Dfci.Recovery.Enable </Id> <PMask> 128 </PMask> <DMask> 128 </DMask> </Permission> <Permission> <!-- Needs 128 (Owner Permission) to set the key, Needs 64 (User Permission) for User to roll the key --> <Id> Dfci.UserKey.Enum </Id> <PMask> 192 </PMask> <DMask> 128 </DMask> </Permission> <Permission> <Id> Dfci.RecoveryBootstrapUrl.String </Id> <PMask> 128 </PMask> <DMask> 128 </DMask> </Permission> <Permission> <Id> Dfci.RecoveryUrl.String </Id> <PMask> 128 </PMask> <DMask> 128 </DMask> </Permission> <Permission> <Id> Dfci.Hwid.String </Id> <PMask> 128 </PMask> <DMask> 128 </DMask> </Permission> </Permissions> </PermissionsPacket> ``` ## Settings Packet Formats ![Setting packet signature](Images/SettingsPacket.jpg) ![Setting packet signature](Images/DataPacketSignature.jpg) Sample settings payload: ```xml <?xml version=\"1.0\" encoding=\"utf-8\"?> <SettingsPacket xmlns= \"urn:UefiSettings-Schema\" > <CreatedBy> Mike Turner </CreatedBy> <CreatedOn> 2019-03-06 10:10:00 </CreatedOn> <Version> 2 </Version> <!-- Make sure you edit DfciSettingsPattern.xml and then run BuildSettings.bat to generate the DfciSettings.xml --> <LowestSupportedVersion> 2 </LowestSupportedVersion> <Settings> <Setting> <Id> Dfci.RecoveryBootstrapUrl.String </Id> <Value> http://some URL to access recovery cert updates/ </Value> </Setting> <Setting> <Id> Dfci.RecoveryUrl.String </Id> <Value> https://some URL to access recovery update packets/ </Value> </Setting> <Setting> <Id> Dfci.HttpsCert.Binary </Id> <Value> <!-- This is where a BASE64 encoded string of the certificate used for HTTPS operations is stored. --> wA== </Value> </Setting> <Setting> <Id> Dfci.RegistrationId.String </Id> <Value> 12345678-1234-5678-1234-012345674321 </Value> </Setting> <Setting> <Id> Dfci.TenantId.String </Id> <Value> 98765432-1234-5678-1234-012345674321 </Value> </Setting> </Settings> </SettingsPacket> Packet Processing \u00b6 In order to minimize rebooting when accepting packets from the owner, there are 6 mailboxes for DFCI. There are two for each Identity, Permission, and Settings. We call them: Identity Identity2 Permission Permission2 Settings Settings2 Only packets of the correct type are processed out of each mailbox. Packets are processed in the following order: Enroll Identity Enroll Identity2 Apply Permission Apply Permission2 At this point, if there is a severe error, the Identities and Permissions are reverted to what they were before processing the packets. The following are still processed, in order. Settings Settings2 Unenroll Identity 2 Unenroll Identity UEFI CSP \u00b6 Intune accesses the variables through the UEFI CSP provider. Out of band recovery \u00b6 Normally, the cloud provider would just send unenroll packets through the OS to the UEFICsp. However, if Windows is unable to boot, the UEFI front page application has a method to contact the owner via HTTPS.","title":"Internals"},{"location":"dyn/mu_plus/DfciPkg/Docs/Internals/DfciInternals/#dfci-internals","text":"This section describes the internal operations of DFCI.","title":"DFCI Internals"},{"location":"dyn/mu_plus/DfciPkg/Docs/Internals/DfciInternals/#communications-with-provider","text":"DFCI communicates with a controlling identity. One of the controlling identities could be Microsoft Intune. The communications path from the controlling identity is though the use of UEFI variables. DFCI processes the mailbox variables during a system restart.","title":"Communications with Provider"},{"location":"dyn/mu_plus/DfciPkg/Docs/Internals/DfciInternals/#identity-manager","text":"In the source code, the Identity manager is implemented in IdentityAndAuthManager is defined in the DfciPkg located in the mu_plus repository https://github.com/microsoft/mu_plus/ . Identity and Auth Manager is responsible for managing the Identities. The initial state of the system has the Local User with full authentication to make changes to any of the available settings. There are six Identities known by DFCI: Identity Use of the Identity Owner The system owner. Used by a controlling agent - that authorizes Use to control some settings User A delegated user. Used by Microsoft Intune. User1 Not currently used User2 Not currently used Local User Not a certificate - just a known, default, user Zero Touch Limited use Identity to allow an Enroll from a controlling agent. The system has the Zero Touch Certificate installed during manufacturing. Zero Touch cannot be enrolled through the normal enroll operation. Zero Touch has no use when a system is enrolled. The Identity Manager reads the incoming mailbox to process a Identity enroll, Identity certificate update, and Identity unenroll operations. Except for the Local User, when an Identity is enrolled, it means adding a Certificate that will be used to validate incoming settings. The Identity Manager verifies that the incoming identity mailbox packet: Is signed by one of the Identities The signed identity has permission to update the target identity. Target information in the packet matches the system information The one exception is when an new Owner is being enrolled, no Identities validate the mailbox packet, and the Local User has permission to enroll an owner, DFCI will pause booting to prompt the Local User for permission to do the enroll. The user will be asked to validate the enrollment by entering the last two characters of the new owners certificate hash. When installed by the manufacturer of the system, the Zero Touch certificate will have permission to allow the Zero Touch owner packet to be enrolled without user intervention. Hence, the term Zero Touch enrollment.","title":"Identity Manager"},{"location":"dyn/mu_plus/DfciPkg/Docs/Internals/DfciInternals/#permissions-manager","text":"The permission manager processes incoming permission mailbox packets. Permission packets must be signed by one of Owner, User, User1 or User2. When processing the incoming permissions XML, the signer permissions are used to enable adding or changing a permission.","title":"Permissions Manager"},{"location":"dyn/mu_plus/DfciPkg/Docs/Internals/DfciInternals/#settings-manager","text":"The settings manager processes incoming settings mailbox packets. Settings packets must be signed by one of Owner, User, User1 or User2. When processing the incoming settings XML, the signer permissions are used to change a setting.","title":"Settings Manager"},{"location":"dyn/mu_plus/DfciPkg/Docs/Internals/DfciInternals/#identity-packet-formats","text":"An Identity packet consists of a binary header, a DER encoded certificate file, a test signature validating the signing capability, and the signature of the packet: The Test Signature is the detached signature of signing the public key certificate by the private key of the public key certificate. The Signature field of the packet is the detached signature of signing Header-PublicCert-TestSignature by: Operation Signing Key Enroll The private key of the matching Public Key Certificate Roll The private key matching the public cert of the Identity being rolled. Unenroll The private key matching the public cert of the Identity being unenrolled.","title":"Identity Packet Formats"},{"location":"dyn/mu_plus/DfciPkg/Docs/Internals/DfciInternals/#permission-packet-formats","text":"A Permission packet consists of a binary header, an XML payload, and a signature: Sample permission packet: <?xml version=\"1.0\" encoding=\"utf-8\"?> <PermissionsPacket xmlns= \"urn:UefiSettings-Schema\" > <CreatedBy> Cloud Controller </CreatedBy> <CreatedOn> 2018-03-28 </CreatedOn> <Version> 1 </Version> <LowestSupportedVersion> 1 </LowestSupportedVersion> <Permissions Default= \"129\" Delegated= \"192\" Append= \"False\" > <!-- Sample DDS initial enroll permissions Permission Mask - 128 = Owner 64 = User 32 = User1 16 = User2 8 = ZTD 1 = Local User Owner keeps the following settings for itself --> <Permission> <Id> Dfci.OwnerKey.Enum </Id> <PMask> 128 </PMask> <DMask> 128 </DMask> </Permission> <Permission> <Id> Dfci.Recovery.Enable </Id> <PMask> 128 </PMask> <DMask> 128 </DMask> </Permission> <Permission> <!-- Needs 128 (Owner Permission) to set the key, Needs 64 (User Permission) for User to roll the key --> <Id> Dfci.UserKey.Enum </Id> <PMask> 192 </PMask> <DMask> 128 </DMask> </Permission> <Permission> <Id> Dfci.RecoveryBootstrapUrl.String </Id> <PMask> 128 </PMask> <DMask> 128 </DMask> </Permission> <Permission> <Id> Dfci.RecoveryUrl.String </Id> <PMask> 128 </PMask> <DMask> 128 </DMask> </Permission> <Permission> <Id> Dfci.Hwid.String </Id> <PMask> 128 </PMask> <DMask> 128 </DMask> </Permission> </Permissions> </PermissionsPacket> ``` ## Settings Packet Formats ![Setting packet signature](Images/SettingsPacket.jpg) ![Setting packet signature](Images/DataPacketSignature.jpg) Sample settings payload: ```xml <?xml version=\"1.0\" encoding=\"utf-8\"?> <SettingsPacket xmlns= \"urn:UefiSettings-Schema\" > <CreatedBy> Mike Turner </CreatedBy> <CreatedOn> 2019-03-06 10:10:00 </CreatedOn> <Version> 2 </Version> <!-- Make sure you edit DfciSettingsPattern.xml and then run BuildSettings.bat to generate the DfciSettings.xml --> <LowestSupportedVersion> 2 </LowestSupportedVersion> <Settings> <Setting> <Id> Dfci.RecoveryBootstrapUrl.String </Id> <Value> http://some URL to access recovery cert updates/ </Value> </Setting> <Setting> <Id> Dfci.RecoveryUrl.String </Id> <Value> https://some URL to access recovery update packets/ </Value> </Setting> <Setting> <Id> Dfci.HttpsCert.Binary </Id> <Value> <!-- This is where a BASE64 encoded string of the certificate used for HTTPS operations is stored. --> wA== </Value> </Setting> <Setting> <Id> Dfci.RegistrationId.String </Id> <Value> 12345678-1234-5678-1234-012345674321 </Value> </Setting> <Setting> <Id> Dfci.TenantId.String </Id> <Value> 98765432-1234-5678-1234-012345674321 </Value> </Setting> </Settings> </SettingsPacket>","title":"Permission Packet Formats"},{"location":"dyn/mu_plus/DfciPkg/Docs/Internals/DfciInternals/#packet-processing","text":"In order to minimize rebooting when accepting packets from the owner, there are 6 mailboxes for DFCI. There are two for each Identity, Permission, and Settings. We call them: Identity Identity2 Permission Permission2 Settings Settings2 Only packets of the correct type are processed out of each mailbox. Packets are processed in the following order: Enroll Identity Enroll Identity2 Apply Permission Apply Permission2 At this point, if there is a severe error, the Identities and Permissions are reverted to what they were before processing the packets. The following are still processed, in order. Settings Settings2 Unenroll Identity 2 Unenroll Identity","title":"Packet Processing"},{"location":"dyn/mu_plus/DfciPkg/Docs/Internals/DfciInternals/#uefi-csp","text":"Intune accesses the variables through the UEFI CSP provider.","title":"UEFI CSP"},{"location":"dyn/mu_plus/DfciPkg/Docs/Internals/DfciInternals/#out-of-band-recovery","text":"Normally, the cloud provider would just send unenroll packets through the OS to the UEFICsp. However, if Windows is unable to boot, the UEFI front page application has a method to contact the owner via HTTPS.","title":"Out of band recovery"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/DfciDeviceIdSupportLib/","text":"DfciDeviceIdSupportLib \u00b6 DfciDeviceIdSupportLib provides DFCI with three platform strings: Manufacturer name Product name Serial number Restrictions on Device Identifier strings \u00b6 Null terminated CHAR8 strings Maximum of 64 CHAR8 values plus a NULL terminator, for a maximum size of 65 bytes. The following five characters are not allowed & ' \" < > UTF-8, as per Wikipedia UTF-8 , are allowed within the 64 CHAR limit and the character set limitations. Interfaces \u00b6 Interface Function DfciIdSupportV1GetSerialNumber Return the system serial number as a UINTN if possible. Otherwise, return 0. DfciIdSupportGetManufacturer Returns an allocated buffer with the system manufacturer name. DfciIdSupportGetProductName Return an allocated buffer with the system product name. DfciIdSupportGetSerialNumber Return an allocated buffer with the system serial number. Additional Details \u00b6 These fields and their values are critical to the security of DFCI. These values should not be user configurable and should be protected from tampering.","title":"Dfci Device Id Support Lib"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/DfciDeviceIdSupportLib/#dfcideviceidsupportlib","text":"DfciDeviceIdSupportLib provides DFCI with three platform strings: Manufacturer name Product name Serial number","title":"DfciDeviceIdSupportLib"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/DfciDeviceIdSupportLib/#restrictions-on-device-identifier-strings","text":"Null terminated CHAR8 strings Maximum of 64 CHAR8 values plus a NULL terminator, for a maximum size of 65 bytes. The following five characters are not allowed & ' \" < > UTF-8, as per Wikipedia UTF-8 , are allowed within the 64 CHAR limit and the character set limitations.","title":"Restrictions on Device Identifier strings"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/DfciDeviceIdSupportLib/#interfaces","text":"Interface Function DfciIdSupportV1GetSerialNumber Return the system serial number as a UINTN if possible. Otherwise, return 0. DfciIdSupportGetManufacturer Returns an allocated buffer with the system manufacturer name. DfciIdSupportGetProductName Return an allocated buffer with the system product name. DfciIdSupportGetSerialNumber Return an allocated buffer with the system serial number.","title":"Interfaces"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/DfciDeviceIdSupportLib/#additional-details","text":"These fields and their values are critical to the security of DFCI. These values should not be user configurable and should be protected from tampering.","title":"Additional Details"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/DfciGroups/","text":"DFCI Groups \u00b6 DFCI Groups allow numerous like typed settings to be managed together. Groups can also be used to provide multiple names for the same setting. This allows actual device settings to be mapped into a different namespaces for settings. One such example from the Microsoft scenario is Dfci.OnboardCameras.Enable . This group setting is used to manage the state of all onboard cameras and gives the management entity an ability to control all cameras regardless of how many each platform has. A platform may have Device.FrontCamera.Enable and Device.RearCamera.Enable settings. Adding those settings to the group Dfci.OnboardCameras.Enable allows a general purpose management entity to control both. Handling State Reporting \u00b6 If all of the settings of the group have the same value then that value will be returned (ie Enabled or Disabled for an Enable type setting). If they are not the same then Inconsistent will be returned. Unknown will be returned if there are no members in a group. Restrictions on group names \u00b6 Group names and settings names are in the same name space and duplicate names are not allowed. Like settings names, group names are limited to 96 characters in length, and are null terminated CHAR8 strings. DfciGroupLib \u00b6 The DfciGroupLib is how groups are managed. This library separates the grouping configuration from the setting providers to allow better flexibility and better maintainability. DfciGroupLib is defined in the DfciPkg located in mu_plus repository ( https://github.com/microsoft/mu_plus/ ). Library Interfaces \u00b6 Interfaces Usage DfciGetGroupEntries DfciGetGroupEntries returns an array of groups, and each group points to a list of settings that are members of the group.","title":"Dfci Groups"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/DfciGroups/#dfci-groups","text":"DFCI Groups allow numerous like typed settings to be managed together. Groups can also be used to provide multiple names for the same setting. This allows actual device settings to be mapped into a different namespaces for settings. One such example from the Microsoft scenario is Dfci.OnboardCameras.Enable . This group setting is used to manage the state of all onboard cameras and gives the management entity an ability to control all cameras regardless of how many each platform has. A platform may have Device.FrontCamera.Enable and Device.RearCamera.Enable settings. Adding those settings to the group Dfci.OnboardCameras.Enable allows a general purpose management entity to control both.","title":"DFCI Groups"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/DfciGroups/#handling-state-reporting","text":"If all of the settings of the group have the same value then that value will be returned (ie Enabled or Disabled for an Enable type setting). If they are not the same then Inconsistent will be returned. Unknown will be returned if there are no members in a group.","title":"Handling State Reporting"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/DfciGroups/#restrictions-on-group-names","text":"Group names and settings names are in the same name space and duplicate names are not allowed. Like settings names, group names are limited to 96 characters in length, and are null terminated CHAR8 strings.","title":"Restrictions on group names"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/DfciGroups/#dfcigrouplib","text":"The DfciGroupLib is how groups are managed. This library separates the grouping configuration from the setting providers to allow better flexibility and better maintainability. DfciGroupLib is defined in the DfciPkg located in mu_plus repository ( https://github.com/microsoft/mu_plus/ ).","title":"DfciGroupLib"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/DfciGroups/#library-interfaces","text":"Interfaces Usage DfciGetGroupEntries DfciGetGroupEntries returns an array of groups, and each group points to a list of settings that are members of the group.","title":"Library Interfaces"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/DfciSettingProviders/","text":"DFCI Settings Providers \u00b6 Settings providers are the foundation of DFCI. Settings providers provide a common method to get and apply a setting. All of the setting providers are linked to the Settings Manager,which published the Setting Access Protocol. All updates to settings that are provided by an anonymous DFCI settings library should be through the Setting Access protocol. The Setting Access protocol will validate the permission of the setting before allowing the setting to be changed. Overview \u00b6 A setting provider may publish more than one settings. Multiple setting providers are aggregated and accessed through the DFCI Setting Access Protocol as shown below: Lets look at what is needed for a single setting in the Setting Provider environment. A setting provider is an anonymous library linked with the Settings Manager DXE driver. Here is how the DfciSampleProvider library is linked with the Settings Manager as an anonymous library: DfciPkg/SettingsManager/SettingsManagerDxe.inf { <PcdsFeatureFlag> gDfciPkgTokenSpaceGuid.PcdSettingsManagerInstallProvider|TRUE <LibraryClasses> NULL|DfciPkg/Library/DfciSampleProvider/DfciSampleProviderLib.inf } Any number of anonymous libraries can be linked with the Settings Manager. Referring to the DfciSampleProvider code, a setting provider defines a setting as: DFCI_SETTING_PROVIDER mDfciSampleProviderProviderSetting1 = { MY_SETTING_ID__SETTING1 , DFCI_SETTING_TYPE_ENABLE , DFCI_SETTING_FLAGS_NO_PREBOOT_UI , // NO UI element for user to change DfciSampleProviderSet , DfciSampleProviderGet , DfciSampleProviderGetDefault , DfciSampleProviderSetDefault }; This particular setting is a ENABLE/DISABLE type of setting, and is telling DFCI that there is no UI element for this setting. When there is no UI element for a setting, DFCI will set the value to the setting Default Value when DFCI is unenrolled. Each setting provider library must have a constructor with code that checks the PCD gDfciPkgTokenSpaceGuid.PcdSettingsManagerInstallProvider. When the constructor is called with the InstallProvider PCD set to TRUE, the setting provider needs to register for a notification of the Settings Provider Support Protocol. When that notification is called, the Settings Provider calls the RegisterProvider method with each setting that the setting provider provides. The constructor looks like: if ( FeaturePcdGet ( PcdSettingsManagerInstallProvider )) { //Install callback on the SettingsManager gDfciSettingsProviderSupportProtocolGuid protocol mDfciSampleProviderProviderSupportInstallEvent = EfiCreateProtocolNotifyEvent ( & gDfciSettingsProviderSupportProtocolGuid , TPL_CALLBACK , DfciSampleProviderProviderSupportProtocolNotify , NULL , & mDfciSampleProviderProviderSupportInstallEventRegistration ); DEBUG (( DEBUG_INFO , \"%a: Event Registered. \\n \" , __FUNCTION__ )); //Initialize the settings store Status = InitializeSettingStore (); if ( EFI_ERROR ( Status )) { DEBUG (( DEBUG_ERROR , \"%a: Initialize Store failed. %r. \\n \" , __FUNCTION__ , Status )); } } return EFI_SUCCESS ; The notify routine looks like: //locate protocol Status = gBS -> LocateProtocol ( & gDfciSettingsProviderSupportProtocolGuid , NULL , ( VOID ** ) & sp ); if ( EFI_ERROR ( Status )) { if (( CallCount ++ != 0 ) || ( Status != EFI_NOT_FOUND )) { DEBUG (( DEBUG_ERROR , \"%a() - Failed to locate gDfciSettingsProviderSupportProtocolGuid in notify. Status = %r \\n \" , __FUNCTION__ , Status )); } return ; } Status = sp -> RegisterProvider ( sp , & mDfciSampleProviderProviderSetting1 ); if ( EFI_ERROR ( Status )) { DEBUG (( DEBUG_ERROR , \"Failed to Register %a. Status = %r \\n \" , mDfciSampleProviderProviderSetting1 . Id , Status )); } //We got here, this means all protocols were installed and we didn't exit early. //close the event as we don't need to be signaled again. (shouldn't happen anyway) gBS -> CloseEvent ( Event ); Setting Provider \u00b6 When you are writing your setting provider, keep in mind that other, similarly written libraries, are linked together. Define each common routine as STATIC to avoid conflicts with other providers. Refer to the UEFI general timeline here: Quite a few settings are only needed in late DXE, BDS or FrontPage. You may need access to settings values in PEI or in DXE prior to the starting of SettingsManager. To do this, add private methods to your settings library. Doing this will keep a single piece of code that accesses the nonvolatile storage for the settings. Here is a sample local setting function in the Dfci Sample Provider: // Here is where you would have private interfaces to get and or set a settings value EFI_STATUS OEM_GetSampleSetting1 ( OUT UINT8 * LocalSetting ) { UINTN LocalSettingsSize ; EFI_STATUS Status ; LocalSettingSize = sizeof ( * LocalSetting ); Status = DfciSampleProviderGet ( & mDfciSampleProviderProviderSetting1 , & LocalSettingSize , & LocalSetting ); return Status ; }","title":"Dfci Setting Providers"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/DfciSettingProviders/#dfci-settings-providers","text":"Settings providers are the foundation of DFCI. Settings providers provide a common method to get and apply a setting. All of the setting providers are linked to the Settings Manager,which published the Setting Access Protocol. All updates to settings that are provided by an anonymous DFCI settings library should be through the Setting Access protocol. The Setting Access protocol will validate the permission of the setting before allowing the setting to be changed.","title":"DFCI Settings Providers"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/DfciSettingProviders/#overview","text":"A setting provider may publish more than one settings. Multiple setting providers are aggregated and accessed through the DFCI Setting Access Protocol as shown below: Lets look at what is needed for a single setting in the Setting Provider environment. A setting provider is an anonymous library linked with the Settings Manager DXE driver. Here is how the DfciSampleProvider library is linked with the Settings Manager as an anonymous library: DfciPkg/SettingsManager/SettingsManagerDxe.inf { <PcdsFeatureFlag> gDfciPkgTokenSpaceGuid.PcdSettingsManagerInstallProvider|TRUE <LibraryClasses> NULL|DfciPkg/Library/DfciSampleProvider/DfciSampleProviderLib.inf } Any number of anonymous libraries can be linked with the Settings Manager. Referring to the DfciSampleProvider code, a setting provider defines a setting as: DFCI_SETTING_PROVIDER mDfciSampleProviderProviderSetting1 = { MY_SETTING_ID__SETTING1 , DFCI_SETTING_TYPE_ENABLE , DFCI_SETTING_FLAGS_NO_PREBOOT_UI , // NO UI element for user to change DfciSampleProviderSet , DfciSampleProviderGet , DfciSampleProviderGetDefault , DfciSampleProviderSetDefault }; This particular setting is a ENABLE/DISABLE type of setting, and is telling DFCI that there is no UI element for this setting. When there is no UI element for a setting, DFCI will set the value to the setting Default Value when DFCI is unenrolled. Each setting provider library must have a constructor with code that checks the PCD gDfciPkgTokenSpaceGuid.PcdSettingsManagerInstallProvider. When the constructor is called with the InstallProvider PCD set to TRUE, the setting provider needs to register for a notification of the Settings Provider Support Protocol. When that notification is called, the Settings Provider calls the RegisterProvider method with each setting that the setting provider provides. The constructor looks like: if ( FeaturePcdGet ( PcdSettingsManagerInstallProvider )) { //Install callback on the SettingsManager gDfciSettingsProviderSupportProtocolGuid protocol mDfciSampleProviderProviderSupportInstallEvent = EfiCreateProtocolNotifyEvent ( & gDfciSettingsProviderSupportProtocolGuid , TPL_CALLBACK , DfciSampleProviderProviderSupportProtocolNotify , NULL , & mDfciSampleProviderProviderSupportInstallEventRegistration ); DEBUG (( DEBUG_INFO , \"%a: Event Registered. \\n \" , __FUNCTION__ )); //Initialize the settings store Status = InitializeSettingStore (); if ( EFI_ERROR ( Status )) { DEBUG (( DEBUG_ERROR , \"%a: Initialize Store failed. %r. \\n \" , __FUNCTION__ , Status )); } } return EFI_SUCCESS ; The notify routine looks like: //locate protocol Status = gBS -> LocateProtocol ( & gDfciSettingsProviderSupportProtocolGuid , NULL , ( VOID ** ) & sp ); if ( EFI_ERROR ( Status )) { if (( CallCount ++ != 0 ) || ( Status != EFI_NOT_FOUND )) { DEBUG (( DEBUG_ERROR , \"%a() - Failed to locate gDfciSettingsProviderSupportProtocolGuid in notify. Status = %r \\n \" , __FUNCTION__ , Status )); } return ; } Status = sp -> RegisterProvider ( sp , & mDfciSampleProviderProviderSetting1 ); if ( EFI_ERROR ( Status )) { DEBUG (( DEBUG_ERROR , \"Failed to Register %a. Status = %r \\n \" , mDfciSampleProviderProviderSetting1 . Id , Status )); } //We got here, this means all protocols were installed and we didn't exit early. //close the event as we don't need to be signaled again. (shouldn't happen anyway) gBS -> CloseEvent ( Event );","title":"Overview"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/DfciSettingProviders/#setting-provider","text":"When you are writing your setting provider, keep in mind that other, similarly written libraries, are linked together. Define each common routine as STATIC to avoid conflicts with other providers. Refer to the UEFI general timeline here: Quite a few settings are only needed in late DXE, BDS or FrontPage. You may need access to settings values in PEI or in DXE prior to the starting of SettingsManager. To do this, add private methods to your settings library. Doing this will keep a single piece of code that accesses the nonvolatile storage for the settings. Here is a sample local setting function in the Dfci Sample Provider: // Here is where you would have private interfaces to get and or set a settings value EFI_STATUS OEM_GetSampleSetting1 ( OUT UINT8 * LocalSetting ) { UINTN LocalSettingsSize ; EFI_STATUS Status ; LocalSettingSize = sizeof ( * LocalSetting ); Status = DfciSampleProviderGet ( & mDfciSampleProviderProviderSetting1 , & LocalSettingSize , & LocalSetting ); return Status ; }","title":"Setting Provider"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/DfciUiSupportLib/","text":"DfciUiSupportLib \u00b6 DfciUiSupportLib allows DFCI to communicate with the user during DFCI initialization, enrollment, or to indicate a non secure environment is available. Interfaces \u00b6 Interface Usage DfciUiIsManufacturingMode Returns TRUE or FALSE. Used to self OptIn a cert used for zero touch enrollment. If the device doesn't support a manufacturing mode, return FALSE. DfciUiIsAvailable For one touch enrollment, the user has to authorize the enrollment. Since DFCI normally runs before consoles are started, DFCI will wait until END_OF_DXE and then make sure the User Interface (UI) is available. If no UI is available at that time, the enrollment will fail. DfciUiDisplayMessageBox Displays a message box DfciUiDisplayPasswordDialog Displays a prompt for the ADMIN password DfciUiDisplayAuthDialog Displays a prompt for confirmation of an enrolling certificate. The response is the last two characters of the thumbprint. If there is an ADMIN password set, then this dialog will also request the ADMIN password. DfciUiExitSecurityBoundary The platform settings application usually runs in a secure state before variables are locked. The DFCI Menu application will call ExitSecurityBoundary before starting the network or performing USB operations to minimize the security risks associated with external accesses.","title":"Dfci Ui Support Lib"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/DfciUiSupportLib/#dfciuisupportlib","text":"DfciUiSupportLib allows DFCI to communicate with the user during DFCI initialization, enrollment, or to indicate a non secure environment is available.","title":"DfciUiSupportLib"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/DfciUiSupportLib/#interfaces","text":"Interface Usage DfciUiIsManufacturingMode Returns TRUE or FALSE. Used to self OptIn a cert used for zero touch enrollment. If the device doesn't support a manufacturing mode, return FALSE. DfciUiIsAvailable For one touch enrollment, the user has to authorize the enrollment. Since DFCI normally runs before consoles are started, DFCI will wait until END_OF_DXE and then make sure the User Interface (UI) is available. If no UI is available at that time, the enrollment will fail. DfciUiDisplayMessageBox Displays a message box DfciUiDisplayPasswordDialog Displays a prompt for the ADMIN password DfciUiDisplayAuthDialog Displays a prompt for confirmation of an enrolling certificate. The response is the last two characters of the thumbprint. If there is an ADMIN password set, then this dialog will also request the ADMIN password. DfciUiExitSecurityBoundary The platform settings application usually runs in a secure state before variables are locked. The DFCI Menu application will call ExitSecurityBoundary before starting the network or performing USB operations to minimize the security risks associated with external accesses.","title":"Interfaces"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/PlatformIntegrationOverview/","text":"Platform Integration of DFCI \u00b6 This section of documentation is focused on UEFI firmware developers and helping them enable their platforms with the DFCI feature. DFCI consists mostly of a software feature that is written in the DXE phase of UEFI. It has numerous architecture and platform independent modules with only a few required platform libraries. It also requires the platform adhere to and use the DFCI components to ensure the DFCI features work as designed. Finally to enable an End-To-End management scenario there maybe custom requirements in adjacent UEFI firmware components. Dfci Menu application \u00b6 The DfciMenu application is optimized for mu_plus MsGraphicsPkg . It is VFR but since many platforms use custom layouts and graphical representation this area might need some adjustments. The DfciMenu application publishes a HII formset that should be located by your pre-boot UEFI menu application (e.g. \"FrontPage\") and displayed. Formset GUID: gDfciMenuFormsetGuid = {0x3b82283d, 0x7add, 0x4c6a, {0xad, 0x2b, 0x71, 0x9b, 0x8d, 0x7b, 0x77, 0xc9 }} Entry Form: #define DFCI_MENU_FORM_ID 0x2000 Source Location: DfciPkg\\Application\\DfciMenu DFCI DXE Drivers \u00b6 Dxe Driver Location DfciManager.efi DfciPkg/DfciManager/DfciManager.inf IdentityAndAuthManager.efi DfciPkg/IdentityAndAuthManager/IdentityAndAuthManagerDxe.inf SettingsManager.efi DfciPkg/SettingsManager/SettingsManagerDxe.inf DfciMenu.inf DfciPkg/Application/DfciMenu/DfciMenu.inf DFCI Core Libraries \u00b6 These DFCI Standard libraries are expected to be used as is for standard functionality. Library Location DfciRecoveryLib DfciPkg/Library/DfciRecoveryLib/DfciRecoveryLib.inf DfciSettingsLib DfciPkg/Library/DfciSettingsLib/DfciSettingsLib.inf DfciV1SupportLib DfciPkg/Library/DfciV1SupportLibNull/DfciV1SupportLibNull.inf DfciXmlDeviceIdSchemaSupportLib DfciPkg/Library/DfciXmlDeviceIdSchemaSupportLib/DfciXmlDeviceIdSchemaSupportLib.inf DfciXmlIdentitySchemaSupportLib DfciPkg/Library/DfciXmlIdentitySchemaSupportLib/DfciXmlIdentitySchemaSupportLib.inf DfciXmlPermissionSchemaSupportLib DfciPkg/Library/DfciXmlPermissionSchemaSupportLib/DfciXmlPermissionSchemaSupportLib.inf DfciXmlSettingSchemaSupportLib DfciPkg/Library/DfciXmlSettingSchemaSupportLib/DfciXmlSettingSchemaSupportLib.inf ZeroTouchSettingsLib ZeroTouchPkg/Library/ZeroTouchSettings/ZeroTouchSettings.inf DfciSettingPermissionLib DfciPkg/Library/DfciSettingPermissionLib/DfciSettingPermissionLib.inf DFCI Platform provided libraries \u00b6 The following libraries have to be provided by the platform: Library Documentation Function DfciDeviceIdSupportLib Documentation Provides SMBIOS information - Manufacturer, Product, and Serial number DfciGroupLib Documentation Provides lists of platform settings that are in the Dfci group settings. DfciUiSupportLib Documentation Provides UI for various user interactions DFCI Setting Providers \u00b6 Setting providers is how a platform provides a setting to DFCI Setting detailed overview Platform DSC statements \u00b6 Adding DFCI to your system consists of: Write your settings providers. Use DfciPkg/Library/DfciSampleProvider . Writing three library classes for the DfciDeviceIdSupportLib, DfciGroupLib, and DfciUiSupportLib. Adding the DSC sections below. Adding the FDF sections below. [ LibraryClasses.XXX ] DfciXmlSettingSchemaSupportLib|DfciPkg/Library/DfciXmlSettingSchemaSupportLib/DfciXmlSettingSchemaSupportLib.inf DfciXmlPermissionSchemaSupportLib|DfciPkg/Library/DfciXmlPermissionSchemaSupportLib/DfciXmlPermissionSchemaSupportLib.inf DfciXmlDeviceIdSchemaSupportLib|DfciPkg/Library/DfciXmlDeviceIdSchemaSupportLib/DfciXmlDeviceIdSchemaSupportLib.inf DfciXmlIdentitySchemaSupportLib|DfciPkg/Library/DfciXmlIdentitySchemaSupportLib/DfciXmlIdentitySchemaSupportLib.inf ZeroTouchSettingsLib|ZeroTouchPkg/Library/ZeroTouchSettings/ZeroTouchSettings.inf DfciRecoveryLib|DfciPkg/Library/DfciRecoveryLib/DfciRecoveryLib.inf DfciSettingsLib|DfciPkg/Library/DfciSettingsLib/DfciSettingsLib.inf DfciV1SupportLib|DfciPkg/Library/DfciV1SupportLibNull/DfciV1SupportLibNull.inf DfciDeviceIdSupportLib|YOURPLATFORMPKG/Library/DfciDeviceIdSupportLib/DfciDeviceIdSupportLib.inf DfciUiSupportLib|YOURPLATFORMPKG/Library/DfciUiSupportLib/DfciUiSupportLib.inf DfciGroupLib|YOURPLATFORMPKG/Library/DfciGroupLib/DfciGroups.inf [Components.XXX] DfciPkg/SettingsManager/SettingsManagerDxe.inf { #Platform should add all it settings libs here <LibraryClasses> NULL|ZeroTouchPkg/Library/ZeroTouchSettings/ZeroTouchSettings.inf NULL|YOUR_PLATFORM_PKG/Library/YOUR_FIRST_SETTING_PROVIDER.inf NULL|YOUR_PLATFORM_PKG/Library/YOUR_SECOND_SETTING_PROVIDER.inf NULL|DfciPkg/Library/DfciPasswordProvider/DfciPasswordProvider.inf NULL|DfciPkg/Library/DfciSettingsLib/DfciSettingsLib.inf NULL|DfciPkg/Library/DfciVirtualizationSettings/DfciVirtualizationSettings.inf DfciSettingPermissionLib|DfciPkg/Library/DfciSettingPermissionLib/DfciSettingPermissionLib.inf <PcdsFeatureFlag> gDfciPkgTokenSpaceGuid.PcdSettingsManagerInstallProvider|TRUE } DfciPkg/IdentityAndAuthManager/IdentityAndAuthManagerDxe.inf DfciPkg/DfciManager/DfciManager.inf DfciPkg/Application/DfciMenu/DfciMenu.inf Platform FDF statements \u00b6 [ FV.YOUR_DXE_FV ] INF DfciPkg/SettingsManager/SettingsManagerDxe.inf INF DfciPkg/IdentityAndAuthManager/IdentityAndAuthManagerDxe.inf INF DfciPkg/Application/DfciMenu/DfciMenu.inf INF DfciPkg/DfciManager/DfciManager.inf","title":"Platform Integration Overview"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/PlatformIntegrationOverview/#platform-integration-of-dfci","text":"This section of documentation is focused on UEFI firmware developers and helping them enable their platforms with the DFCI feature. DFCI consists mostly of a software feature that is written in the DXE phase of UEFI. It has numerous architecture and platform independent modules with only a few required platform libraries. It also requires the platform adhere to and use the DFCI components to ensure the DFCI features work as designed. Finally to enable an End-To-End management scenario there maybe custom requirements in adjacent UEFI firmware components.","title":"Platform Integration of DFCI"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/PlatformIntegrationOverview/#dfci-menu-application","text":"The DfciMenu application is optimized for mu_plus MsGraphicsPkg . It is VFR but since many platforms use custom layouts and graphical representation this area might need some adjustments. The DfciMenu application publishes a HII formset that should be located by your pre-boot UEFI menu application (e.g. \"FrontPage\") and displayed. Formset GUID: gDfciMenuFormsetGuid = {0x3b82283d, 0x7add, 0x4c6a, {0xad, 0x2b, 0x71, 0x9b, 0x8d, 0x7b, 0x77, 0xc9 }} Entry Form: #define DFCI_MENU_FORM_ID 0x2000 Source Location: DfciPkg\\Application\\DfciMenu","title":"Dfci Menu application"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/PlatformIntegrationOverview/#dfci-dxe-drivers","text":"Dxe Driver Location DfciManager.efi DfciPkg/DfciManager/DfciManager.inf IdentityAndAuthManager.efi DfciPkg/IdentityAndAuthManager/IdentityAndAuthManagerDxe.inf SettingsManager.efi DfciPkg/SettingsManager/SettingsManagerDxe.inf DfciMenu.inf DfciPkg/Application/DfciMenu/DfciMenu.inf","title":"DFCI DXE Drivers"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/PlatformIntegrationOverview/#dfci-core-libraries","text":"These DFCI Standard libraries are expected to be used as is for standard functionality. Library Location DfciRecoveryLib DfciPkg/Library/DfciRecoveryLib/DfciRecoveryLib.inf DfciSettingsLib DfciPkg/Library/DfciSettingsLib/DfciSettingsLib.inf DfciV1SupportLib DfciPkg/Library/DfciV1SupportLibNull/DfciV1SupportLibNull.inf DfciXmlDeviceIdSchemaSupportLib DfciPkg/Library/DfciXmlDeviceIdSchemaSupportLib/DfciXmlDeviceIdSchemaSupportLib.inf DfciXmlIdentitySchemaSupportLib DfciPkg/Library/DfciXmlIdentitySchemaSupportLib/DfciXmlIdentitySchemaSupportLib.inf DfciXmlPermissionSchemaSupportLib DfciPkg/Library/DfciXmlPermissionSchemaSupportLib/DfciXmlPermissionSchemaSupportLib.inf DfciXmlSettingSchemaSupportLib DfciPkg/Library/DfciXmlSettingSchemaSupportLib/DfciXmlSettingSchemaSupportLib.inf ZeroTouchSettingsLib ZeroTouchPkg/Library/ZeroTouchSettings/ZeroTouchSettings.inf DfciSettingPermissionLib DfciPkg/Library/DfciSettingPermissionLib/DfciSettingPermissionLib.inf","title":"DFCI Core Libraries"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/PlatformIntegrationOverview/#dfci-platform-provided-libraries","text":"The following libraries have to be provided by the platform: Library Documentation Function DfciDeviceIdSupportLib Documentation Provides SMBIOS information - Manufacturer, Product, and Serial number DfciGroupLib Documentation Provides lists of platform settings that are in the Dfci group settings. DfciUiSupportLib Documentation Provides UI for various user interactions","title":"DFCI Platform provided libraries"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/PlatformIntegrationOverview/#dfci-setting-providers","text":"Setting providers is how a platform provides a setting to DFCI Setting detailed overview","title":"DFCI Setting Providers"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/PlatformIntegrationOverview/#platform-dsc-statements","text":"Adding DFCI to your system consists of: Write your settings providers. Use DfciPkg/Library/DfciSampleProvider . Writing three library classes for the DfciDeviceIdSupportLib, DfciGroupLib, and DfciUiSupportLib. Adding the DSC sections below. Adding the FDF sections below. [ LibraryClasses.XXX ] DfciXmlSettingSchemaSupportLib|DfciPkg/Library/DfciXmlSettingSchemaSupportLib/DfciXmlSettingSchemaSupportLib.inf DfciXmlPermissionSchemaSupportLib|DfciPkg/Library/DfciXmlPermissionSchemaSupportLib/DfciXmlPermissionSchemaSupportLib.inf DfciXmlDeviceIdSchemaSupportLib|DfciPkg/Library/DfciXmlDeviceIdSchemaSupportLib/DfciXmlDeviceIdSchemaSupportLib.inf DfciXmlIdentitySchemaSupportLib|DfciPkg/Library/DfciXmlIdentitySchemaSupportLib/DfciXmlIdentitySchemaSupportLib.inf ZeroTouchSettingsLib|ZeroTouchPkg/Library/ZeroTouchSettings/ZeroTouchSettings.inf DfciRecoveryLib|DfciPkg/Library/DfciRecoveryLib/DfciRecoveryLib.inf DfciSettingsLib|DfciPkg/Library/DfciSettingsLib/DfciSettingsLib.inf DfciV1SupportLib|DfciPkg/Library/DfciV1SupportLibNull/DfciV1SupportLibNull.inf DfciDeviceIdSupportLib|YOURPLATFORMPKG/Library/DfciDeviceIdSupportLib/DfciDeviceIdSupportLib.inf DfciUiSupportLib|YOURPLATFORMPKG/Library/DfciUiSupportLib/DfciUiSupportLib.inf DfciGroupLib|YOURPLATFORMPKG/Library/DfciGroupLib/DfciGroups.inf [Components.XXX] DfciPkg/SettingsManager/SettingsManagerDxe.inf { #Platform should add all it settings libs here <LibraryClasses> NULL|ZeroTouchPkg/Library/ZeroTouchSettings/ZeroTouchSettings.inf NULL|YOUR_PLATFORM_PKG/Library/YOUR_FIRST_SETTING_PROVIDER.inf NULL|YOUR_PLATFORM_PKG/Library/YOUR_SECOND_SETTING_PROVIDER.inf NULL|DfciPkg/Library/DfciPasswordProvider/DfciPasswordProvider.inf NULL|DfciPkg/Library/DfciSettingsLib/DfciSettingsLib.inf NULL|DfciPkg/Library/DfciVirtualizationSettings/DfciVirtualizationSettings.inf DfciSettingPermissionLib|DfciPkg/Library/DfciSettingPermissionLib/DfciSettingPermissionLib.inf <PcdsFeatureFlag> gDfciPkgTokenSpaceGuid.PcdSettingsManagerInstallProvider|TRUE } DfciPkg/IdentityAndAuthManager/IdentityAndAuthManagerDxe.inf DfciPkg/DfciManager/DfciManager.inf DfciPkg/Application/DfciMenu/DfciMenu.inf","title":"Platform DSC statements"},{"location":"dyn/mu_plus/DfciPkg/Docs/PlatformIntegration/PlatformIntegrationOverview/#platform-fdf-statements","text":"[ FV.YOUR_DXE_FV ] INF DfciPkg/SettingsManager/SettingsManagerDxe.inf INF DfciPkg/IdentityAndAuthManager/IdentityAndAuthManagerDxe.inf INF DfciPkg/Application/DfciMenu/DfciMenu.inf INF DfciPkg/DfciManager/DfciManager.inf","title":"Platform FDF statements"},{"location":"dyn/mu_plus/DfciPkg/Docs/Scenarios/DfciScenarios/","text":"Microsoft DFCI Scenarios \u00b6 Overview \u00b6 Microsoft leverages DFCI to provide automated UEFI settings management via Microsoft Intune . Intune provides the IT manager with abstracted, easy button settings that are generally applicable to all platforms, for example, disable booting USB devices or disable all cameras. IT managers depend on the UEFI implementation to protect and enforce DFCI configurations such that they cannot be bypassed by an operating system or casual physical attacker. Windows Autopilot provides the trusted device ownership database, mapping devices to Azure Active Directory Tenants. The \"Microsoft Device Management Trust\" certificate must be included in UEFI to act as the root of trust for automated UEFI management. The Autopilot Service (APS) exposes a cloud endpoint to enable recovery from a BIOS menu in case the device can no longer boot due to misconfiguration or disk corruption. Microsoft DFCI Scenario Requirements \u00b6 PCs must include the DFCI feature in their UEFI PCs must be registered to the Windows Autopilot service by an OEM or Microsoft Cloud Solution Provider PCs must be managed with Microsoft Intune OEMs that support DFCI \u00b6 More are in the works... Lifecycle \u00b6 The DFCI lifecycle can be viewed as UEFI integration, device registration, profile creation, enrollment, management, retirement, & recovery. Stage Description UEFI Integration PCs must first include a UEFI BIOS that integrates the DFCI code and includes the Microsoft Device Management Trust certificate. Device Registration Device ownership must be registered via the Windows Autopilot program by an OEM or Microsoft Cloud Solution Provider Profile Creation An IT administrator leverages Intune to create DFCI Profiles for their devices. Enrollment The DFCI enrollment process is kicked off when a PC is enrolled into Intune and has a matching DFCI Profile. Enrollment includes Intune requesting enrollment packets from APS, sending the packets to the Windows UEFI configuration service provider (CSP) endpoints , the CSP writes the packets to UEFI variables, and triggers an OS reboot to allow UEFI firmware to process the DFCI packets. Management For day-to-day management, Intune creates device-specific packets, digitally signs them, and sends them through the same UEFI configuration service provider , UEFI variable, and reboot process. Retirement When a device is removed from Windows Autopilot, they are marked as unenrolled in APS. Intune will attempt to restore permissions (un-grey all settings) and remove its management authority from the device. Recovery Recovery shall be provided via a pre-boot UEFI menu, always available to a physically-present user, that can refresh DFCI configuration via web, USB, or other. Enrollment Flow \u00b6 Prior to the time of Enrollment, Microsoft Device Management Trust delegates management to the APS by signing a wildcard enrollment packet (targeting all manufacturer, model, & serial number) that authorizes enrollment of the APS certificate. At the request of Intune, the APS authorizes enrollment of a device, creates and signs per-device-targeted enrollment packets that enroll the Intune DFCI management certificate. The APS provides a level of indirection as well as an extra level of recovery via a web recovery service endpoint. The APS additionally configures recovery settings as well as permissions that deny Intune access to modify them. DEFINED: A device is considered \"enrolled\" when IS_OWNER_IDENTITY_ENROLLED(IdMask) returns TRUE, unenrolled when FALSE. Intune creates device-specific packets to provision the Intune authority and configure DFCI settings and permissions as specified in the matching Intune DFCI Profile. Intune delivers these packets via the Windows UEFI configuration service provider which writes them to UEFI mailboxes, to be processed by DFCI on the following boot. Device-specific packets include the SMBIOS manufacturer, model, & serial number, along with an anti-rollback count, to be leveraged by the UEFI DFCI code to determine applicability. Retirement & Recovery Flows \u00b6 Retirement \u00b6 An IT administrator leverages the Intune console to remove devices from Autopilot. Intune creates and sends device-specific packets that both restore DFCI permissions (effectively un-managing settings, making them available in the BIOS menu) and remove the Intune authority from DFCI. Note that this does not restore the settings to default values, they remain as is. Intune also notifies APS that the device is in the unenrolled state. If the device owner wants to further remove the APS authority and/or opt-out of DFCI management, they must leverage the Recovery flow. Recovery \u00b6 Recovery is essential because UEFI misconfiguration may prevent booting to an operating system, for example if USB and network boot are disabled and the hard disk becomes corrupted. When a device is enrolled, UEFI must provide alternative mechanisms for the physically-present user to place packets in the DFCI request mailboxes - this MUST NOT be blocked by a BIOS password or similar. Note that when a device is not enrolled, a BIOS password should prevent access to DFCI enrollment by a physically-present user until they have entered the correct credential. The APS keeps track of the enrollment state of devices. When an administrator removes a device from Autopilot, APS creates signed, device-specific un-enrollment packets and makes them available via a REST endpoint at DFCI_SETTING_ID__DFCI_RECOVERY_URL. These packets should delete the Intune and APS certificates and provide the local user with access to all settings (they should no longer be greyed out in BIOS menus). Note that retirement does not restore visible settings to their default values. Standard Settings \u00b6 The following standard settings are defined for DFCI v1.0, will be exposed by Intune, and Settings Providers for them must be implemented by the UEFI provider. Note that for device management, the settings apply only to built-in devices, not externally attached devices. All cameras All audio (microphones & speakers) All radios (Wi-Fi, broadband, NFC, BT, ...) CPU & IO virtualization (exposed, enabled for use by the OS so that Virtualization Based Security may be enabled) Boot to external media (e.g. USB, SD, ...) Boot via on-board network devices Refer to the \"Group Settings\" section of DfciSettings.h Device Ownership \u00b6 The Windows Autopilot database is used for authorizing DFCI management. It includes a map of device identifiers an owner's Azure Active Directory tenant. It is populated as part of the Windows Autopilot program by OEMs and Microsoft-authorized Cloud Solution Providers . The DFCI UEFI code leverages the SMBIOS manufacturer, model, and serial number for packet targeting. DFCI is a Windows Autopilot feature, available from Microsoft Intune. Management Authorities \u00b6 DFCI supports UEFI settings and permission management by multiple entities. In the recommended configuration, systems are shipped with 1 Microsoft public certificate included, \"CN=Microsoft Device Management Trust\", which provides the root of trust for management. This certificate authorizes Microsoft to automatically enroll management delegates with varying permissions. \"Microsoft Device Management Trust\" is only used to delegate management to APS and to provide second-chance recovery. APS in turn delegates management to Intune and provides a REST endpoint for online recovery. After enrollment, Intune performs the day-to-day UEFI management, whereas APS and Device Management Trust authorities provide various recovery paths. Authority DFCI ID Usage \"CN=Microsoft Device Management Trust\" DFCI_IDENTITY_SIGNER_ZTD Allowed to enroll a management delegates without a physical presence prompt. Enrolls the Autopilot Service authority. Can act as a backup recovery service. Autopilot Service (APS) DFCI_IDENTITY_SIGNER_OWNER Enrolls Microsoft Intune as a delegated management provider. Provides an online recovery service in case the OS is disconnected from Intune. Microsoft Intune DFCI_IDENTITY_SIGNER_USER Performs day-to-day UEFI settings and permissions management Security & Privacy Considerations \u00b6 When a DFCI owner is enrolled, DFCI must take precedence over any other UEFI management solution. Physically-present user, including authenticated, may not bypass DFCI permissions on DFCI_IDENTITY_LOCAL . Protection: The hardware/firmware implementation of DFCI must protect DFCI configuration code and data such that they cannot by bypassed by an operating system or casual physical attacker. For example, non-volatile storage that is locked prior to Boot Device Selection. Enforcement: The hardware/firmware implementation of DFCI must enforce DFCI configurations such that they cannot by bypassed by an operating system or casual physical attacker. For example, power to devices is disabled or busses disabled and the configuration is locked prior to Boot Device Selection. It is recommended to include an \"Opt Out\" button that enables a physically-present user on an unenrolled device to eject the DFCI_IDENTITY_SIGNER_ZTD from DFCI. The effectively disables automated management - any enrollment attempt will display a red prompt at boot. A user could then prevent enrollment by configuring a BIOS password or enroll their own User certificate (proceeding through the red prompt). Online Recovery via the Autopilot Service \u00b6 The Recovery REST interface includes machine identities. Before transferring machine identities, the server's authenticity should be verified against DFCI_SETTING_ID__DFCI_HTTPS_CERT. After authenticating the server, the network traffic, including machine identities, are kept private by HTTPS encryption. But wait, there's more... the server certificate is updated regularly, so UEFI must first ensure it has the up-to-date DFCI_SETTING_ID__DFCI_HTTPS_CERT. DFCI_SETTING_ID__DFCI_BOOTSTRAP_URL provides a REST API to download a signed settings packet containing DFCI_SETTING_ID__DFCI_HTTPS_CERT. For this workflow, the server is not authenticated, but the payload will be authenticated prior to consumption. Unknown Certificate Enrollment \u00b6 This is not a Microsoft-supported scenario but might be encountered during development and testing. On an unenrolled system, if enrollment packets are supplied to the DFCI mailboxes that are signed by an unknown certificate, a red authorization prompt is displayed during boot. The prompt requests the physically-present user to authorize the enrollment of the unknown certificate by typing the last 2 characters of the certificate's SHA-1 thumbprint. If a BIOS password is configured, the password must be entered prior to authorizing the enrollment. This is designed to avoid accidental or nefarious enrollment while allowing for valid custom identity management.","title":"Scenarios"},{"location":"dyn/mu_plus/DfciPkg/Docs/Scenarios/DfciScenarios/#microsoft-dfci-scenarios","text":"","title":"Microsoft DFCI Scenarios"},{"location":"dyn/mu_plus/DfciPkg/Docs/Scenarios/DfciScenarios/#overview","text":"Microsoft leverages DFCI to provide automated UEFI settings management via Microsoft Intune . Intune provides the IT manager with abstracted, easy button settings that are generally applicable to all platforms, for example, disable booting USB devices or disable all cameras. IT managers depend on the UEFI implementation to protect and enforce DFCI configurations such that they cannot be bypassed by an operating system or casual physical attacker. Windows Autopilot provides the trusted device ownership database, mapping devices to Azure Active Directory Tenants. The \"Microsoft Device Management Trust\" certificate must be included in UEFI to act as the root of trust for automated UEFI management. The Autopilot Service (APS) exposes a cloud endpoint to enable recovery from a BIOS menu in case the device can no longer boot due to misconfiguration or disk corruption.","title":"Overview"},{"location":"dyn/mu_plus/DfciPkg/Docs/Scenarios/DfciScenarios/#microsoft-dfci-scenario-requirements","text":"PCs must include the DFCI feature in their UEFI PCs must be registered to the Windows Autopilot service by an OEM or Microsoft Cloud Solution Provider PCs must be managed with Microsoft Intune","title":"Microsoft DFCI Scenario Requirements"},{"location":"dyn/mu_plus/DfciPkg/Docs/Scenarios/DfciScenarios/#oems-that-support-dfci","text":"More are in the works...","title":"OEMs that support DFCI"},{"location":"dyn/mu_plus/DfciPkg/Docs/Scenarios/DfciScenarios/#lifecycle","text":"The DFCI lifecycle can be viewed as UEFI integration, device registration, profile creation, enrollment, management, retirement, & recovery. Stage Description UEFI Integration PCs must first include a UEFI BIOS that integrates the DFCI code and includes the Microsoft Device Management Trust certificate. Device Registration Device ownership must be registered via the Windows Autopilot program by an OEM or Microsoft Cloud Solution Provider Profile Creation An IT administrator leverages Intune to create DFCI Profiles for their devices. Enrollment The DFCI enrollment process is kicked off when a PC is enrolled into Intune and has a matching DFCI Profile. Enrollment includes Intune requesting enrollment packets from APS, sending the packets to the Windows UEFI configuration service provider (CSP) endpoints , the CSP writes the packets to UEFI variables, and triggers an OS reboot to allow UEFI firmware to process the DFCI packets. Management For day-to-day management, Intune creates device-specific packets, digitally signs them, and sends them through the same UEFI configuration service provider , UEFI variable, and reboot process. Retirement When a device is removed from Windows Autopilot, they are marked as unenrolled in APS. Intune will attempt to restore permissions (un-grey all settings) and remove its management authority from the device. Recovery Recovery shall be provided via a pre-boot UEFI menu, always available to a physically-present user, that can refresh DFCI configuration via web, USB, or other.","title":"Lifecycle"},{"location":"dyn/mu_plus/DfciPkg/Docs/Scenarios/DfciScenarios/#enrollment-flow","text":"Prior to the time of Enrollment, Microsoft Device Management Trust delegates management to the APS by signing a wildcard enrollment packet (targeting all manufacturer, model, & serial number) that authorizes enrollment of the APS certificate. At the request of Intune, the APS authorizes enrollment of a device, creates and signs per-device-targeted enrollment packets that enroll the Intune DFCI management certificate. The APS provides a level of indirection as well as an extra level of recovery via a web recovery service endpoint. The APS additionally configures recovery settings as well as permissions that deny Intune access to modify them. DEFINED: A device is considered \"enrolled\" when IS_OWNER_IDENTITY_ENROLLED(IdMask) returns TRUE, unenrolled when FALSE. Intune creates device-specific packets to provision the Intune authority and configure DFCI settings and permissions as specified in the matching Intune DFCI Profile. Intune delivers these packets via the Windows UEFI configuration service provider which writes them to UEFI mailboxes, to be processed by DFCI on the following boot. Device-specific packets include the SMBIOS manufacturer, model, & serial number, along with an anti-rollback count, to be leveraged by the UEFI DFCI code to determine applicability.","title":"Enrollment Flow"},{"location":"dyn/mu_plus/DfciPkg/Docs/Scenarios/DfciScenarios/#retirement-recovery-flows","text":"","title":"Retirement &amp; Recovery Flows"},{"location":"dyn/mu_plus/DfciPkg/Docs/Scenarios/DfciScenarios/#retirement","text":"An IT administrator leverages the Intune console to remove devices from Autopilot. Intune creates and sends device-specific packets that both restore DFCI permissions (effectively un-managing settings, making them available in the BIOS menu) and remove the Intune authority from DFCI. Note that this does not restore the settings to default values, they remain as is. Intune also notifies APS that the device is in the unenrolled state. If the device owner wants to further remove the APS authority and/or opt-out of DFCI management, they must leverage the Recovery flow.","title":"Retirement"},{"location":"dyn/mu_plus/DfciPkg/Docs/Scenarios/DfciScenarios/#recovery","text":"Recovery is essential because UEFI misconfiguration may prevent booting to an operating system, for example if USB and network boot are disabled and the hard disk becomes corrupted. When a device is enrolled, UEFI must provide alternative mechanisms for the physically-present user to place packets in the DFCI request mailboxes - this MUST NOT be blocked by a BIOS password or similar. Note that when a device is not enrolled, a BIOS password should prevent access to DFCI enrollment by a physically-present user until they have entered the correct credential. The APS keeps track of the enrollment state of devices. When an administrator removes a device from Autopilot, APS creates signed, device-specific un-enrollment packets and makes them available via a REST endpoint at DFCI_SETTING_ID__DFCI_RECOVERY_URL. These packets should delete the Intune and APS certificates and provide the local user with access to all settings (they should no longer be greyed out in BIOS menus). Note that retirement does not restore visible settings to their default values.","title":"Recovery"},{"location":"dyn/mu_plus/DfciPkg/Docs/Scenarios/DfciScenarios/#standard-settings","text":"The following standard settings are defined for DFCI v1.0, will be exposed by Intune, and Settings Providers for them must be implemented by the UEFI provider. Note that for device management, the settings apply only to built-in devices, not externally attached devices. All cameras All audio (microphones & speakers) All radios (Wi-Fi, broadband, NFC, BT, ...) CPU & IO virtualization (exposed, enabled for use by the OS so that Virtualization Based Security may be enabled) Boot to external media (e.g. USB, SD, ...) Boot via on-board network devices Refer to the \"Group Settings\" section of DfciSettings.h","title":"Standard Settings"},{"location":"dyn/mu_plus/DfciPkg/Docs/Scenarios/DfciScenarios/#device-ownership","text":"The Windows Autopilot database is used for authorizing DFCI management. It includes a map of device identifiers an owner's Azure Active Directory tenant. It is populated as part of the Windows Autopilot program by OEMs and Microsoft-authorized Cloud Solution Providers . The DFCI UEFI code leverages the SMBIOS manufacturer, model, and serial number for packet targeting. DFCI is a Windows Autopilot feature, available from Microsoft Intune.","title":"Device Ownership"},{"location":"dyn/mu_plus/DfciPkg/Docs/Scenarios/DfciScenarios/#management-authorities","text":"DFCI supports UEFI settings and permission management by multiple entities. In the recommended configuration, systems are shipped with 1 Microsoft public certificate included, \"CN=Microsoft Device Management Trust\", which provides the root of trust for management. This certificate authorizes Microsoft to automatically enroll management delegates with varying permissions. \"Microsoft Device Management Trust\" is only used to delegate management to APS and to provide second-chance recovery. APS in turn delegates management to Intune and provides a REST endpoint for online recovery. After enrollment, Intune performs the day-to-day UEFI management, whereas APS and Device Management Trust authorities provide various recovery paths. Authority DFCI ID Usage \"CN=Microsoft Device Management Trust\" DFCI_IDENTITY_SIGNER_ZTD Allowed to enroll a management delegates without a physical presence prompt. Enrolls the Autopilot Service authority. Can act as a backup recovery service. Autopilot Service (APS) DFCI_IDENTITY_SIGNER_OWNER Enrolls Microsoft Intune as a delegated management provider. Provides an online recovery service in case the OS is disconnected from Intune. Microsoft Intune DFCI_IDENTITY_SIGNER_USER Performs day-to-day UEFI settings and permissions management","title":"Management Authorities"},{"location":"dyn/mu_plus/DfciPkg/Docs/Scenarios/DfciScenarios/#security-privacy-considerations","text":"When a DFCI owner is enrolled, DFCI must take precedence over any other UEFI management solution. Physically-present user, including authenticated, may not bypass DFCI permissions on DFCI_IDENTITY_LOCAL . Protection: The hardware/firmware implementation of DFCI must protect DFCI configuration code and data such that they cannot by bypassed by an operating system or casual physical attacker. For example, non-volatile storage that is locked prior to Boot Device Selection. Enforcement: The hardware/firmware implementation of DFCI must enforce DFCI configurations such that they cannot by bypassed by an operating system or casual physical attacker. For example, power to devices is disabled or busses disabled and the configuration is locked prior to Boot Device Selection. It is recommended to include an \"Opt Out\" button that enables a physically-present user on an unenrolled device to eject the DFCI_IDENTITY_SIGNER_ZTD from DFCI. The effectively disables automated management - any enrollment attempt will display a red prompt at boot. A user could then prevent enrollment by configuring a BIOS password or enroll their own User certificate (proceeding through the red prompt).","title":"Security &amp; Privacy Considerations"},{"location":"dyn/mu_plus/DfciPkg/Docs/Scenarios/DfciScenarios/#online-recovery-via-the-autopilot-service","text":"The Recovery REST interface includes machine identities. Before transferring machine identities, the server's authenticity should be verified against DFCI_SETTING_ID__DFCI_HTTPS_CERT. After authenticating the server, the network traffic, including machine identities, are kept private by HTTPS encryption. But wait, there's more... the server certificate is updated regularly, so UEFI must first ensure it has the up-to-date DFCI_SETTING_ID__DFCI_HTTPS_CERT. DFCI_SETTING_ID__DFCI_BOOTSTRAP_URL provides a REST API to download a signed settings packet containing DFCI_SETTING_ID__DFCI_HTTPS_CERT. For this workflow, the server is not authenticated, but the payload will be authenticated prior to consumption.","title":"Online Recovery via the Autopilot Service"},{"location":"dyn/mu_plus/DfciPkg/Docs/Scenarios/DfciScenarios/#unknown-certificate-enrollment","text":"This is not a Microsoft-supported scenario but might be encountered during development and testing. On an unenrolled system, if enrollment packets are supplied to the DFCI mailboxes that are signed by an unknown certificate, a red authorization prompt is displayed during boot. The prompt requests the physically-present user to authorize the enrollment of the unknown certificate by typing the last 2 characters of the certificate's SHA-1 thumbprint. If a BIOS password is configured, the password must be entered prior to authorizing the enrollment. This is designed to avoid accidental or nefarious enrollment while allowing for valid custom identity management.","title":"Unknown Certificate Enrollment"},{"location":"dyn/mu_plus/DfciPkg/IdentityAndAuthManager/ReadMe/","text":"Identity and Authentication Manager \u00b6 Basic overview of the IdentityAndAuthManager module. File Overview \u00b6 IdentityAndAuthManager.H \u00b6 Private header file defining private functions for use across module Define the internal structure that holds the auth handle to identity mapping IdentityAndAuthManagerDxe \u00b6 Implement the Dxe specific parts of this. Including: Event handling Protocol access Protocol installation AuthManager.C \u00b6 Provide the implementation for the auth protocol functions AuthManagerProvision.C \u00b6 Support using Variable to set, change, or remove the AuthManager Key based identities AuthManagerProvisionedData.C \u00b6 Support NV storage of Provisioned Data. This manages loading internal store and saving changes to internal store. This differs from the Provision.c file in that this has nothing to do with User input or applying user changes. This is internal to the module only. IdentityManager.C \u00b6 Support the get identity functionality Dispose Auth Handle Private Identity / auth token map management (Add, Free, Find) Add security TODO IdentityAndAuthManagerDxe.INF \u00b6 Dxe Module inf file DfciAuthentication.h PUBLIC HEADER FILE \u00b6 Defines the DXE protocol to access Identity and Auth management","title":"Identity And Auth Manager"},{"location":"dyn/mu_plus/DfciPkg/IdentityAndAuthManager/ReadMe/#identity-and-authentication-manager","text":"Basic overview of the IdentityAndAuthManager module.","title":"Identity and Authentication Manager"},{"location":"dyn/mu_plus/DfciPkg/IdentityAndAuthManager/ReadMe/#file-overview","text":"","title":"File Overview"},{"location":"dyn/mu_plus/DfciPkg/IdentityAndAuthManager/ReadMe/#identityandauthmanagerh","text":"Private header file defining private functions for use across module Define the internal structure that holds the auth handle to identity mapping","title":"IdentityAndAuthManager.H"},{"location":"dyn/mu_plus/DfciPkg/IdentityAndAuthManager/ReadMe/#identityandauthmanagerdxe","text":"Implement the Dxe specific parts of this. Including: Event handling Protocol access Protocol installation","title":"IdentityAndAuthManagerDxe"},{"location":"dyn/mu_plus/DfciPkg/IdentityAndAuthManager/ReadMe/#authmanagerc","text":"Provide the implementation for the auth protocol functions","title":"AuthManager.C"},{"location":"dyn/mu_plus/DfciPkg/IdentityAndAuthManager/ReadMe/#authmanagerprovisionc","text":"Support using Variable to set, change, or remove the AuthManager Key based identities","title":"AuthManagerProvision.C"},{"location":"dyn/mu_plus/DfciPkg/IdentityAndAuthManager/ReadMe/#authmanagerprovisioneddatac","text":"Support NV storage of Provisioned Data. This manages loading internal store and saving changes to internal store. This differs from the Provision.c file in that this has nothing to do with User input or applying user changes. This is internal to the module only.","title":"AuthManagerProvisionedData.C"},{"location":"dyn/mu_plus/DfciPkg/IdentityAndAuthManager/ReadMe/#identitymanagerc","text":"Support the get identity functionality Dispose Auth Handle Private Identity / auth token map management (Add, Free, Find) Add security TODO","title":"IdentityManager.C"},{"location":"dyn/mu_plus/DfciPkg/IdentityAndAuthManager/ReadMe/#identityandauthmanagerdxeinf","text":"Dxe Module inf file","title":"IdentityAndAuthManagerDxe.INF"},{"location":"dyn/mu_plus/DfciPkg/IdentityAndAuthManager/ReadMe/#dfciauthenticationh-public-header-file","text":"Defines the DXE protocol to access Identity and Auth management","title":"DfciAuthentication.h  PUBLIC HEADER FILE"},{"location":"dyn/mu_plus/DfciPkg/Library/DfciSampleProvider/readme/","text":"Dfci Sample Provider \u00b6 This is a DXE driver that publishes the gDfciSettingsProviderSupportProtocolGuid protocol, which is a settings provider for DFCI. This is not to be used in production but is provided as a sample for reference when creating your own provider. For more information, please refer to the DFCI documentation here Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Dfci Sample Provider"},{"location":"dyn/mu_plus/DfciPkg/Library/DfciSampleProvider/readme/#dfci-sample-provider","text":"This is a DXE driver that publishes the gDfciSettingsProviderSupportProtocolGuid protocol, which is a settings provider for DFCI. This is not to be used in production but is provided as a sample for reference when creating your own provider. For more information, please refer to the DFCI documentation here","title":"Dfci Sample Provider"},{"location":"dyn/mu_plus/DfciPkg/Library/DfciSampleProvider/readme/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_plus/DfciPkg/UnitTests/DevicdIdTest/readme/","text":"Verify DfciDevicedIdLib library functionality \u00b6 The library DfciDeviceIdLib provided Dfci with platform information that Dfci needs. This include the manufacturer name, product name, and serial number. Dfci has limit on the characters supported, and the length of the strings returned. Device Id Library rules: The following five characters are not allowed: \" ' < > & The maximum string length is 64 characters plus a terminating '\\0' '\\0' is a required terminator. The interfaces return the string and the size of the string (including the '\\0'). The string is a valid UTF-8 string (ie, no 8-bit ASCII) About \u00b6 These tests verify that the DeviceIdLib Library functions properly. DeviceIdIdTestApp \u00b6 This application consumes the DfciDeviceIdLib executed test cases for the verification of the Device Id Strings. Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Devicd Id Test"},{"location":"dyn/mu_plus/DfciPkg/UnitTests/DevicdIdTest/readme/#verify-dfcidevicedidlib-library-functionality","text":"The library DfciDeviceIdLib provided Dfci with platform information that Dfci needs. This include the manufacturer name, product name, and serial number. Dfci has limit on the characters supported, and the length of the strings returned. Device Id Library rules: The following five characters are not allowed: \" ' < > & The maximum string length is 64 characters plus a terminating '\\0' '\\0' is a required terminator. The interfaces return the string and the size of the string (including the '\\0'). The string is a valid UTF-8 string (ie, no 8-bit ASCII)","title":"Verify DfciDevicedIdLib library functionality"},{"location":"dyn/mu_plus/DfciPkg/UnitTests/DevicdIdTest/readme/#about","text":"These tests verify that the DeviceIdLib Library functions properly.","title":"About"},{"location":"dyn/mu_plus/DfciPkg/UnitTests/DevicdIdTest/readme/#deviceididtestapp","text":"This application consumes the DfciDeviceIdLib executed test cases for the verification of the Device Id Strings.","title":"DeviceIdIdTestApp"},{"location":"dyn/mu_plus/DfciPkg/UnitTests/DevicdIdTest/readme/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_plus/DfciPkg/UnitTests/DfciTests/readme/","text":"Testing DFCI \u00b6 This describes the test structure for insuring DFCI operates properly. A Host System (HOST) to run the test cases. A Device Under Test (DUT) to be tested. with the new DFCI supported also running current Windows. Both systems on the same network. Overview \u00b6 The DFCI tests are a collection of Robot Framework test cases. Each Robot Framework test case collection is contained in a directory, and, as a minimum, contains a run.robot file. Each test case collection is run manually in a proscribed order, and its status is verified before running the next test case. The tests must be run in order, as they alter the system state, much like the real usage of DFCI. Equipment needed \u00b6 The following equipment is needed to run the DFCI tests: A system to run the test cases running a current version of Windows. A System Under Test also running current version of Windows. Both systems able to communicate with each other across the same network. Optional equipment is a mechanism to collect firmware logs from the system under test. Included is a serial port support function that looks for an FTDI serial connection. See the Platforms\\SimpleFTDI folder. Setting up the Device Under Test (DUT) \u00b6 Copy the files needed for the DUT. There is a script to help you do this. With a removable device at drive letter D for example, issue the command DeviceUnderTest\\CollectFilesForDut.cmd D:\\DfciSetup . This will create a directory on the USB key named DfciSetup with the required files for setting up the remote server. Mount the USB device on the DUT and run the SetupDUT.cmd . This will download and install Python 3.7.4, robotframework, robotremoteserver, and pypiwin32. In addition, the SetupDUT command will update the firewall for the robot framework testing, and a make a couple of configuration changes to Windows for a better test experience. Setting up the HOST system \u00b6 The HOST system requires the following software (NOTE - There are dependencies on x86-64 versions of Windows): A current version of Windows 10 x86-64. The current Windows SDK, available here Windows SDK . Python 3.7.4 x86-64 (the version tested), available here Python 3.7.4 . python -m pip install pip --upgrade python -m pip install robotframework python -m pop install edk2-pytool-library Copy the DfciTests directory, including all of the contents of the subdirectories, onto the HOST system. Test Cases Collections \u00b6 Table of DFCI Test case collections: | Test Case Collection | Description of Test Case | | ----- | ----- | ----- | | DFCI_CertChainingTest | Verifies that a ZeroTouch enroll actually prompts for authorization to Enroll when the enroll package is not signed by the proper key.| | DFCI_InitialState | Verifies that the firmware indicates support for DFCI and that the system is not already enrolled into DFCI. | | DFCI_InTuneBadUpdate | Tries to apply a settings package signed with the wrong key | | DFCI_InTunePermissions | Applies multiple sets of permissions to an InTune Enrolled system. | | DFCI_InTuneEnroll | Applies a InTune Owner, an InTune Manager, and the appropriate permissions and settings. | | DFCI_InTuneRollCerts | Updates the Owner and Manager certificates. This test can be run multiple times as it just swaps between two sets of certificates. | | DFCI_InTuneSettings | Applies multipl sets of settings to a InTune Enrolled system. | | DFCI_InTuneUnenroll | Applies an InTune Owner unenroll package, that removes both the InTune Owner and the InTune Manager, resets the Permission Database, and restores settings marked No UI to their default state. | Note on the firmware for testing DFCI \u00b6 Most of DFCI functionality can be tested without regard of the Zero Touch certificate. To test functionality of the Zero Touch feature, the firmware needs to be built with the ZTD_Leaf.cer file instead of the ZtdRecovery.cer file. To do this, change your platform .fdf file from: FILE FREEFORM = PCD ( gZeroTouchPkgTokenSpaceGuid . PcdZeroTouchCertificateFile ) { SECTION RAW = ZeroTouchPkg / Certs / ZeroTouch / ZtdRecovery . cer } to: FILE FREEFORM = PCD ( gZeroTouchPkgTokenSpaceGuid . PcdZeroTouchCertificateFile ) { SECTION RAW = DfciPkg / UnitTests / DfciTests / ZTD_Leaf . cer } WARNING: Do not ship with the ZTD_Leaf.cer certificate in your firmware Running The First Test Case \u00b6 Run the first test as shown replacing 11.11.11.211 with the actual IP address of the DUT. You should expect to see similar output with all four tests passing. DfciTests > RunDfciTest . bat TestCases \\ DFCI_InitialState 11 . 11 . 11 . 211 DfciTests > python . exe - m robot . run - L TRACE - x DFCI_InitialState . xml - A Platforms \\ SimpleFTDI \\ Args . txt - v IP_OF_DUT : 11 . 11 . 11 . 211 - v TEST_OUTPUT_BASE : C : \\ TestLogs \\ robot \\ DFCI_InitialState \\ logs_20191113_121224 - d C : \\ TestLogs \\ robot \\ DFCI_InitialState \\ logs_20191113_121224 TestCases \\ DFCI_InitialState \\ run . robot ============================================================================== Run :: DFCI Initial State test - Verifies that there are no enrolled identi ... ============================================================================== Ensure Mailboxes Are Clean .. . L : \\ Common \\ MU \\ DfciPkg \\ UnitTests \\ DfciTests \\ TestCases \\ DFCI_InitialState \\ run . robot Ensure Mailboxes Are Clean | PASS | ------------------------------------------------------------------------------ Get the starting DFCI Settings | PASS | ------------------------------------------------------------------------------ Obtain Target Parameters From Target | PASS | ------------------------------------------------------------------------------ Process Complete Testcase List .. Initializing testcases .. Running test Process Complete Testcase List | PASS | ------------------------------------------------------------------------------ Run :: DFCI Initial State test - Verifies that there are no enroll ... | PASS | 4 critical tests , 4 passed , 0 failed 4 tests total , 4 passed , 0 failed ============================================================================== Output : C : \\ TestLogs \\ robot \\ DFCI_InitialState \\ logs_20191113_121224 \\ output . xml XUnit : C : \\ TestLogs \\ robot \\ DFCI_InitialState \\ logs_20191113_121224 \\ DFCI_InitialState . xml Log : C : \\ TestLogs \\ robot \\ DFCI_InitialState \\ logs_20191113_121224 \\ log . html Report : C : \\ TestLogs \\ robot \\ DFCI_InitialState \\ logs_20191113_121224 \\ report . html Standard Testing \u00b6 Starting with a DUT that is not enrolled in DFCI, run the tests in the following order: DFCI_InitialState DFCI_InTuneEnroll DFCI_InTuneRollCerts DFCI_InTunePermissions DFCI_InTuneSettings DFCI_InTuneBadUpdate DFCI_InTuneUnenroll Steps 3 through 6 can and should be repeated in any order. Extended Testing \u00b6 This tests also start with a DUT that is not enrolled in DFCI, and will leave the system not enrolled if it completes successfully. DFCI_CertChainingTest Recovering from errors \u00b6 Code issues an present issues with DFCI that may require deleting the Idenity and Permission data bases. Using privileged access of a DUT that unlocks the varstore, you can delete the two master variables of DFCI. These variable are: _SPP _IPCVN USB Refresh Test \u00b6 The test cases DFCI_InTuneEnroll and DFCI_InTuneUnenroll have a GenUsb.bat file. The GenUsb.bat file will generate a .dfi file that UEFI management menu can read. GenUsb MFGNAME PRODUCTNAME SERIALNUMBER If there is a space or other special characters, add double quotes as in: GenUsb Fabrikam \"Fabrikam Spelunker Kit\" \"SN-47599011345\"","title":"Dfci Tests"},{"location":"dyn/mu_plus/DfciPkg/UnitTests/DfciTests/readme/#testing-dfci","text":"This describes the test structure for insuring DFCI operates properly. A Host System (HOST) to run the test cases. A Device Under Test (DUT) to be tested. with the new DFCI supported also running current Windows. Both systems on the same network.","title":"Testing DFCI"},{"location":"dyn/mu_plus/DfciPkg/UnitTests/DfciTests/readme/#overview","text":"The DFCI tests are a collection of Robot Framework test cases. Each Robot Framework test case collection is contained in a directory, and, as a minimum, contains a run.robot file. Each test case collection is run manually in a proscribed order, and its status is verified before running the next test case. The tests must be run in order, as they alter the system state, much like the real usage of DFCI.","title":"Overview"},{"location":"dyn/mu_plus/DfciPkg/UnitTests/DfciTests/readme/#equipment-needed","text":"The following equipment is needed to run the DFCI tests: A system to run the test cases running a current version of Windows. A System Under Test also running current version of Windows. Both systems able to communicate with each other across the same network. Optional equipment is a mechanism to collect firmware logs from the system under test. Included is a serial port support function that looks for an FTDI serial connection. See the Platforms\\SimpleFTDI folder.","title":"Equipment needed"},{"location":"dyn/mu_plus/DfciPkg/UnitTests/DfciTests/readme/#setting-up-the-device-under-test-dut","text":"Copy the files needed for the DUT. There is a script to help you do this. With a removable device at drive letter D for example, issue the command DeviceUnderTest\\CollectFilesForDut.cmd D:\\DfciSetup . This will create a directory on the USB key named DfciSetup with the required files for setting up the remote server. Mount the USB device on the DUT and run the SetupDUT.cmd . This will download and install Python 3.7.4, robotframework, robotremoteserver, and pypiwin32. In addition, the SetupDUT command will update the firewall for the robot framework testing, and a make a couple of configuration changes to Windows for a better test experience.","title":"Setting up the Device Under Test (DUT)"},{"location":"dyn/mu_plus/DfciPkg/UnitTests/DfciTests/readme/#setting-up-the-host-system","text":"The HOST system requires the following software (NOTE - There are dependencies on x86-64 versions of Windows): A current version of Windows 10 x86-64. The current Windows SDK, available here Windows SDK . Python 3.7.4 x86-64 (the version tested), available here Python 3.7.4 . python -m pip install pip --upgrade python -m pip install robotframework python -m pop install edk2-pytool-library Copy the DfciTests directory, including all of the contents of the subdirectories, onto the HOST system.","title":"Setting up the HOST system"},{"location":"dyn/mu_plus/DfciPkg/UnitTests/DfciTests/readme/#test-cases-collections","text":"Table of DFCI Test case collections: | Test Case Collection | Description of Test Case | | ----- | ----- | ----- | | DFCI_CertChainingTest | Verifies that a ZeroTouch enroll actually prompts for authorization to Enroll when the enroll package is not signed by the proper key.| | DFCI_InitialState | Verifies that the firmware indicates support for DFCI and that the system is not already enrolled into DFCI. | | DFCI_InTuneBadUpdate | Tries to apply a settings package signed with the wrong key | | DFCI_InTunePermissions | Applies multiple sets of permissions to an InTune Enrolled system. | | DFCI_InTuneEnroll | Applies a InTune Owner, an InTune Manager, and the appropriate permissions and settings. | | DFCI_InTuneRollCerts | Updates the Owner and Manager certificates. This test can be run multiple times as it just swaps between two sets of certificates. | | DFCI_InTuneSettings | Applies multipl sets of settings to a InTune Enrolled system. | | DFCI_InTuneUnenroll | Applies an InTune Owner unenroll package, that removes both the InTune Owner and the InTune Manager, resets the Permission Database, and restores settings marked No UI to their default state. |","title":"Test Cases Collections"},{"location":"dyn/mu_plus/DfciPkg/UnitTests/DfciTests/readme/#note-on-the-firmware-for-testing-dfci","text":"Most of DFCI functionality can be tested without regard of the Zero Touch certificate. To test functionality of the Zero Touch feature, the firmware needs to be built with the ZTD_Leaf.cer file instead of the ZtdRecovery.cer file. To do this, change your platform .fdf file from: FILE FREEFORM = PCD ( gZeroTouchPkgTokenSpaceGuid . PcdZeroTouchCertificateFile ) { SECTION RAW = ZeroTouchPkg / Certs / ZeroTouch / ZtdRecovery . cer } to: FILE FREEFORM = PCD ( gZeroTouchPkgTokenSpaceGuid . PcdZeroTouchCertificateFile ) { SECTION RAW = DfciPkg / UnitTests / DfciTests / ZTD_Leaf . cer } WARNING: Do not ship with the ZTD_Leaf.cer certificate in your firmware","title":"Note on the firmware for testing DFCI"},{"location":"dyn/mu_plus/DfciPkg/UnitTests/DfciTests/readme/#running-the-first-test-case","text":"Run the first test as shown replacing 11.11.11.211 with the actual IP address of the DUT. You should expect to see similar output with all four tests passing. DfciTests > RunDfciTest . bat TestCases \\ DFCI_InitialState 11 . 11 . 11 . 211 DfciTests > python . exe - m robot . run - L TRACE - x DFCI_InitialState . xml - A Platforms \\ SimpleFTDI \\ Args . txt - v IP_OF_DUT : 11 . 11 . 11 . 211 - v TEST_OUTPUT_BASE : C : \\ TestLogs \\ robot \\ DFCI_InitialState \\ logs_20191113_121224 - d C : \\ TestLogs \\ robot \\ DFCI_InitialState \\ logs_20191113_121224 TestCases \\ DFCI_InitialState \\ run . robot ============================================================================== Run :: DFCI Initial State test - Verifies that there are no enrolled identi ... ============================================================================== Ensure Mailboxes Are Clean .. . L : \\ Common \\ MU \\ DfciPkg \\ UnitTests \\ DfciTests \\ TestCases \\ DFCI_InitialState \\ run . robot Ensure Mailboxes Are Clean | PASS | ------------------------------------------------------------------------------ Get the starting DFCI Settings | PASS | ------------------------------------------------------------------------------ Obtain Target Parameters From Target | PASS | ------------------------------------------------------------------------------ Process Complete Testcase List .. Initializing testcases .. Running test Process Complete Testcase List | PASS | ------------------------------------------------------------------------------ Run :: DFCI Initial State test - Verifies that there are no enroll ... | PASS | 4 critical tests , 4 passed , 0 failed 4 tests total , 4 passed , 0 failed ============================================================================== Output : C : \\ TestLogs \\ robot \\ DFCI_InitialState \\ logs_20191113_121224 \\ output . xml XUnit : C : \\ TestLogs \\ robot \\ DFCI_InitialState \\ logs_20191113_121224 \\ DFCI_InitialState . xml Log : C : \\ TestLogs \\ robot \\ DFCI_InitialState \\ logs_20191113_121224 \\ log . html Report : C : \\ TestLogs \\ robot \\ DFCI_InitialState \\ logs_20191113_121224 \\ report . html","title":"Running The First Test Case"},{"location":"dyn/mu_plus/DfciPkg/UnitTests/DfciTests/readme/#standard-testing","text":"Starting with a DUT that is not enrolled in DFCI, run the tests in the following order: DFCI_InitialState DFCI_InTuneEnroll DFCI_InTuneRollCerts DFCI_InTunePermissions DFCI_InTuneSettings DFCI_InTuneBadUpdate DFCI_InTuneUnenroll Steps 3 through 6 can and should be repeated in any order.","title":"Standard Testing"},{"location":"dyn/mu_plus/DfciPkg/UnitTests/DfciTests/readme/#extended-testing","text":"This tests also start with a DUT that is not enrolled in DFCI, and will leave the system not enrolled if it completes successfully. DFCI_CertChainingTest","title":"Extended Testing"},{"location":"dyn/mu_plus/DfciPkg/UnitTests/DfciTests/readme/#recovering-from-errors","text":"Code issues an present issues with DFCI that may require deleting the Idenity and Permission data bases. Using privileged access of a DUT that unlocks the varstore, you can delete the two master variables of DFCI. These variable are: _SPP _IPCVN","title":"Recovering from errors"},{"location":"dyn/mu_plus/DfciPkg/UnitTests/DfciTests/readme/#usb-refresh-test","text":"The test cases DFCI_InTuneEnroll and DFCI_InTuneUnenroll have a GenUsb.bat file. The GenUsb.bat file will generate a .dfi file that UEFI management menu can read. GenUsb MFGNAME PRODUCTNAME SERIALNUMBER If there is a space or other special characters, add double quotes as in: GenUsb Fabrikam \"Fabrikam Spelunker Kit\" \"SN-47599011345\"","title":"USB Refresh Test"},{"location":"dyn/mu_plus/MsCorePkg/Feature_DebugRouting_Readme/","text":"Debugging with DxeDebugLibRouter \u00b6 Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent About \u00b6 DxeDebugLibRouter \u00b6 The DxeDebugLibRouter is an implementation of DebugLib that routes the DebugPrint and DebugAssert messages depending on what the platform is capable of and what has been set-up. In the example below we show how to use DxeDebugLibRouter to route debug messages through either the serial interface or the report status code interface, depending on what protocols and libraries are being used. StatusCodeHandler \u00b6 If you wish to make use of the Report Status Code debugging feature you will need to set up a status code handler and install the gMsSerialStatusCodeHandlerDxeProtocolGuid tag GUID. The MsCorePkg version of StatusCodeHandler is setup to do this. DebugPortProtocolInstallLib \u00b6 The DebugPortProtocolInstallLib is a shim library whos only purpose is to install a protocol that points to the currently linked DebugLib being used by the module. You can see how this is used in the DSC example shown below. ReportStatusCodeRouter \u00b6 This library handles the routing of ReportStatusCode if the DxeDebugLibRouter is set-up to use the ReportStatusCode debug path. We have only implemented the serial output for report status code, but there are many ways you can implement a RSC observer including Serial Port Listener Save Debug To File System Save Debug To Memory ... How To Use \u00b6 To make full use of the DxeDebugLibRouter each Dxe Driver will need to use the DebugPort implementation of DebugLib to route their messages through the DxeDebugLibRouter. The Flowchart below shows how this would work. To set up DxeCore to work as the router you will need to set up the DSC as below: [LibraryClasses.X64] DebugLib|MdePkg/Library/UefiDebugLibDebugPortProtocol/UefiDebugLibDebugPortProtocol.inf [Components.X64] MdeModulePkg/Core/Dxe/DxeMain.inf { <LibraryClasses> NULL|MsCorePkg/Library/DebugPortProtocolInstallLib/DebugPortProtocolInstallLib.inf DebugLib|MsCorePkg/Library/DxeDebugLibRouter/DxeDebugLibRouter.inf } Debug Flow \u00b6 1: The NULL library responsible for publishing the DebugPort protocol is linked against DxeMain. This allows the DebugLib used by Dxe drivers to locate the DebugLib used by Dxe Main 2: Dxe Driver makes a DebugPrint which is routed to the DebugLib linked to DxeMain 3: DebugLib routes the DebugPrint through either Serial or Report Status Code depending on what is installed at the time A: This step can happen at any time. When the StatusCodeHandler is dispatched it installs a tag GUID letting the DebugLib know that Report Status Code is now available","title":"Feature Debug Routing Readme"},{"location":"dyn/mu_plus/MsCorePkg/Feature_DebugRouting_Readme/#debugging-with-dxedebuglibrouter","text":"","title":"Debugging with DxeDebugLibRouter"},{"location":"dyn/mu_plus/MsCorePkg/Feature_DebugRouting_Readme/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_plus/MsCorePkg/Feature_DebugRouting_Readme/#about","text":"","title":"About"},{"location":"dyn/mu_plus/MsCorePkg/Feature_DebugRouting_Readme/#dxedebuglibrouter","text":"The DxeDebugLibRouter is an implementation of DebugLib that routes the DebugPrint and DebugAssert messages depending on what the platform is capable of and what has been set-up. In the example below we show how to use DxeDebugLibRouter to route debug messages through either the serial interface or the report status code interface, depending on what protocols and libraries are being used.","title":"DxeDebugLibRouter"},{"location":"dyn/mu_plus/MsCorePkg/Feature_DebugRouting_Readme/#statuscodehandler","text":"If you wish to make use of the Report Status Code debugging feature you will need to set up a status code handler and install the gMsSerialStatusCodeHandlerDxeProtocolGuid tag GUID. The MsCorePkg version of StatusCodeHandler is setup to do this.","title":"StatusCodeHandler"},{"location":"dyn/mu_plus/MsCorePkg/Feature_DebugRouting_Readme/#debugportprotocolinstalllib","text":"The DebugPortProtocolInstallLib is a shim library whos only purpose is to install a protocol that points to the currently linked DebugLib being used by the module. You can see how this is used in the DSC example shown below.","title":"DebugPortProtocolInstallLib"},{"location":"dyn/mu_plus/MsCorePkg/Feature_DebugRouting_Readme/#reportstatuscoderouter","text":"This library handles the routing of ReportStatusCode if the DxeDebugLibRouter is set-up to use the ReportStatusCode debug path. We have only implemented the serial output for report status code, but there are many ways you can implement a RSC observer including Serial Port Listener Save Debug To File System Save Debug To Memory ...","title":"ReportStatusCodeRouter"},{"location":"dyn/mu_plus/MsCorePkg/Feature_DebugRouting_Readme/#how-to-use","text":"To make full use of the DxeDebugLibRouter each Dxe Driver will need to use the DebugPort implementation of DebugLib to route their messages through the DxeDebugLibRouter. The Flowchart below shows how this would work. To set up DxeCore to work as the router you will need to set up the DSC as below: [LibraryClasses.X64] DebugLib|MdePkg/Library/UefiDebugLibDebugPortProtocol/UefiDebugLibDebugPortProtocol.inf [Components.X64] MdeModulePkg/Core/Dxe/DxeMain.inf { <LibraryClasses> NULL|MsCorePkg/Library/DebugPortProtocolInstallLib/DebugPortProtocolInstallLib.inf DebugLib|MsCorePkg/Library/DxeDebugLibRouter/DxeDebugLibRouter.inf }","title":"How To Use"},{"location":"dyn/mu_plus/MsCorePkg/Feature_DebugRouting_Readme/#debug-flow","text":"1: The NULL library responsible for publishing the DebugPort protocol is linked against DxeMain. This allows the DebugLib used by Dxe drivers to locate the DebugLib used by Dxe Main 2: Dxe Driver makes a DebugPrint which is routed to the DebugLib linked to DxeMain 3: DebugLib routes the DebugPrint through either Serial or Report Status Code depending on what is installed at the time A: This step can happen at any time. When the StatusCodeHandler is dispatched it installs a tag GUID letting the DebugLib know that Report Status Code is now available","title":"Debug Flow"},{"location":"dyn/mu_plus/MsCorePkg/ReadMe/","text":"MS Core Package \u00b6 About \u00b6 This package has shared drivers and libraries that are silicon and platform independent. Testing \u00b6 There are UEFI shell application based unit tests for each library. These tests attempt to verify basic functionality of public interfaces. Check the UntTests folder at the root of the package. Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Read Me"},{"location":"dyn/mu_plus/MsCorePkg/ReadMe/#ms-core-package","text":"","title":"MS Core Package"},{"location":"dyn/mu_plus/MsCorePkg/ReadMe/#about","text":"This package has shared drivers and libraries that are silicon and platform independent.","title":"About"},{"location":"dyn/mu_plus/MsCorePkg/ReadMe/#testing","text":"There are UEFI shell application based unit tests for each library. These tests attempt to verify basic functionality of public interfaces. Check the UntTests folder at the root of the package.","title":"Testing"},{"location":"dyn/mu_plus/MsCorePkg/ReadMe/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_plus/MsCorePkg/AcpiRGRT/feature_acpi_rgrt/","text":"ACPI RGRT \u00b6 What is it? \u00b6 The Regulartory Graphics Resource Table is an entry into the ACPI table. It provides a way to publish a PNG with the regulatory information of a given product. This could include FCC id, UL, Model number, or CMIIT ID just to name a few. This driver publishes that entry in the ACPI table to be picked up by the OS later to display for regulatory reasons. The table definition \u00b6 The format of the table is defined by spec.png and consists of a standard ACPI header along with an RGRT payload. At time of writing, the only image format supported by RGRT is PNG. Using in your project \u00b6 Using this is as simple as including the INF in both your DSC and INF. You will also need to define the file for the PCD in your FDF. DSC: [Components] MsCorePkg/AcpiRGRT/AcpiRgrt.inf FDF: [FV.FVDXE] # Regulatory Graphic Driver INF MsCorePkg/AcpiRGRT/AcpiRgrt.inf FILE FREEFORM = PCD(gMsCorePkgTokenSpaceGuid.PcdRegulatoryGraphicFileGuid) { SECTION RAW = $(GRAPHICS_RESOURCES)/RGRT.png }","title":"Acpi RGRT"},{"location":"dyn/mu_plus/MsCorePkg/AcpiRGRT/feature_acpi_rgrt/#acpi-rgrt","text":"","title":"ACPI RGRT"},{"location":"dyn/mu_plus/MsCorePkg/AcpiRGRT/feature_acpi_rgrt/#what-is-it","text":"The Regulartory Graphics Resource Table is an entry into the ACPI table. It provides a way to publish a PNG with the regulatory information of a given product. This could include FCC id, UL, Model number, or CMIIT ID just to name a few. This driver publishes that entry in the ACPI table to be picked up by the OS later to display for regulatory reasons.","title":"What is it?"},{"location":"dyn/mu_plus/MsCorePkg/AcpiRGRT/feature_acpi_rgrt/#the-table-definition","text":"The format of the table is defined by spec.png and consists of a standard ACPI header along with an RGRT payload. At time of writing, the only image format supported by RGRT is PNG.","title":"The table definition"},{"location":"dyn/mu_plus/MsCorePkg/AcpiRGRT/feature_acpi_rgrt/#using-in-your-project","text":"Using this is as simple as including the INF in both your DSC and INF. You will also need to define the file for the PCD in your FDF. DSC: [Components] MsCorePkg/AcpiRGRT/AcpiRgrt.inf FDF: [FV.FVDXE] # Regulatory Graphic Driver INF MsCorePkg/AcpiRGRT/AcpiRgrt.inf FILE FREEFORM = PCD(gMsCorePkgTokenSpaceGuid.PcdRegulatoryGraphicFileGuid) { SECTION RAW = $(GRAPHICS_RESOURCES)/RGRT.png }","title":"Using in your project"},{"location":"dyn/mu_plus/MsCorePkg/CheckHardwareConnected/readme/","text":"CheckHardwareConnected \u00b6 About \u00b6 This driver determines at boot if the specified pci devices found in the DeviceSpecificBusInfoLib library are properly connected Usage \u00b6 To employ this driver, simply build it and supply the DeviceSpecificBusInfoLib which implements the DeviceSpecificBusInfoLib.h interface. The info for each PCI device will be contained within the DEVICE_PCI_INFO struct which contains fields: DeviceName: A friendly name for the device. This Ascii name will be contained within the AdditionalInfo2 field of the MU_TELEMETRY_CPER_SECTION_DATA telemetry struct. IsFatal: A boolean which if true states that the pci device being absent crashes the device upon OS boot SegmentNumber , BusNumber , DeviceNumber , FunctionNumber : Info required to locate the PCI device The interface has methods: GetPciCheckDevices() : Populates an array of pointers to DEVICE_PCI_INFO structs. The pointer to the unallocated array is passed in as an argument and should be allocated within the function. The DEVICE_PCI_INFO structs should be global variables in the library and the array should contain their addresses. Function returns the number of DEVICE_PCI_INFO struct pointers within the allocated array. If there are specific cases when you do not want to check for certain PCI devices (such as when a device has been purposefully disabled), simply exclude the DEVICE_PCI_INFO associated with that device when allocating and returning the array. Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Check Hardware Connected"},{"location":"dyn/mu_plus/MsCorePkg/CheckHardwareConnected/readme/#checkhardwareconnected","text":"","title":"CheckHardwareConnected"},{"location":"dyn/mu_plus/MsCorePkg/CheckHardwareConnected/readme/#about","text":"This driver determines at boot if the specified pci devices found in the DeviceSpecificBusInfoLib library are properly connected","title":"About"},{"location":"dyn/mu_plus/MsCorePkg/CheckHardwareConnected/readme/#usage","text":"To employ this driver, simply build it and supply the DeviceSpecificBusInfoLib which implements the DeviceSpecificBusInfoLib.h interface. The info for each PCI device will be contained within the DEVICE_PCI_INFO struct which contains fields: DeviceName: A friendly name for the device. This Ascii name will be contained within the AdditionalInfo2 field of the MU_TELEMETRY_CPER_SECTION_DATA telemetry struct. IsFatal: A boolean which if true states that the pci device being absent crashes the device upon OS boot SegmentNumber , BusNumber , DeviceNumber , FunctionNumber : Info required to locate the PCI device The interface has methods: GetPciCheckDevices() : Populates an array of pointers to DEVICE_PCI_INFO structs. The pointer to the unallocated array is passed in as an argument and should be allocated within the function. The DEVICE_PCI_INFO structs should be global variables in the library and the array should contain their addresses. Function returns the number of DEVICE_PCI_INFO struct pointers within the allocated array. If there are specific cases when you do not want to check for certain PCI devices (such as when a device has been purposefully disabled), simply exclude the DEVICE_PCI_INFO associated with that device when allocating and returning the array.","title":"Usage"},{"location":"dyn/mu_plus/MsCorePkg/CheckHardwareConnected/readme/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_plus/MsCorePkg/DebugFileLoggerII/","text":"Debug File Logger II \u00b6 This is Debug File Logger II. Debug File Logger was tried a few times with varying degrees of success. It was thought that the not quite right USB stack was the issue with the Debug File Logger. When the Debug File Logger was enabled to write logs in the ESP (EFI System Partition) on the NVM/E drive, there would be systems that would no longer boot. Over time, we have found that there were issues in the Windows FAT file system driver across hibernate where the OS assumed no change to the file system metadata (The FAT and Directory Entries) causing irreparable damage to the ESP. Top design points. \u00b6 The debug log files are pre-allocated the first time they are needed on a particular device. Every SimpleFileSystem device that is connected during POST is eligible to be a log device. A USB device must have a directory in the root named 'Logs' to be considered a log device. The NVM/E device(s) will get a hidden directory named 'Logs' the first time is is mounted. The time to create the initial logs is about 2 seconds. The time to register the logs during subsequent time through POST is about 2ms. The logs can be obtained in the OS or by booting to the Shell. In the OS, mount the ESP using: mountvol P: /S The drive letter can be any available drive letter. Logs will be recorded at: When a registered device is connected When just prior to ExitBootServices to any previously registered devices When the system is reset (from TPL <= TPL_CALLBACK) to any previously registered devices If you want to collect logs on a USB device, you can insert the non-bootable USB drive with the Logs directory installed, then power on the system and hold VOL/- to attempt booting from USB. This will mount the USB filesystem, the Logs directory will be acknowledged, and the log written. The system will continue to boot and should append the rest of the log at ExitBootServices. Future items \u00b6 Let me know what future items are necessary. Some under considerations: Refactor DebugLib again to insure logs are collected during the whole boot. Add a new feature to DebugLib to capture the full UEFI log, but only send selected entries to the serial port (to handle devices with a slow serial port). Always connect the NVM/E drive at console connect. For 99.9% of the cases, the system is going to boot the NVM/E drive, so connecting it early would not significantly change the boot time, but would allow logs to be collected in more cases. When the log cannot be written (too high of TPL for the FileSystem to work), have a method to store the last 100 lines or so of the log somewhere (system dependent) that can be collected on the next boot. Timestamp the head of the log to assist in sorting logs Installation Instructions \u00b6 Replace the current Pei/DebugFileLoggerPei.inf and /Dxe/DebugFileLogger.inf with the new versions. Delete the DebugFileLoggerLib as it is no longer necessary. Remove these lines of the old Debug File Logger: DebugFileLoggerLib|XxxxxPkg/Library/DebugFileLoggerLib/DebugFileLoggerLib.inf XxxxxPkg/DebugFileLogger/Pei/DebugFileLoggerPei.inf { DebugLib|MdePkg/Library/BaseDebugLibSerialPort/BaseDebugLibSerialPort.inf } XxxxxPkg/DebugFileLogger/Dxe/DebugFileLogger.inf Replace the old Debug file logger files with these new Debug File Logger II entries: MsCorePkg/DebugFileLoggerII/Pei/DebugFileLoggerPei.inf { DebugLib|MdePkg/Library/BaseDebugLibSerialPort/BaseDebugLibSerialPort.inf } MsCorePkg/DebugFileLoggerII/Dxe/DebugFileLogger.inf { DebugLib|MdePkg/Library/BaseDebugLibSerialPort/BaseDebugLibSerialPort.inf } There is no check for Manufacturing mode. The idea is for the File Logger to be robust enough to be on in all builds.","title":"Debug File Logger II"},{"location":"dyn/mu_plus/MsCorePkg/DebugFileLoggerII/#debug-file-logger-ii","text":"This is Debug File Logger II. Debug File Logger was tried a few times with varying degrees of success. It was thought that the not quite right USB stack was the issue with the Debug File Logger. When the Debug File Logger was enabled to write logs in the ESP (EFI System Partition) on the NVM/E drive, there would be systems that would no longer boot. Over time, we have found that there were issues in the Windows FAT file system driver across hibernate where the OS assumed no change to the file system metadata (The FAT and Directory Entries) causing irreparable damage to the ESP.","title":"Debug File Logger II"},{"location":"dyn/mu_plus/MsCorePkg/DebugFileLoggerII/#top-design-points","text":"The debug log files are pre-allocated the first time they are needed on a particular device. Every SimpleFileSystem device that is connected during POST is eligible to be a log device. A USB device must have a directory in the root named 'Logs' to be considered a log device. The NVM/E device(s) will get a hidden directory named 'Logs' the first time is is mounted. The time to create the initial logs is about 2 seconds. The time to register the logs during subsequent time through POST is about 2ms. The logs can be obtained in the OS or by booting to the Shell. In the OS, mount the ESP using: mountvol P: /S The drive letter can be any available drive letter. Logs will be recorded at: When a registered device is connected When just prior to ExitBootServices to any previously registered devices When the system is reset (from TPL <= TPL_CALLBACK) to any previously registered devices If you want to collect logs on a USB device, you can insert the non-bootable USB drive with the Logs directory installed, then power on the system and hold VOL/- to attempt booting from USB. This will mount the USB filesystem, the Logs directory will be acknowledged, and the log written. The system will continue to boot and should append the rest of the log at ExitBootServices.","title":"Top design points."},{"location":"dyn/mu_plus/MsCorePkg/DebugFileLoggerII/#future-items","text":"Let me know what future items are necessary. Some under considerations: Refactor DebugLib again to insure logs are collected during the whole boot. Add a new feature to DebugLib to capture the full UEFI log, but only send selected entries to the serial port (to handle devices with a slow serial port). Always connect the NVM/E drive at console connect. For 99.9% of the cases, the system is going to boot the NVM/E drive, so connecting it early would not significantly change the boot time, but would allow logs to be collected in more cases. When the log cannot be written (too high of TPL for the FileSystem to work), have a method to store the last 100 lines or so of the log somewhere (system dependent) that can be collected on the next boot. Timestamp the head of the log to assist in sorting logs","title":"Future items"},{"location":"dyn/mu_plus/MsCorePkg/DebugFileLoggerII/#installation-instructions","text":"Replace the current Pei/DebugFileLoggerPei.inf and /Dxe/DebugFileLogger.inf with the new versions. Delete the DebugFileLoggerLib as it is no longer necessary. Remove these lines of the old Debug File Logger: DebugFileLoggerLib|XxxxxPkg/Library/DebugFileLoggerLib/DebugFileLoggerLib.inf XxxxxPkg/DebugFileLogger/Pei/DebugFileLoggerPei.inf { DebugLib|MdePkg/Library/BaseDebugLibSerialPort/BaseDebugLibSerialPort.inf } XxxxxPkg/DebugFileLogger/Dxe/DebugFileLogger.inf Replace the old Debug file logger files with these new Debug File Logger II entries: MsCorePkg/DebugFileLoggerII/Pei/DebugFileLoggerPei.inf { DebugLib|MdePkg/Library/BaseDebugLibSerialPort/BaseDebugLibSerialPort.inf } MsCorePkg/DebugFileLoggerII/Dxe/DebugFileLogger.inf { DebugLib|MdePkg/Library/BaseDebugLibSerialPort/BaseDebugLibSerialPort.inf } There is no check for Manufacturing mode. The idea is for the File Logger to be robust enough to be on in all builds.","title":"Installation Instructions"},{"location":"dyn/mu_plus/MsCorePkg/MuCryptoDxe/Readme/","text":"MuCryptoDxe \u00b6 Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent About \u00b6 MuCryptoDxe is a DXE_DRIVER you can include in your platform to have a protocol that can call Crypto functions without having to staticly link against the crypto library in many places Supported Architectures \u00b6 This package is not architecturally dependent. Methods supported \u00b6 There are two protocols exposed in this GUID _MU_PKCS5_PASSWORD_HASH_PROTOCOL \u00b6 HashPassword \u00b6 Hashes a password by passing through to the BaseCryptLib. Returns EFI_STATUS NOTE: DigestSize will be used to determine the hash algorithm and must correspond to a known hash digest size. Use standards. @retval EFI_SUCCESS Congratulations! Your hash is in the output buffer. @retval EFI_INVALID_PARAMETER One of the pointers was NULL or one of the sizes was too large. @retval EFI_INVALID_PARAMETER The hash algorithm could not be determined from the digest size. @retval EFI_ABORTED An error occurred in the OpenSSL subroutines. Inputs : IN CONST MU_PKCS5_PASSWORD_HASH_PROTOCOL IN UINTN PasswordSize IN CONST CHAR8 *Password IN UINTN SaltSize IN CONST UINT8 *Salt IN UINTN IterationCount IN UINTN DigestSize IN UINTN OutputSize OUT UINT8 *Output _MU_PKCS7_PROTOCOL \u00b6 Verify \u00b6 Verifies the validility of a PKCS#7 signed data as described in \"PKCS #7: Cryptographic Message Syntax Standard\". The input signed data could be wrapped in a ContentInfo structure. If P7Data, TrustedCert or InData is NULL, then return EFI_INVALID_PARAMETER. If P7Length, CertLength or DataLength overflow, then return EFI_INVALID_PARAMETER. If this interface is not supported, then return EFI_UNSUPPORTED. @retval EFI_SUCCESS The specified PKCS#7 signed data is valid. @retval EFI_SECURITY_VIOLATION Invalid PKCS#7 signed data. @retval EFI_UNSUPPORTED This interface is not supported. Inputs: IN CONST MU_PKCS7_PROTOCOL IN CONST UINT8 *P7Data, IN UINTN P7DataLength, IN CONST UINT8 *TrustedCert, IN UINTN TrustedCertLength, IN CONST UINT8 *Data, IN UINTN DataLength (in bytes) VerifyEKU \u00b6 This function receives a PKCS7 formatted signature, and then verifies that the specified Enhanced or Extended Key Usages (EKU's) are present in the end-entity leaf signing certificate. Note that this function does not validate the certificate chain. Applications for custom EKU's are quite flexible. For example, a policy EKU may be present in an Issuing Certificate Authority (CA), and any sub-ordinate certificate issued might also contain this EKU, thus constraining the sub-ordinate certificate. Other applications might allow a certificate embedded in a device to specify that other Object Identifiers (OIDs) are present which contains binary data specifying custom capabilities that the device is able to do. @retval EFI_SUCCESS - The required EKUs were found in the signature. @retval EFI_INVALID_PARAMETER - A parameter was invalid. @retval EFI_NOT_FOUND - One or more EKU's were not found in the signature. Inputs: IN CONST MU_PKCS7_PROTOCOL IN CONST UINT8 *Pkcs7Signature, IN CONST UINT32 SignatureSize, (in bytes) IN CONST CHAR8 *RequiredEKUs[], null-terminated strings listing OIDs of required EKUs IN CONST UINT32 RequiredEKUsSize, IN BOOLEAN RequireAllPresent Including in your platform \u00b6 Sample DSC change \u00b6 [Components.<arch>] ... ... MsCorePkg/MuCryptoDxe/MuCryptoDxe.inf Sample FDF change \u00b6 [FV.<a DXE firmware volume>] ... ... INF MsCorePkg/MuCryptoDxe/MuCryptoDxe.inf","title":"Mu Crypto Dxe"},{"location":"dyn/mu_plus/MsCorePkg/MuCryptoDxe/Readme/#mucryptodxe","text":"","title":"MuCryptoDxe"},{"location":"dyn/mu_plus/MsCorePkg/MuCryptoDxe/Readme/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_plus/MsCorePkg/MuCryptoDxe/Readme/#about","text":"MuCryptoDxe is a DXE_DRIVER you can include in your platform to have a protocol that can call Crypto functions without having to staticly link against the crypto library in many places","title":"About"},{"location":"dyn/mu_plus/MsCorePkg/MuCryptoDxe/Readme/#supported-architectures","text":"This package is not architecturally dependent.","title":"Supported Architectures"},{"location":"dyn/mu_plus/MsCorePkg/MuCryptoDxe/Readme/#methods-supported","text":"There are two protocols exposed in this GUID","title":"Methods supported"},{"location":"dyn/mu_plus/MsCorePkg/MuCryptoDxe/Readme/#_mu_pkcs5_password_hash_protocol","text":"","title":"_MU_PKCS5_PASSWORD_HASH_PROTOCOL"},{"location":"dyn/mu_plus/MsCorePkg/MuCryptoDxe/Readme/#hashpassword","text":"Hashes a password by passing through to the BaseCryptLib. Returns EFI_STATUS NOTE: DigestSize will be used to determine the hash algorithm and must correspond to a known hash digest size. Use standards. @retval EFI_SUCCESS Congratulations! Your hash is in the output buffer. @retval EFI_INVALID_PARAMETER One of the pointers was NULL or one of the sizes was too large. @retval EFI_INVALID_PARAMETER The hash algorithm could not be determined from the digest size. @retval EFI_ABORTED An error occurred in the OpenSSL subroutines. Inputs : IN CONST MU_PKCS5_PASSWORD_HASH_PROTOCOL IN UINTN PasswordSize IN CONST CHAR8 *Password IN UINTN SaltSize IN CONST UINT8 *Salt IN UINTN IterationCount IN UINTN DigestSize IN UINTN OutputSize OUT UINT8 *Output","title":"HashPassword"},{"location":"dyn/mu_plus/MsCorePkg/MuCryptoDxe/Readme/#_mu_pkcs7_protocol","text":"","title":"_MU_PKCS7_PROTOCOL"},{"location":"dyn/mu_plus/MsCorePkg/MuCryptoDxe/Readme/#verify","text":"Verifies the validility of a PKCS#7 signed data as described in \"PKCS #7: Cryptographic Message Syntax Standard\". The input signed data could be wrapped in a ContentInfo structure. If P7Data, TrustedCert or InData is NULL, then return EFI_INVALID_PARAMETER. If P7Length, CertLength or DataLength overflow, then return EFI_INVALID_PARAMETER. If this interface is not supported, then return EFI_UNSUPPORTED. @retval EFI_SUCCESS The specified PKCS#7 signed data is valid. @retval EFI_SECURITY_VIOLATION Invalid PKCS#7 signed data. @retval EFI_UNSUPPORTED This interface is not supported. Inputs: IN CONST MU_PKCS7_PROTOCOL IN CONST UINT8 *P7Data, IN UINTN P7DataLength, IN CONST UINT8 *TrustedCert, IN UINTN TrustedCertLength, IN CONST UINT8 *Data, IN UINTN DataLength (in bytes)","title":"Verify"},{"location":"dyn/mu_plus/MsCorePkg/MuCryptoDxe/Readme/#verifyeku","text":"This function receives a PKCS7 formatted signature, and then verifies that the specified Enhanced or Extended Key Usages (EKU's) are present in the end-entity leaf signing certificate. Note that this function does not validate the certificate chain. Applications for custom EKU's are quite flexible. For example, a policy EKU may be present in an Issuing Certificate Authority (CA), and any sub-ordinate certificate issued might also contain this EKU, thus constraining the sub-ordinate certificate. Other applications might allow a certificate embedded in a device to specify that other Object Identifiers (OIDs) are present which contains binary data specifying custom capabilities that the device is able to do. @retval EFI_SUCCESS - The required EKUs were found in the signature. @retval EFI_INVALID_PARAMETER - A parameter was invalid. @retval EFI_NOT_FOUND - One or more EKU's were not found in the signature. Inputs: IN CONST MU_PKCS7_PROTOCOL IN CONST UINT8 *Pkcs7Signature, IN CONST UINT32 SignatureSize, (in bytes) IN CONST CHAR8 *RequiredEKUs[], null-terminated strings listing OIDs of required EKUs IN CONST UINT32 RequiredEKUsSize, IN BOOLEAN RequireAllPresent","title":"VerifyEKU"},{"location":"dyn/mu_plus/MsCorePkg/MuCryptoDxe/Readme/#including-in-your-platform","text":"","title":"Including in your platform"},{"location":"dyn/mu_plus/MsCorePkg/MuCryptoDxe/Readme/#sample-dsc-change","text":"[Components.<arch>] ... ... MsCorePkg/MuCryptoDxe/MuCryptoDxe.inf","title":"Sample DSC change"},{"location":"dyn/mu_plus/MsCorePkg/MuCryptoDxe/Readme/#sample-fdf-change","text":"[FV.<a DXE firmware volume>] ... ... INF MsCorePkg/MuCryptoDxe/MuCryptoDxe.inf","title":"Sample FDF change"},{"location":"dyn/mu_plus/MsCorePkg/MuVarPolicyFoundationDxe/Feature_MuVarPolicyFoundationDxe_Readme/","text":"-# MuVarPolicyFoundationDxe Driver and Policies Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent Overview \u00b6 This driver works in conjunction with the Variable Policy engine to create two policy-based concepts that can be leveraged by other drivers in the system and to produce an EDK2 Variable Locking Protocol. The two concepts are: Dxe Phase Indicators and Write Once State Variables. DXE Phase Indicators \u00b6 To support the DXE Phase Indicators, a new policy is installed that creates the gMuVarPolicyDxePhaseGuid variable namespace. By policy, all variables in this namespace are required to be the size of a PHASE_INDICATOR (which is a UINT8 ) and must be volatile and readable in both BootServices and Runtime. All variables in this namespace will also be made read-only immediately upon creation (as such, they are also Write-Once, but have a special purpose). Because these variables are required to be volatile, it will not be possible to create more after ExitBootServices. This driver will also register callbacks for EndOfDxe, ReadyToBoot, and ExitBootServices. At the time of the corresponding event, a new PHASE_INDICATOR variable will be created for the callback that has just been triggered. The purpose of these variables is two-fold: They can be queried by any driver or library that needs to know what phases of boot have already occurred. This is especially convenient for libraries that might be linked against any type of driver or applicaiton, but may not have been able to register callbacks for all the events because they don't know their execution order or time. They can be used as the delegated \"Variable State\" variables in other Variable Policy lock policies. As such, you could describe a variable that locks \"at ReadyToBoot\" or \"EndOfDxe\". Note that the PHASE_INDICATOR variables are intentionally named as short abbreviations, such as \"EOD\" and \"RTB\". This is to minimize the size of the policy entries for lock-on-state policies. Important Note on Timing \u00b6 The EndOfDxe and ReadyToBoot state varibles will be created at the end of the Notify list for the respective event. As such, any variable that locks on those events will still be writeable in Notify callbacks for the event. However, the ExitBootServices state variable (due to architectural requirements) will be created at a non-deterministic time in the Notify list. Therefore, it is unpredictable whether any variable which locks \"on ExitBootServices\" would still be writeable in any given callback. Write-Once State Variables \u00b6 The Write-Once State Variables are actually quite similar to the DXE Phase Variables in terms of policy, but are kept distinct because of the different intentions for their use. This driver will also register a policy that creates the gMuVarPolicyWriteOnceStateVarGuid namespace. This policy will also limit these variables to being: volatile, BootServices and Runtime, read-only on create, and a fixed size -- sizeof(POLICY_LOCK_VAR) , which is also a UINT8 , given that that is the size of the Value field of the current VARIABLE_LOCK_ON_VAR_STATE_POLICY structure. The primary difference is that the DXE Phase Variables have a very clear purpose and meaning. While the gMuVarPolicyWriteOnceStateVarGuid namespace is designed to be general-purpose and used to easily describe delegated variables for VARIABLE_POLICY_TYPE_LOCK_ON_VAR_STATE -type policies without requiring every driver to define two policies in order to employ this common pattern. Example \u00b6 Driver XYZ wants to define a policy to allow a boot application (that executes after the policy creation interface may be closed) to modify a collection of variables until the boot application chooses to lock them. One way to accomplish this is the create a policy that delegates the lock control of the target variables to the state of variable A. However, now the variable A is the linchpin for the protections of the target variables and should have some protections of its own. This might require another policy. Instead, the driver could create a policy that delegates lock control of the target variables to a variable in the gMuVarPolicyWriteOnceStateVarGuid . This would not require a second policy, as a general-purpose policy is already in place. Now the boot application only needs to create variable A in the gMuVarPolicyWriteOnceStateVarGuid namespace (with the value designated in the policy created by driver XYZ) in order to lock the target variables. Important Note on Naming \u00b6 Since the gMuVarPolicyWriteOnceStateVarGuid namespace is a single variable namespace, naming collisions are possible. Because of the write-once nature of the policy, it is not a concern that an existing lock may be overwritten, but there still may be policy confusion if there are two policies that delegate to the exact same name. It is up to the platform architect to ensure that naming collisions like this do not occur. EDK2 Variable Locking Protocol \u00b6 Finally, this driver installs an EDK2 Variable Locking Protocol instance. This implementation locks a variable by creating a policy entry and registering it via the Variable Policy Protocol. The policy entry is of type VARIABLE_POLICY_TYPE_LOCK_ON_VAR_STATE and locks the variable based on state of the Phase Indicator variable \"EOD\" (meaning End Of Dxe) in gMuVarPolicyDxePhaseGuid namespace.","title":"Mu Var Policy Foundation Dxe"},{"location":"dyn/mu_plus/MsCorePkg/MuVarPolicyFoundationDxe/Feature_MuVarPolicyFoundationDxe_Readme/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_plus/MsCorePkg/MuVarPolicyFoundationDxe/Feature_MuVarPolicyFoundationDxe_Readme/#overview","text":"This driver works in conjunction with the Variable Policy engine to create two policy-based concepts that can be leveraged by other drivers in the system and to produce an EDK2 Variable Locking Protocol. The two concepts are: Dxe Phase Indicators and Write Once State Variables.","title":"Overview"},{"location":"dyn/mu_plus/MsCorePkg/MuVarPolicyFoundationDxe/Feature_MuVarPolicyFoundationDxe_Readme/#dxe-phase-indicators","text":"To support the DXE Phase Indicators, a new policy is installed that creates the gMuVarPolicyDxePhaseGuid variable namespace. By policy, all variables in this namespace are required to be the size of a PHASE_INDICATOR (which is a UINT8 ) and must be volatile and readable in both BootServices and Runtime. All variables in this namespace will also be made read-only immediately upon creation (as such, they are also Write-Once, but have a special purpose). Because these variables are required to be volatile, it will not be possible to create more after ExitBootServices. This driver will also register callbacks for EndOfDxe, ReadyToBoot, and ExitBootServices. At the time of the corresponding event, a new PHASE_INDICATOR variable will be created for the callback that has just been triggered. The purpose of these variables is two-fold: They can be queried by any driver or library that needs to know what phases of boot have already occurred. This is especially convenient for libraries that might be linked against any type of driver or applicaiton, but may not have been able to register callbacks for all the events because they don't know their execution order or time. They can be used as the delegated \"Variable State\" variables in other Variable Policy lock policies. As such, you could describe a variable that locks \"at ReadyToBoot\" or \"EndOfDxe\". Note that the PHASE_INDICATOR variables are intentionally named as short abbreviations, such as \"EOD\" and \"RTB\". This is to minimize the size of the policy entries for lock-on-state policies.","title":"DXE Phase Indicators"},{"location":"dyn/mu_plus/MsCorePkg/MuVarPolicyFoundationDxe/Feature_MuVarPolicyFoundationDxe_Readme/#important-note-on-timing","text":"The EndOfDxe and ReadyToBoot state varibles will be created at the end of the Notify list for the respective event. As such, any variable that locks on those events will still be writeable in Notify callbacks for the event. However, the ExitBootServices state variable (due to architectural requirements) will be created at a non-deterministic time in the Notify list. Therefore, it is unpredictable whether any variable which locks \"on ExitBootServices\" would still be writeable in any given callback.","title":"Important Note on Timing"},{"location":"dyn/mu_plus/MsCorePkg/MuVarPolicyFoundationDxe/Feature_MuVarPolicyFoundationDxe_Readme/#write-once-state-variables","text":"The Write-Once State Variables are actually quite similar to the DXE Phase Variables in terms of policy, but are kept distinct because of the different intentions for their use. This driver will also register a policy that creates the gMuVarPolicyWriteOnceStateVarGuid namespace. This policy will also limit these variables to being: volatile, BootServices and Runtime, read-only on create, and a fixed size -- sizeof(POLICY_LOCK_VAR) , which is also a UINT8 , given that that is the size of the Value field of the current VARIABLE_LOCK_ON_VAR_STATE_POLICY structure. The primary difference is that the DXE Phase Variables have a very clear purpose and meaning. While the gMuVarPolicyWriteOnceStateVarGuid namespace is designed to be general-purpose and used to easily describe delegated variables for VARIABLE_POLICY_TYPE_LOCK_ON_VAR_STATE -type policies without requiring every driver to define two policies in order to employ this common pattern.","title":"Write-Once State Variables"},{"location":"dyn/mu_plus/MsCorePkg/MuVarPolicyFoundationDxe/Feature_MuVarPolicyFoundationDxe_Readme/#example","text":"Driver XYZ wants to define a policy to allow a boot application (that executes after the policy creation interface may be closed) to modify a collection of variables until the boot application chooses to lock them. One way to accomplish this is the create a policy that delegates the lock control of the target variables to the state of variable A. However, now the variable A is the linchpin for the protections of the target variables and should have some protections of its own. This might require another policy. Instead, the driver could create a policy that delegates lock control of the target variables to a variable in the gMuVarPolicyWriteOnceStateVarGuid . This would not require a second policy, as a general-purpose policy is already in place. Now the boot application only needs to create variable A in the gMuVarPolicyWriteOnceStateVarGuid namespace (with the value designated in the policy created by driver XYZ) in order to lock the target variables.","title":"Example"},{"location":"dyn/mu_plus/MsCorePkg/MuVarPolicyFoundationDxe/Feature_MuVarPolicyFoundationDxe_Readme/#important-note-on-naming","text":"Since the gMuVarPolicyWriteOnceStateVarGuid namespace is a single variable namespace, naming collisions are possible. Because of the write-once nature of the policy, it is not a concern that an existing lock may be overwritten, but there still may be policy confusion if there are two policies that delegate to the exact same name. It is up to the platform architect to ensure that naming collisions like this do not occur.","title":"Important Note on Naming"},{"location":"dyn/mu_plus/MsCorePkg/MuVarPolicyFoundationDxe/Feature_MuVarPolicyFoundationDxe_Readme/#edk2-variable-locking-protocol","text":"Finally, this driver installs an EDK2 Variable Locking Protocol instance. This implementation locks a variable by creating a policy entry and registering it via the Variable Policy Protocol. The policy entry is of type VARIABLE_POLICY_TYPE_LOCK_ON_VAR_STATE and locks the variable based on state of the Phase Indicator variable \"EOD\" (meaning End Of Dxe) in gMuVarPolicyDxePhaseGuid namespace.","title":"EDK2 Variable Locking Protocol"},{"location":"dyn/mu_plus/MsCorePkg/UnitTests/JsonTest/readme/","text":"Verify Json Lite library functionality \u00b6 The Json Lite Library parses json strings in to tuples and encodes tuples into a json string. About \u00b6 These tests verify that the Json Lite Library functions properly. JsonTestApp \u00b6 This application consumes the UnitTestLib and implements various test cases for the verification of the Json Lite Library. Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Json Test"},{"location":"dyn/mu_plus/MsCorePkg/UnitTests/JsonTest/readme/#verify-json-lite-library-functionality","text":"The Json Lite Library parses json strings in to tuples and encodes tuples into a json string.","title":"Verify Json Lite library functionality"},{"location":"dyn/mu_plus/MsCorePkg/UnitTests/JsonTest/readme/#about","text":"These tests verify that the Json Lite Library functions properly.","title":"About"},{"location":"dyn/mu_plus/MsCorePkg/UnitTests/JsonTest/readme/#jsontestapp","text":"This application consumes the UnitTestLib and implements various test cases for the verification of the Json Lite Library.","title":"JsonTestApp"},{"location":"dyn/mu_plus/MsCorePkg/UnitTests/JsonTest/readme/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_plus/MsGraphicsPkg/PrintScreenLogger/Readme/","text":"PrintScreenLogger \u00b6 About \u00b6 PrintScreenLogger is a DXE_DRIVER you can include in your platform to obtain Screen Captures during the preboot environment by pressing the Ctrl-PrtScn key combination. This action will creates a 24bbp (Bits Per Pixel) .BMP file of the screen's contents and write it to a enabled USB drive. Supported Architectures \u00b6 This package is not architecturally dependent. This package is dependent upon the Gop pixel format, and only supports these two pixel formats: 1. PixelRedGreenBlueReserved8BitPerColor 2. PixelBlueGreenRedReserved8BitPerColor PrintScreenLogger operation \u00b6 During initialization, the Print Screen Loggger registers for notification of the Ctrl-PrtScn key combination is pressed. When a Print Screen callback occurs: Looks for a mounted USB drive that contains a file in the root directory called PrintScreenEnable.txt . This limits PrintScreenLogger to only write to enabled USB devices. Looks for the next available filename in the form PrtScreen####.bmp , starting with 0000. Creates the new PrtScreen####.bmp file. Call GraphicsOutput->Blt to obtain the complete screen. Converts the BLT buffer to a 24bbp BMP structure. Writes the BMP structure to the new PrtScreen####.bmp file. Including in your platform \u00b6 Sample DSC change \u00b6 [Components.<arch>] ... ... MsGraphicsPkg/PrintScreenLogger/PrintScreenLogger.inf Sample FDF change \u00b6 [FV.<a DXE firmware volume>] ... ... INF MsGraphicsPkg/PrintScreenLogger/PrintScreenLogger.inf Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Print Screen Logger"},{"location":"dyn/mu_plus/MsGraphicsPkg/PrintScreenLogger/Readme/#printscreenlogger","text":"","title":"PrintScreenLogger"},{"location":"dyn/mu_plus/MsGraphicsPkg/PrintScreenLogger/Readme/#about","text":"PrintScreenLogger is a DXE_DRIVER you can include in your platform to obtain Screen Captures during the preboot environment by pressing the Ctrl-PrtScn key combination. This action will creates a 24bbp (Bits Per Pixel) .BMP file of the screen's contents and write it to a enabled USB drive.","title":"About"},{"location":"dyn/mu_plus/MsGraphicsPkg/PrintScreenLogger/Readme/#supported-architectures","text":"This package is not architecturally dependent. This package is dependent upon the Gop pixel format, and only supports these two pixel formats: 1. PixelRedGreenBlueReserved8BitPerColor 2. PixelBlueGreenRedReserved8BitPerColor","title":"Supported Architectures"},{"location":"dyn/mu_plus/MsGraphicsPkg/PrintScreenLogger/Readme/#printscreenlogger-operation","text":"During initialization, the Print Screen Loggger registers for notification of the Ctrl-PrtScn key combination is pressed. When a Print Screen callback occurs: Looks for a mounted USB drive that contains a file in the root directory called PrintScreenEnable.txt . This limits PrintScreenLogger to only write to enabled USB devices. Looks for the next available filename in the form PrtScreen####.bmp , starting with 0000. Creates the new PrtScreen####.bmp file. Call GraphicsOutput->Blt to obtain the complete screen. Converts the BLT buffer to a 24bbp BMP structure. Writes the BMP structure to the new PrtScreen####.bmp file.","title":"PrintScreenLogger operation"},{"location":"dyn/mu_plus/MsGraphicsPkg/PrintScreenLogger/Readme/#including-in-your-platform","text":"","title":"Including in your platform"},{"location":"dyn/mu_plus/MsGraphicsPkg/PrintScreenLogger/Readme/#sample-dsc-change","text":"[Components.<arch>] ... ... MsGraphicsPkg/PrintScreenLogger/PrintScreenLogger.inf","title":"Sample DSC change"},{"location":"dyn/mu_plus/MsGraphicsPkg/PrintScreenLogger/Readme/#sample-fdf-change","text":"[FV.<a DXE firmware volume>] ... ... INF MsGraphicsPkg/PrintScreenLogger/PrintScreenLogger.inf","title":"Sample FDF change"},{"location":"dyn/mu_plus/MsGraphicsPkg/PrintScreenLogger/Readme/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_plus/MsWheaPkg/readme/","text":"MS WHEA Package \u00b6 About \u00b6 This package contains drivers and infrastructure for reporting errors and telemetry through the CPER (Common Platform Error Record) HwErrRecord interface, specifically targeting systems that also leverage WHEA (Windows Hardware Error Architecture). The MsWhea drivers provide the same functionality at different stages of UEFI by binding to the REPORT_STATUS_CODE interface. Together, they store hardware errors and corrected faults into non-volatile memory that is later picked up by Windows. In Windows, this can be emitted as telemetry which is then used to idenitfy errors and patterns for devices (as of time of writing any event besides EFI_GENERIC_ERROR_INFO will be sent). Project MU has updated design which defines a specific section type under gMuTelemetrySectionTypeGuid to be used in CPER header for WHEA telemetry. All reported data will be formatted to this section. Please refer to MuTelemetryCperSection.h for field definitions. This package also contains optional solutions for persisting error records in the pre-memory space. In early boot stages, drivers need to emit events as critical in order for them to be logged. Detailed information about CPER can be found in Appendix N of the UEFI spec. How to include this driver \u00b6 This driver must be included via DSC by including the EarlyStorageLib: MsWheaEarlyStorageLib|MsWheaPkg/Library/MsWheaEarlyStorageLib/MsWheaEarlyStorageLib.inf Then the PEI stage driver will be included in the DSC Components.IA32 or PEI section: MsWheaPkg/MsWheaReport/Pei/MsWheaReportPei.inf Then the DXE stage driver will be included in the Components.X64 or DXE section: MsWheaPkg/MsWheaReport/Dxe/MsWheaReportDxe.inf Important Notes \u00b6 The PCD value of gMsWheaPkgTokenSpaceGuid.PcdDeviceIdentifierGuid must be overriden by each platform as this is later used in the CPER as the Platform ID (byte offset 32 in the record). In the DXE phase, errors will be picked up by MsWhea for you. In early phases of boot, the errors must be explicitly logged. To do so, first add the library into your INF: MsWheaPkg/MsWheaPkg.dec These headers must be included: - MsWheaErrorStatus.h - MuTelemetryCperSection.h - Library/BaseLib.h Once you're ready to log your error, you can fill the MU_TELEMETRY_CPER_SECTION_DATA and report the StatusCode. The failure type is of type EFI_STATUS_CODE_VALUE. Note that MsWhea drivers will only listen to report status code calls with (EFI_ERROR_MINOR | EFI_ERROR_CODE) or (EFI_ERROR_MAJOR | EFI_ERROR_CODE) at EFI_STATUS_CODE_TYPE. ReportStatusCode( MS_WHEA_ERROR_STATUS_TYPE_FATAL, ); for error under EFI_GENERIC_ERROR_FATAL severity. Or: ReportStatusCode( MS_WHEA_ERROR_STATUS_TYPE_INFO, ); will report errors under EFI_GENERIC_ERROR_INFO severity. Testing \u00b6 There is a UEFI shell application based unit test for WHEA reports. This test attempts to verify basic functionality of public interfaces. Check the UnitTests folder at the root of the package. There is also a feature flag that can inject reports on each boot during various uefi stages. This flag should be off in production. Helper Lib \u00b6 A helper lib to help integrate the MsWhea package has been provided. It is entirely optional and can be easily dropped in. It provides a few macros that are detailed here. LOG_INFO_TELEMETRY ( ClassId, LibraryId, IhvId, ExtraData1, ExtraData ) LOG_INFO_EVENT ( ClassId ) LOG_CRITICAL_TELEMETRY ( ClassId, LibraryId, IhvId, ExtraData1, ExtraData ) LOG_CRITICAL_EVENT ( ClassId ) LOG_FATAL_TELEMETRY ( ClassId, LibraryId, IhvId, ExtraData1, ExtraData ) LOG_FATAL_EVENT ( ClassId ) Currently FATAL and CRITICAL map to the same level in the WHEA log but has been implemented to provide future functionality. The parameters are as follows. ClassId - An EFI_STATUS_CODE_VALUE representing the event that has occurred. This should be unique enough to identify a module or region of code. LibraryId - An optional EFI_GUID that should identify the library emitting this event IhvId - An optional EFI_GUID that should identify the Ihv that is most applicable to this. This will often be NULL ExtraData1 - A UINT64 that can be used to provide contextual or runtime data. It will be persisted and can be useful for debugging purposes. ExtraData2 - Another UINT64 that is also used for contextual and runtime data similar to ExtraData1. By default, gEfiCallerIdGuid is used as the module ID when using the macros. If you need complete control over the WHEA entry, you can use the LogTelemetry function to log a telemetry event. This is the function that the Macros use. More information on this function is in the public header for MuTelemetryHelperLib. Including the Helper Lib \u00b6 The helper lib can easily be included by including it in your DSC. [LibraryClasses] MuTelemetryHelperLib|MsWheaPkg/Library/MuTelemetryHelperLib/MuTelemetryHelperLib.inf Since it is a BASE library, it is available for all architectures. Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Modules"},{"location":"dyn/mu_plus/MsWheaPkg/readme/#ms-whea-package","text":"","title":"MS WHEA Package"},{"location":"dyn/mu_plus/MsWheaPkg/readme/#about","text":"This package contains drivers and infrastructure for reporting errors and telemetry through the CPER (Common Platform Error Record) HwErrRecord interface, specifically targeting systems that also leverage WHEA (Windows Hardware Error Architecture). The MsWhea drivers provide the same functionality at different stages of UEFI by binding to the REPORT_STATUS_CODE interface. Together, they store hardware errors and corrected faults into non-volatile memory that is later picked up by Windows. In Windows, this can be emitted as telemetry which is then used to idenitfy errors and patterns for devices (as of time of writing any event besides EFI_GENERIC_ERROR_INFO will be sent). Project MU has updated design which defines a specific section type under gMuTelemetrySectionTypeGuid to be used in CPER header for WHEA telemetry. All reported data will be formatted to this section. Please refer to MuTelemetryCperSection.h for field definitions. This package also contains optional solutions for persisting error records in the pre-memory space. In early boot stages, drivers need to emit events as critical in order for them to be logged. Detailed information about CPER can be found in Appendix N of the UEFI spec.","title":"About"},{"location":"dyn/mu_plus/MsWheaPkg/readme/#how-to-include-this-driver","text":"This driver must be included via DSC by including the EarlyStorageLib: MsWheaEarlyStorageLib|MsWheaPkg/Library/MsWheaEarlyStorageLib/MsWheaEarlyStorageLib.inf Then the PEI stage driver will be included in the DSC Components.IA32 or PEI section: MsWheaPkg/MsWheaReport/Pei/MsWheaReportPei.inf Then the DXE stage driver will be included in the Components.X64 or DXE section: MsWheaPkg/MsWheaReport/Dxe/MsWheaReportDxe.inf","title":"How to include this driver"},{"location":"dyn/mu_plus/MsWheaPkg/readme/#important-notes","text":"The PCD value of gMsWheaPkgTokenSpaceGuid.PcdDeviceIdentifierGuid must be overriden by each platform as this is later used in the CPER as the Platform ID (byte offset 32 in the record). In the DXE phase, errors will be picked up by MsWhea for you. In early phases of boot, the errors must be explicitly logged. To do so, first add the library into your INF: MsWheaPkg/MsWheaPkg.dec These headers must be included: - MsWheaErrorStatus.h - MuTelemetryCperSection.h - Library/BaseLib.h Once you're ready to log your error, you can fill the MU_TELEMETRY_CPER_SECTION_DATA and report the StatusCode. The failure type is of type EFI_STATUS_CODE_VALUE. Note that MsWhea drivers will only listen to report status code calls with (EFI_ERROR_MINOR | EFI_ERROR_CODE) or (EFI_ERROR_MAJOR | EFI_ERROR_CODE) at EFI_STATUS_CODE_TYPE. ReportStatusCode( MS_WHEA_ERROR_STATUS_TYPE_FATAL, ); for error under EFI_GENERIC_ERROR_FATAL severity. Or: ReportStatusCode( MS_WHEA_ERROR_STATUS_TYPE_INFO, ); will report errors under EFI_GENERIC_ERROR_INFO severity.","title":"Important Notes"},{"location":"dyn/mu_plus/MsWheaPkg/readme/#testing","text":"There is a UEFI shell application based unit test for WHEA reports. This test attempts to verify basic functionality of public interfaces. Check the UnitTests folder at the root of the package. There is also a feature flag that can inject reports on each boot during various uefi stages. This flag should be off in production.","title":"Testing"},{"location":"dyn/mu_plus/MsWheaPkg/readme/#helper-lib","text":"A helper lib to help integrate the MsWhea package has been provided. It is entirely optional and can be easily dropped in. It provides a few macros that are detailed here. LOG_INFO_TELEMETRY ( ClassId, LibraryId, IhvId, ExtraData1, ExtraData ) LOG_INFO_EVENT ( ClassId ) LOG_CRITICAL_TELEMETRY ( ClassId, LibraryId, IhvId, ExtraData1, ExtraData ) LOG_CRITICAL_EVENT ( ClassId ) LOG_FATAL_TELEMETRY ( ClassId, LibraryId, IhvId, ExtraData1, ExtraData ) LOG_FATAL_EVENT ( ClassId ) Currently FATAL and CRITICAL map to the same level in the WHEA log but has been implemented to provide future functionality. The parameters are as follows. ClassId - An EFI_STATUS_CODE_VALUE representing the event that has occurred. This should be unique enough to identify a module or region of code. LibraryId - An optional EFI_GUID that should identify the library emitting this event IhvId - An optional EFI_GUID that should identify the Ihv that is most applicable to this. This will often be NULL ExtraData1 - A UINT64 that can be used to provide contextual or runtime data. It will be persisted and can be useful for debugging purposes. ExtraData2 - Another UINT64 that is also used for contextual and runtime data similar to ExtraData1. By default, gEfiCallerIdGuid is used as the module ID when using the macros. If you need complete control over the WHEA entry, you can use the LogTelemetry function to log a telemetry event. This is the function that the Macros use. More information on this function is in the public header for MuTelemetryHelperLib.","title":"Helper Lib"},{"location":"dyn/mu_plus/MsWheaPkg/readme/#including-the-helper-lib","text":"The helper lib can easily be included by including it in your DSC. [LibraryClasses] MuTelemetryHelperLib|MsWheaPkg/Library/MuTelemetryHelperLib/MuTelemetryHelperLib.inf Since it is a BASE library, it is available for all architectures.","title":"Including the Helper Lib"},{"location":"dyn/mu_plus/MsWheaPkg/readme/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_plus/PcBdsPkg/Library/MsNetworkDependencyLib/readme/","text":"MS Network DependencyLib and NetworkDelayLib \u00b6 About \u00b6 These libraries implement a method to disable the network stack unless there is a reason to use the network. Not starting the network improves boot performance and causes fewer issues during manufacturing. How to use these libraries \u00b6 Add the NetworkDelayLib as a NULL library reference. All this library does is introduce a [Depex] on a NetworkDelay protocol. MdeModulePkg/Universal/Network/SnpDxe/SnpDxe.inf { <LibraryClasses> NULL|PcBdsPkg/Library/MsNetworkDelayLib/MsNetworkDelayLib.inf } The NetworkDependencylib implements a StartNetwork() interface that will publish the NetworkDelay protocol. The future goal of this functionality is to have the EFI_BOOT_MANAGER_POLICY.ConnectDeviceClass() function be overriden and insert a call to DeviceDependencyLib.StartNetwork() when a request is made to start the network class. You nay see other references to NetworkDependencyLib as the conversion to using EFI_BOOT_MANAGER_POLICY.ConnectDeviceClass() is not complete. Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Ms Network Dependency Lib"},{"location":"dyn/mu_plus/PcBdsPkg/Library/MsNetworkDependencyLib/readme/#ms-network-dependencylib-and-networkdelaylib","text":"","title":"MS Network DependencyLib and NetworkDelayLib"},{"location":"dyn/mu_plus/PcBdsPkg/Library/MsNetworkDependencyLib/readme/#about","text":"These libraries implement a method to disable the network stack unless there is a reason to use the network. Not starting the network improves boot performance and causes fewer issues during manufacturing.","title":"About"},{"location":"dyn/mu_plus/PcBdsPkg/Library/MsNetworkDependencyLib/readme/#how-to-use-these-libraries","text":"Add the NetworkDelayLib as a NULL library reference. All this library does is introduce a [Depex] on a NetworkDelay protocol. MdeModulePkg/Universal/Network/SnpDxe/SnpDxe.inf { <LibraryClasses> NULL|PcBdsPkg/Library/MsNetworkDelayLib/MsNetworkDelayLib.inf } The NetworkDependencylib implements a StartNetwork() interface that will publish the NetworkDelay protocol. The future goal of this functionality is to have the EFI_BOOT_MANAGER_POLICY.ConnectDeviceClass() function be overriden and insert a call to DeviceDependencyLib.StartNetwork() when a request is made to start the network class. You nay see other references to NetworkDependencyLib as the conversion to using EFI_BOOT_MANAGER_POLICY.ConnectDeviceClass() is not complete.","title":"How to use these libraries"},{"location":"dyn/mu_plus/PcBdsPkg/Library/MsNetworkDependencyLib/readme/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_plus/SharedCryptoPkg/feature_sharedcrypto/","text":"Shared Crypto Package \u00b6 Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent About \u00b6 The purpose of this package is to provide a wrapper around BaseCryptLib without having to compile the underlying Crypto Library. In the past, this was a source of pain in both compile times and FV size. The package is comprised of a Library Instance that is called SharedCryptoLib. It is contains all of the functions included by BaseCryptLib as well as a few more (mainly X509 related stuff). The library simply locates the protocol and then calls the protocol version of BaseCryptLib (SharedCryptoProtocol). It should be a drop in replacement for the BaseCryptLib. We have decided to hide the protocol headers to discourage users from directly accessing the protocol. The goal of this is to be a drop in replacement for BaseCryptLib. The protocol is installed by several prebuilt EFI that gets downloaded via nuget. The EFI is packaged for consumption via an INF that is loaded and installed. Currently the only supported architectures are AARCH64, X64, and IA32. Caveat: Currently SharedCrypto does not support PEI on AARCH64 or ARM Versioning \u00b6 A typical version consists of 4 numbers. The year, the month of the EDK II release, the revision number, and the build number. An example of this would be 2019.03.02.01 , which would translate to EDK II 1903 release, the second revision and the first build. This means that there were two code changes within 1903 (either in BaseCryptLib or OpenSSL). Release notes will be provided on the NuGet package page and on this repo. Build numbers are reved whenever there needs to be a recompiled binary due to a mistake on our part or a build flag is tweaked. Flavors \u00b6 Additionally there are a few different \"flavors\" of SharedCrypto. The three main flavors are ShaOnly, Mu, and Full. ShaOnly only supports functions related to Sha1 and Sha256 and results in a very small EFI binary. The Mu flavor is the functions that are utilized in the platforms that depend on Project Mu and what we believe are the most practical functions. The full flavor supports all functions that BaseCryptLib does. When a function is called that a flavor doesn't have, the ProtocolFunctionNotFound method from the library is called and an assert is generated. If you have a flavor in mind that doesn't currently exist, open a PR against this repo with the suggested changes, contact one of the maintainers, or use our DriverBuilder to build your own binary. Reproducibility \u00b6 For those wishing to verify for themselves that the packaged EFI's in the Nuget Feed match the code in this package can compile the Driver themselves by sing the SharedCryptoDriver.dsc or SharedCryptoSettings.py. SharedCryptoSettings is the same exact script that we run to update the package in Nuget. Modifying \u00b6 If you wish to swap the underlying Crypto library, replace the BaseCryptLib dependency in the DriverDSC with another version of BaseCryptLib that utilizes another library but conforms to the same interface. If you wish to use a different flavor or make a new flavor, create two new INF's. One for the Binary compilation (in the /Driver folder) and one for the external package that is consumed by the platform (in the /Package folder). By examining existing INF's, it should be fairly trival to create new ones. When making a new flavor please follow the convetion of SharedCryptoPkg{Phase}{Flavor}.{Target}.inf with Phase being DXE, SMM, or PEI. Target should be DEBUG or RELEASE. Using Different Pre-built EFI's \u00b6 If you wish to use your own PreBuild EFI's, they can be placed in the /Package folder and have INF's point to them. Or the INF can be overridden fairly easily. Building SharedCryptoPkg \u00b6 There are two pieces to be build: the library and the driver. The library is to be included in your project as BaseCryptLib. There are 24 different versions of the prebuild driver. One for each arch (X86, AARCH64, ARM, X64) and for each phase (DXE, PEI, SMM) and for mode (DEBUG, RELEASE). The nuget system should take care of downloading the correct version for your project. To build, run SharedNetworkSettings.py. This will compile the correct EFI's and copy the needed files such as the license and markdown. In order to build, you'll need to have a few repos cloned into your tree, which is most easily accomplished by running SharedNetworkSettings.py --SETUP and SharedNetworkSettings.py --UPDATE, which will clone the correct repos automatically for you. Under the hood SharedNetworkSettings is invoking stuart_update, stuart_ci_setup, and BinaryDriveryBuilder as needed. This should be a one-time step to setup the tree. SharedNetworkSettings supports all the same options that a regular platformbuilder does, such as --update and --skipbuild. If you wish to publish to Nuget, provide an API key by including it on the command line. SharedNetworkSettings.py --api-key=XXXXXX The output will go to the Build directory under .SharedCrypto_Nuget Supported Architectures \u00b6 This currently supports x86, x64, AARCH64, and ARM. Including in your platform \u00b6 SharedCryptoLib is a wrapper that has the same API as BaseCryptLib so you can just drop it in. Sample DSC change \u00b6 This would replace where you would normally include BaseCryptLib. [LibraryClasses.X64] BaseCryptLib|SharedCryptoPkg/Library/CryptLibSharedDriver/DxeCryptLibSharedDriver.inf ... [LibraryClasses.IA32.PEIM] BaseCryptLib|SharedCryptoPkg/Library/CryptLibSharedDriver/PeiCryptLibSharedDriver.inf ... [LibraryClasses.DXE_SMM] BaseCryptLib|SharedCryptoPkg/Library/CryptLibSharedDriver/SmmCryptLibSharedDriver.inf ... Unfortunatly, due to the way that the EDK build system works, you'll also need to include the package in your component section. Make sure this matches whatever you put in your FDF. You'll also need to include a macro for the current TARGET. [Components.IA32] SharedCryptoPkg/Package/SharedCryptoPkgPeiShaOnly.$(TARGET).inf ... [Components.X64] SharedCryptoPkg/Package/SharedCryptoPkgDxeMu.$(TARGET).inf SharedCryptoPkg/Package/SharedCryptoPkgSmmMu.$(TARGET).inf ... Make sure that the flavor you're using in your FDF matches the flavor you include in your DSC. Sample FDF change \u00b6 Include this file in your FV and the module will get loaded. In this example, we are looking at the Mu flavor for DXE and the ShaOnly flavor for PEI, but feel free to swap it out with whatever flavor(s) you are using on your platform. [FV.FVBOOTBLOCK] INF SharedCryptoPkg/Package/SharedCryptoPkgPeiShaOnly.$(TARGET).inf ... [FV.FVDXE] INF SharedCryptoPkg/Package/SharedCryptoPkgDxeMu.$(TARGET).inf INF SharedCryptoPkg/Package/SharedCryptoPkgSmmMu.$(TARGET).inf ... Release Process \u00b6 The decision to release a new Shared Crypto shold not be taken lightly. When preparing to release, please follow this checklist. Delete Build folder in the root of your tree Run the build Verify all architectures requested showed up Verify that each flavor is represented Test each architecture on a platform by put it into the ext_dep folder for SharedCrypto This means an ARM64 platform, a x64 platform, and a IA32 platform if possible These platforms must use SharedCrypto, verify in the boot log that you see the images load and install the protocol and PPI A boot test is a fairly good litmus test as on many platforms PEI images need to be hashed to verify integrity, SMM and DXE often use similar hashing to verify firmware volumes and capsules Make sure your platform uses BaseCryptLib in each phase of your product (PEI, DXE, SMM, or others) The boot test is not an end all, be all test. In the future this will include more steps. Check build notes that they are correct Kick off the Azure Pipeline release build Future Improvements \u00b6 Future improvements mostly center around a few specific categories: Release Process and Flavoring In the future we will try to add stricter checks on release process. Right now, it is hard to test all the functionality in SharedCrypto outside of DXE. The boot test right now is the best we have as far as a test goes, but there are some large caveats that need to be addressed. Every platform is different and it can be hard to verify that SharedCrypto was actually exercised. The other area of improvement is making it easier to create a new flavor. For example, if you have a platform that had special hardware acceleration for cryptographic functions and you wanted to take advantages of them, you would want to create a platform specific version of SharedCrypto. Right now, the new invokable method of doing this makes it fairly easy to do multiple compilation passes, but more work needs to be done to enable the scenario I mentioned. Have a question for Shared Crypto? Feel free to post feedback here: https://github.com/microsoft/mu/issues","title":"feature sharedcrypto"},{"location":"dyn/mu_plus/SharedCryptoPkg/feature_sharedcrypto/#shared-crypto-package","text":"","title":"Shared Crypto Package"},{"location":"dyn/mu_plus/SharedCryptoPkg/feature_sharedcrypto/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_plus/SharedCryptoPkg/feature_sharedcrypto/#about","text":"The purpose of this package is to provide a wrapper around BaseCryptLib without having to compile the underlying Crypto Library. In the past, this was a source of pain in both compile times and FV size. The package is comprised of a Library Instance that is called SharedCryptoLib. It is contains all of the functions included by BaseCryptLib as well as a few more (mainly X509 related stuff). The library simply locates the protocol and then calls the protocol version of BaseCryptLib (SharedCryptoProtocol). It should be a drop in replacement for the BaseCryptLib. We have decided to hide the protocol headers to discourage users from directly accessing the protocol. The goal of this is to be a drop in replacement for BaseCryptLib. The protocol is installed by several prebuilt EFI that gets downloaded via nuget. The EFI is packaged for consumption via an INF that is loaded and installed. Currently the only supported architectures are AARCH64, X64, and IA32. Caveat: Currently SharedCrypto does not support PEI on AARCH64 or ARM","title":"About"},{"location":"dyn/mu_plus/SharedCryptoPkg/feature_sharedcrypto/#versioning","text":"A typical version consists of 4 numbers. The year, the month of the EDK II release, the revision number, and the build number. An example of this would be 2019.03.02.01 , which would translate to EDK II 1903 release, the second revision and the first build. This means that there were two code changes within 1903 (either in BaseCryptLib or OpenSSL). Release notes will be provided on the NuGet package page and on this repo. Build numbers are reved whenever there needs to be a recompiled binary due to a mistake on our part or a build flag is tweaked.","title":"Versioning"},{"location":"dyn/mu_plus/SharedCryptoPkg/feature_sharedcrypto/#flavors","text":"Additionally there are a few different \"flavors\" of SharedCrypto. The three main flavors are ShaOnly, Mu, and Full. ShaOnly only supports functions related to Sha1 and Sha256 and results in a very small EFI binary. The Mu flavor is the functions that are utilized in the platforms that depend on Project Mu and what we believe are the most practical functions. The full flavor supports all functions that BaseCryptLib does. When a function is called that a flavor doesn't have, the ProtocolFunctionNotFound method from the library is called and an assert is generated. If you have a flavor in mind that doesn't currently exist, open a PR against this repo with the suggested changes, contact one of the maintainers, or use our DriverBuilder to build your own binary.","title":"Flavors"},{"location":"dyn/mu_plus/SharedCryptoPkg/feature_sharedcrypto/#reproducibility","text":"For those wishing to verify for themselves that the packaged EFI's in the Nuget Feed match the code in this package can compile the Driver themselves by sing the SharedCryptoDriver.dsc or SharedCryptoSettings.py. SharedCryptoSettings is the same exact script that we run to update the package in Nuget.","title":"Reproducibility"},{"location":"dyn/mu_plus/SharedCryptoPkg/feature_sharedcrypto/#modifying","text":"If you wish to swap the underlying Crypto library, replace the BaseCryptLib dependency in the DriverDSC with another version of BaseCryptLib that utilizes another library but conforms to the same interface. If you wish to use a different flavor or make a new flavor, create two new INF's. One for the Binary compilation (in the /Driver folder) and one for the external package that is consumed by the platform (in the /Package folder). By examining existing INF's, it should be fairly trival to create new ones. When making a new flavor please follow the convetion of SharedCryptoPkg{Phase}{Flavor}.{Target}.inf with Phase being DXE, SMM, or PEI. Target should be DEBUG or RELEASE.","title":"Modifying"},{"location":"dyn/mu_plus/SharedCryptoPkg/feature_sharedcrypto/#using-different-pre-built-efis","text":"If you wish to use your own PreBuild EFI's, they can be placed in the /Package folder and have INF's point to them. Or the INF can be overridden fairly easily.","title":"Using Different Pre-built EFI's"},{"location":"dyn/mu_plus/SharedCryptoPkg/feature_sharedcrypto/#building-sharedcryptopkg","text":"There are two pieces to be build: the library and the driver. The library is to be included in your project as BaseCryptLib. There are 24 different versions of the prebuild driver. One for each arch (X86, AARCH64, ARM, X64) and for each phase (DXE, PEI, SMM) and for mode (DEBUG, RELEASE). The nuget system should take care of downloading the correct version for your project. To build, run SharedNetworkSettings.py. This will compile the correct EFI's and copy the needed files such as the license and markdown. In order to build, you'll need to have a few repos cloned into your tree, which is most easily accomplished by running SharedNetworkSettings.py --SETUP and SharedNetworkSettings.py --UPDATE, which will clone the correct repos automatically for you. Under the hood SharedNetworkSettings is invoking stuart_update, stuart_ci_setup, and BinaryDriveryBuilder as needed. This should be a one-time step to setup the tree. SharedNetworkSettings supports all the same options that a regular platformbuilder does, such as --update and --skipbuild. If you wish to publish to Nuget, provide an API key by including it on the command line. SharedNetworkSettings.py --api-key=XXXXXX The output will go to the Build directory under .SharedCrypto_Nuget","title":"Building SharedCryptoPkg"},{"location":"dyn/mu_plus/SharedCryptoPkg/feature_sharedcrypto/#supported-architectures","text":"This currently supports x86, x64, AARCH64, and ARM.","title":"Supported Architectures"},{"location":"dyn/mu_plus/SharedCryptoPkg/feature_sharedcrypto/#including-in-your-platform","text":"SharedCryptoLib is a wrapper that has the same API as BaseCryptLib so you can just drop it in.","title":"Including in your platform"},{"location":"dyn/mu_plus/SharedCryptoPkg/feature_sharedcrypto/#sample-dsc-change","text":"This would replace where you would normally include BaseCryptLib. [LibraryClasses.X64] BaseCryptLib|SharedCryptoPkg/Library/CryptLibSharedDriver/DxeCryptLibSharedDriver.inf ... [LibraryClasses.IA32.PEIM] BaseCryptLib|SharedCryptoPkg/Library/CryptLibSharedDriver/PeiCryptLibSharedDriver.inf ... [LibraryClasses.DXE_SMM] BaseCryptLib|SharedCryptoPkg/Library/CryptLibSharedDriver/SmmCryptLibSharedDriver.inf ... Unfortunatly, due to the way that the EDK build system works, you'll also need to include the package in your component section. Make sure this matches whatever you put in your FDF. You'll also need to include a macro for the current TARGET. [Components.IA32] SharedCryptoPkg/Package/SharedCryptoPkgPeiShaOnly.$(TARGET).inf ... [Components.X64] SharedCryptoPkg/Package/SharedCryptoPkgDxeMu.$(TARGET).inf SharedCryptoPkg/Package/SharedCryptoPkgSmmMu.$(TARGET).inf ... Make sure that the flavor you're using in your FDF matches the flavor you include in your DSC.","title":"Sample DSC change"},{"location":"dyn/mu_plus/SharedCryptoPkg/feature_sharedcrypto/#sample-fdf-change","text":"Include this file in your FV and the module will get loaded. In this example, we are looking at the Mu flavor for DXE and the ShaOnly flavor for PEI, but feel free to swap it out with whatever flavor(s) you are using on your platform. [FV.FVBOOTBLOCK] INF SharedCryptoPkg/Package/SharedCryptoPkgPeiShaOnly.$(TARGET).inf ... [FV.FVDXE] INF SharedCryptoPkg/Package/SharedCryptoPkgDxeMu.$(TARGET).inf INF SharedCryptoPkg/Package/SharedCryptoPkgSmmMu.$(TARGET).inf ...","title":"Sample FDF change"},{"location":"dyn/mu_plus/SharedCryptoPkg/feature_sharedcrypto/#release-process","text":"The decision to release a new Shared Crypto shold not be taken lightly. When preparing to release, please follow this checklist. Delete Build folder in the root of your tree Run the build Verify all architectures requested showed up Verify that each flavor is represented Test each architecture on a platform by put it into the ext_dep folder for SharedCrypto This means an ARM64 platform, a x64 platform, and a IA32 platform if possible These platforms must use SharedCrypto, verify in the boot log that you see the images load and install the protocol and PPI A boot test is a fairly good litmus test as on many platforms PEI images need to be hashed to verify integrity, SMM and DXE often use similar hashing to verify firmware volumes and capsules Make sure your platform uses BaseCryptLib in each phase of your product (PEI, DXE, SMM, or others) The boot test is not an end all, be all test. In the future this will include more steps. Check build notes that they are correct Kick off the Azure Pipeline release build","title":"Release Process"},{"location":"dyn/mu_plus/SharedCryptoPkg/feature_sharedcrypto/#future-improvements","text":"Future improvements mostly center around a few specific categories: Release Process and Flavoring In the future we will try to add stricter checks on release process. Right now, it is hard to test all the functionality in SharedCrypto outside of DXE. The boot test right now is the best we have as far as a test goes, but there are some large caveats that need to be addressed. Every platform is different and it can be hard to verify that SharedCrypto was actually exercised. The other area of improvement is making it easier to create a new flavor. For example, if you have a platform that had special hardware acceleration for cryptographic functions and you wanted to take advantages of them, you would want to create a platform specific version of SharedCrypto. Right now, the new invokable method of doing this makes it fairly easy to do multiple compilation passes, but more work needs to be done to enable the scenario I mentioned. Have a question for Shared Crypto? Feel free to post feedback here: https://github.com/microsoft/mu/issues","title":"Future Improvements"},{"location":"dyn/mu_plus/SharedCryptoPkg/release_notes/","text":"Release Notes for Shared Crypto \u00b6 Shared Crypto is packaged for your convenience. Here are all the versions that we have released in the past (in order of most recent to oldest):","title":"release notes"},{"location":"dyn/mu_plus/SharedCryptoPkg/release_notes/#release-notes-for-shared-crypto","text":"Shared Crypto is packaged for your convenience. Here are all the versions that we have released in the past (in order of most recent to oldest):","title":"Release Notes for Shared Crypto"},{"location":"dyn/mu_plus/SharedCryptoPkg/Driver/Mu-SharedCrypto/","text":"Crypto Bin Package \u00b6 Copyright \u00b6 Version 20190329.0.4 \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent About \u00b6 The purpose of this package is to provide a wrapper around BaseCryptLib without having to compile the EFI. In theory, linking against BaseCryptLib should give you a higher rate of compression since it will inline code, but in practice the linker gets clever and messes things up. It is comprised of a Library Instance that is called SharedCryptoLib. It is contains many of the functions included by BaseCryptLib as well as a few more (mainly X509 related stuff). The library simply locates the protocol and then calls the protocol version of BaseCryptLib (SharedCryptoProtocol). It should be a drop in replacement for the BaseCryptLib. Alternatively, you call the protocol directly. The protocol is installed by a prebuilt EFI that gets downloaded via nuget. The EFI is packaged for consumption via an INF that is loaded and installed. Building SharedCryptoPkg \u00b6 There are two pieces to be build: the library and the driver. The library is to be included in your project There are 24 different versions of the prebuild driver. One for each arch (X86, AARCH64, ARM, X64) and for each phase (DXE, PEI, SMM) and for mode (DEBUG, RELEASE). The nuget system should take care of downloading the correct version for your project. Supported Functions \u00b6 Pkcs PKCS1_ENCRYPT_V2 Encrypts a blob using PKCS1v2 (RSAES-OAEP) schema. On success, will return the encrypted message in a newly allocated buffer. PKCS5_PW_HASH Hashes a password using Pkcs5 PKCS7_VERIFY_EKU Receives a PKCS7 formatted signature, and then verifies that the specified EKU's are present in the end-entity leaf signing certificate PKCS7_VERIFY Verifies the validility of a PKCS7 signed data. The data can be wrappedin a ContentInfo structure. RSA RSA_VERIFY_PKCS1 Verifies the RSA-SSA signature with EMSA-PKCS1-v1_5 encoding scheme defined in RSA PKCS1 RSA_FREE Release the specified RSA context RSA_GET_PUBLIC_KEY_FROM_X509 Retrieve the RSA Public Key from one DER-encoded X509 certificate. SHA SHA1_GET_CONTEXT_SIZE Retrieves the size, in bytes, of the context buffer required for SHA-1 hash operations. SHA1_INIT Initializes user-supplied memory pointed by Sha1Context as SHA-1 hash context for subsequent use. SHA1_DUPLICATE Makes a copy of an existing SHA1 context SHA1_UPDATE Digests input and updates SHA content SHA1_FINAL Completes computation of SHA1 digest values SHA1_HASH_ALL Computes the SHA1 message digest of an data buffer SHA256_GET_CONTEXT_SIZE Retrieves the size, in bytes, of the context buffer required for SHA-256 hash operation SHA256_INIT Initializes user-supplied memory pointed by Sha1Context as SHA-256 hash context for subsequent use. SHA256_DUPLICATE Makes a copy of an existing SHA256 context SHA256_UPDATE Digests input and updates SHA content SHA256_FINAL Completes computation of SHA256 digest values SHA256_HASH_ALL Computes the SHA256 message digest of an data buffer X509 X509_GET_SUBJECT_NAME Retrive the subject btyes from one X.509 cert X509_GET_COMMON_NAME Retrieve the common name (CN) string from one X.509 certificate. X509_GET_ORGANIZATION_NAME Retrieve the organization name (O) string from one X.509 certificate. MD5 + Hmac HMAC_SHA256_GetContextSize HMAC_SHA256_New HMAC_SHA256_Free HMAC_SHA256_Init HMAC_SHA256_Duplicate HMAC_SHA256_Update HMAC_SHA256_Final HMAC_SHA1_GetContextSize HMAC_SHA1_New HMAC_SHA1_Free HMAC_SHA1_Init HMAC_SHA1_Duplicate HMAC_SHA1_Update HMAC_SHA1_Final Random RANDOM_Bytes Generates a pseudorandom byte stream of the specified size RANDOM_Seed Sets up the seed value for the pseudorandom number generator. Supported Architectures \u00b6 This currently supports x86, x64, AARCH64, and ARM. Including in your platform \u00b6 There are two ways to include this in your project: getting the protocol directly or using SharedCryptoLib. SharedCryptoLib is a wrapper that is a super set of the API for BaseCryptLib so you can just drop it in. Sample DSC change \u00b6 This would replace where you would normally include BaseCryptLib. [LibraryClasses.X64] BaseCryptLib|SharedCryptoPkg/Library/SharedCryptoLib/SharedCryptoLibDxe.inf ... [LibraryClasses.IA32.PEIM] BaseCryptLib|SharedCryptoPkg/Library/SharedCryptoLib/SharedCryptoLibPei.inf ... [LibraryClasses.DXE_SMM] BaseCryptLib|SharedCryptoPkg/Library/SharedCryptoLib/SharedCryptoLibSmm.inf ... Sample FDF change \u00b6 TODO: re-evluate this once nuget dependency is live Include this file in your FV and the module will get loaded. [FV.FVDXE] INF SharedCryptoPkg/Package/SharedCryptoPkgDxe.inf ... ...","title":"Driver"},{"location":"dyn/mu_plus/SharedCryptoPkg/Driver/Mu-SharedCrypto/#crypto-bin-package","text":"","title":"Crypto Bin Package"},{"location":"dyn/mu_plus/SharedCryptoPkg/Driver/Mu-SharedCrypto/#copyright","text":"","title":"Copyright"},{"location":"dyn/mu_plus/SharedCryptoPkg/Driver/Mu-SharedCrypto/#version-2019032904","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Version 20190329.0.4"},{"location":"dyn/mu_plus/SharedCryptoPkg/Driver/Mu-SharedCrypto/#about","text":"The purpose of this package is to provide a wrapper around BaseCryptLib without having to compile the EFI. In theory, linking against BaseCryptLib should give you a higher rate of compression since it will inline code, but in practice the linker gets clever and messes things up. It is comprised of a Library Instance that is called SharedCryptoLib. It is contains many of the functions included by BaseCryptLib as well as a few more (mainly X509 related stuff). The library simply locates the protocol and then calls the protocol version of BaseCryptLib (SharedCryptoProtocol). It should be a drop in replacement for the BaseCryptLib. Alternatively, you call the protocol directly. The protocol is installed by a prebuilt EFI that gets downloaded via nuget. The EFI is packaged for consumption via an INF that is loaded and installed.","title":"About"},{"location":"dyn/mu_plus/SharedCryptoPkg/Driver/Mu-SharedCrypto/#building-sharedcryptopkg","text":"There are two pieces to be build: the library and the driver. The library is to be included in your project There are 24 different versions of the prebuild driver. One for each arch (X86, AARCH64, ARM, X64) and for each phase (DXE, PEI, SMM) and for mode (DEBUG, RELEASE). The nuget system should take care of downloading the correct version for your project.","title":"Building SharedCryptoPkg"},{"location":"dyn/mu_plus/SharedCryptoPkg/Driver/Mu-SharedCrypto/#supported-functions","text":"Pkcs PKCS1_ENCRYPT_V2 Encrypts a blob using PKCS1v2 (RSAES-OAEP) schema. On success, will return the encrypted message in a newly allocated buffer. PKCS5_PW_HASH Hashes a password using Pkcs5 PKCS7_VERIFY_EKU Receives a PKCS7 formatted signature, and then verifies that the specified EKU's are present in the end-entity leaf signing certificate PKCS7_VERIFY Verifies the validility of a PKCS7 signed data. The data can be wrappedin a ContentInfo structure. RSA RSA_VERIFY_PKCS1 Verifies the RSA-SSA signature with EMSA-PKCS1-v1_5 encoding scheme defined in RSA PKCS1 RSA_FREE Release the specified RSA context RSA_GET_PUBLIC_KEY_FROM_X509 Retrieve the RSA Public Key from one DER-encoded X509 certificate. SHA SHA1_GET_CONTEXT_SIZE Retrieves the size, in bytes, of the context buffer required for SHA-1 hash operations. SHA1_INIT Initializes user-supplied memory pointed by Sha1Context as SHA-1 hash context for subsequent use. SHA1_DUPLICATE Makes a copy of an existing SHA1 context SHA1_UPDATE Digests input and updates SHA content SHA1_FINAL Completes computation of SHA1 digest values SHA1_HASH_ALL Computes the SHA1 message digest of an data buffer SHA256_GET_CONTEXT_SIZE Retrieves the size, in bytes, of the context buffer required for SHA-256 hash operation SHA256_INIT Initializes user-supplied memory pointed by Sha1Context as SHA-256 hash context for subsequent use. SHA256_DUPLICATE Makes a copy of an existing SHA256 context SHA256_UPDATE Digests input and updates SHA content SHA256_FINAL Completes computation of SHA256 digest values SHA256_HASH_ALL Computes the SHA256 message digest of an data buffer X509 X509_GET_SUBJECT_NAME Retrive the subject btyes from one X.509 cert X509_GET_COMMON_NAME Retrieve the common name (CN) string from one X.509 certificate. X509_GET_ORGANIZATION_NAME Retrieve the organization name (O) string from one X.509 certificate. MD5 + Hmac HMAC_SHA256_GetContextSize HMAC_SHA256_New HMAC_SHA256_Free HMAC_SHA256_Init HMAC_SHA256_Duplicate HMAC_SHA256_Update HMAC_SHA256_Final HMAC_SHA1_GetContextSize HMAC_SHA1_New HMAC_SHA1_Free HMAC_SHA1_Init HMAC_SHA1_Duplicate HMAC_SHA1_Update HMAC_SHA1_Final Random RANDOM_Bytes Generates a pseudorandom byte stream of the specified size RANDOM_Seed Sets up the seed value for the pseudorandom number generator.","title":"Supported Functions"},{"location":"dyn/mu_plus/SharedCryptoPkg/Driver/Mu-SharedCrypto/#supported-architectures","text":"This currently supports x86, x64, AARCH64, and ARM.","title":"Supported Architectures"},{"location":"dyn/mu_plus/SharedCryptoPkg/Driver/Mu-SharedCrypto/#including-in-your-platform","text":"There are two ways to include this in your project: getting the protocol directly or using SharedCryptoLib. SharedCryptoLib is a wrapper that is a super set of the API for BaseCryptLib so you can just drop it in.","title":"Including in your platform"},{"location":"dyn/mu_plus/SharedCryptoPkg/Driver/Mu-SharedCrypto/#sample-dsc-change","text":"This would replace where you would normally include BaseCryptLib. [LibraryClasses.X64] BaseCryptLib|SharedCryptoPkg/Library/SharedCryptoLib/SharedCryptoLibDxe.inf ... [LibraryClasses.IA32.PEIM] BaseCryptLib|SharedCryptoPkg/Library/SharedCryptoLib/SharedCryptoLibPei.inf ... [LibraryClasses.DXE_SMM] BaseCryptLib|SharedCryptoPkg/Library/SharedCryptoLib/SharedCryptoLibSmm.inf ...","title":"Sample DSC change"},{"location":"dyn/mu_plus/SharedCryptoPkg/Driver/Mu-SharedCrypto/#sample-fdf-change","text":"TODO: re-evluate this once nuget dependency is live Include this file in your FV and the module will get loaded. [FV.FVDXE] INF SharedCryptoPkg/Package/SharedCryptoPkgDxe.inf ... ...","title":"Sample FDF change"},{"location":"dyn/mu_plus/UefiTestingPkg/Readme/","text":"UEFI Testing Package \u00b6 About \u00b6 This package adds tests. System Functional tests \u00b6 Tests that invoke system functions and query system state for verification. MemmapAndMatTestApp \u00b6 This test compares the UEFI memory map and Memory Attributes Table against known requirements. The MAT has strict requirements to allow OS usage and page protections. MorLockTestApp \u00b6 This test verifies the UEFI variable store handling of MorLock v1 and v2 behavior. SmmPagingProtections \u00b6 This test verifies the SMM paging attributes by invoking operations that should cause cpu exceptions if the memory protections are in place. The SMM cpu exception handler needs to be configured to force reset on trap to allow automated testing. See UefiCpuPkg/Include/Protocol/SmmExceptionTestProtocol.h , gUefiCpuPkgTokenSpaceGuid.PcdSmmExceptionRebootInsteadOfHaltDefault , and gUefiCpuPkgTokenSpaceGuid.PcdSmmExceptionTestModeSupport . VarPolicyUnitTestApp \u00b6 This test verifies functionality of the Variable Policy Protocol by registering various variable policies and exercising them, as well as tests locking the policy, disabling it, and dumping the policy entries. System Audit tests \u00b6 UEFI applications that collect data from the system and then that data can be used to compare against known good values. UefiVarLockAudit \u00b6 Audit collection tool that gathers information about UEFI variables. This allows auditing the variables within a system, checking attributes, and confirming read/write status. This information is put into an XML file that allows for easy comparison and programmatic auditing. UEFI \u00b6 UEFI shell application that gets the current variable information from the UEFI shell and creates an XML file. Windows \u00b6 Python script that can be run from the Windows OS. It takes the UEFI created XML file as input and then queries all listed variables and updates the XML with access and status codes. This gives additional verification for variables that may employ late locking or other protections from OS access. TpmEventLogAudit \u00b6 Audit tool to collect the TPM Event Log from the system in standard format. It can then be programmatically compared against a known event log for the given system. Easy this that can be tested are the number of events in some PCRs, confirm that all PCRs should be capped, etc. SMMPagingAudit \u00b6 Audit tool creates a human readable description of the SMM page tables and memory environment. App \u00b6 UEFI shell application collects information from SMM and writes it to files. SMM \u00b6 SMM Library linked into SMM driver used to collect information about SMM environment. Activated by the shell app collects IDT, GDT, page tables, and loaded images. Windows \u00b6 Python scripts that process the files generated by the UEFI app and output a report for verification and analysis. Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Readme"},{"location":"dyn/mu_plus/UefiTestingPkg/Readme/#uefi-testing-package","text":"","title":"UEFI Testing Package"},{"location":"dyn/mu_plus/UefiTestingPkg/Readme/#about","text":"This package adds tests.","title":"About"},{"location":"dyn/mu_plus/UefiTestingPkg/Readme/#system-functional-tests","text":"Tests that invoke system functions and query system state for verification.","title":"System Functional tests"},{"location":"dyn/mu_plus/UefiTestingPkg/Readme/#memmapandmattestapp","text":"This test compares the UEFI memory map and Memory Attributes Table against known requirements. The MAT has strict requirements to allow OS usage and page protections.","title":"MemmapAndMatTestApp"},{"location":"dyn/mu_plus/UefiTestingPkg/Readme/#morlocktestapp","text":"This test verifies the UEFI variable store handling of MorLock v1 and v2 behavior.","title":"MorLockTestApp"},{"location":"dyn/mu_plus/UefiTestingPkg/Readme/#smmpagingprotections","text":"This test verifies the SMM paging attributes by invoking operations that should cause cpu exceptions if the memory protections are in place. The SMM cpu exception handler needs to be configured to force reset on trap to allow automated testing. See UefiCpuPkg/Include/Protocol/SmmExceptionTestProtocol.h , gUefiCpuPkgTokenSpaceGuid.PcdSmmExceptionRebootInsteadOfHaltDefault , and gUefiCpuPkgTokenSpaceGuid.PcdSmmExceptionTestModeSupport .","title":"SmmPagingProtections"},{"location":"dyn/mu_plus/UefiTestingPkg/Readme/#varpolicyunittestapp","text":"This test verifies functionality of the Variable Policy Protocol by registering various variable policies and exercising them, as well as tests locking the policy, disabling it, and dumping the policy entries.","title":"VarPolicyUnitTestApp"},{"location":"dyn/mu_plus/UefiTestingPkg/Readme/#system-audit-tests","text":"UEFI applications that collect data from the system and then that data can be used to compare against known good values.","title":"System Audit tests"},{"location":"dyn/mu_plus/UefiTestingPkg/Readme/#uefivarlockaudit","text":"Audit collection tool that gathers information about UEFI variables. This allows auditing the variables within a system, checking attributes, and confirming read/write status. This information is put into an XML file that allows for easy comparison and programmatic auditing.","title":"UefiVarLockAudit"},{"location":"dyn/mu_plus/UefiTestingPkg/Readme/#uefi","text":"UEFI shell application that gets the current variable information from the UEFI shell and creates an XML file.","title":"UEFI"},{"location":"dyn/mu_plus/UefiTestingPkg/Readme/#windows","text":"Python script that can be run from the Windows OS. It takes the UEFI created XML file as input and then queries all listed variables and updates the XML with access and status codes. This gives additional verification for variables that may employ late locking or other protections from OS access.","title":"Windows"},{"location":"dyn/mu_plus/UefiTestingPkg/Readme/#tpmeventlogaudit","text":"Audit tool to collect the TPM Event Log from the system in standard format. It can then be programmatically compared against a known event log for the given system. Easy this that can be tested are the number of events in some PCRs, confirm that all PCRs should be capped, etc.","title":"TpmEventLogAudit"},{"location":"dyn/mu_plus/UefiTestingPkg/Readme/#smmpagingaudit","text":"Audit tool creates a human readable description of the SMM page tables and memory environment.","title":"SMMPagingAudit"},{"location":"dyn/mu_plus/UefiTestingPkg/Readme/#app","text":"UEFI shell application collects information from SMM and writes it to files.","title":"App"},{"location":"dyn/mu_plus/UefiTestingPkg/Readme/#smm","text":"SMM Library linked into SMM driver used to collect information about SMM environment. Activated by the shell app collects IDT, GDT, page tables, and loaded images.","title":"SMM"},{"location":"dyn/mu_plus/UefiTestingPkg/Readme/#windows_1","text":"Python scripts that process the files generated by the UEFI app and output a report for verification and analysis.","title":"Windows"},{"location":"dyn/mu_plus/UefiTestingPkg/Readme/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/DMAProtectionAudit/UEFI/","text":"DMAR Table Audit \u00b6 Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent About \u00b6 DMAProtectionUnitTestApp.c Shell based UEFI unit test based off of the MsUnitTestPkg that test for: 1. IOMMU status register shows IOMMU enabled 2. All excluded regions are set as EfiReservedMemoryType(VTd)/EfiACPIMemoryNVS(IVRS) 3. Bus mastering enabled (BME) is disabled on ExitBootServices. Because we can no longer write to file after ExitBootServices a variable is used to store the test state and the machine. Note: this unit test requires a restart to finish its testing. If you plan to use this unit test in automation make sure to set up your startup.nsh script properly.","title":"UEFI"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/DMAProtectionAudit/UEFI/#dmar-table-audit","text":"","title":"DMAR Table Audit"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/DMAProtectionAudit/UEFI/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/DMAProtectionAudit/UEFI/#about","text":"DMAProtectionUnitTestApp.c Shell based UEFI unit test based off of the MsUnitTestPkg that test for: 1. IOMMU status register shows IOMMU enabled 2. All excluded regions are set as EfiReservedMemoryType(VTd)/EfiACPIMemoryNVS(IVRS) 3. Bus mastering enabled (BME) is disabled on ExitBootServices. Because we can no longer write to file after ExitBootServices a variable is used to store the test state and the machine. Note: this unit test requires a restart to finish its testing. If you plan to use this unit test in automation make sure to set up your startup.nsh script properly.","title":"About"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/DMAProtectionAudit/Windows/Readme/","text":"DMAR Table Audit \u00b6 Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent About \u00b6 DMARTableAudit.py Unit test that checks: 1. DMA remapping bit is enabled 2. No ANDD structures are included in DMAR table 3. RMRRs are limited to only the RMRRs specified in provided XML file (if no XML provided then verify no RMRRs exist) Software Requirements 1. Python3 2. Pywin32 ```pip install Pywin32``` Project Mu python library pip install mu-python-library","title":"Windows"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/DMAProtectionAudit/Windows/Readme/#dmar-table-audit","text":"","title":"DMAR Table Audit"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/DMAProtectionAudit/Windows/Readme/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/DMAProtectionAudit/Windows/Readme/#about","text":"DMARTableAudit.py Unit test that checks: 1. DMA remapping bit is enabled 2. No ANDD structures are included in DMAR table 3. RMRRs are limited to only the RMRRs specified in provided XML file (if no XML provided then verify no RMRRs exist) Software Requirements 1. Python3 2. Pywin32 ```pip install Pywin32``` Project Mu python library pip install mu-python-library","title":"About"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/PagingAudit/","text":"Paging Audit \u00b6 SmmPagingAudit \u00b6 Code Basics \u00b6 SMM is a privileged mode of the ia32/x64 cpu architecture. In this environment nearly all system state can be inspected including that of the operating system, kernel, and hypervisor. Due to it's capabilities SMM has become an area of interest for those searching to exploit the system. To help minimize the interest and impact of an exploit in SMM the SMI handlers should operate in a least privileged model. To do this standard paging can be leveraged to limit the SMI handlers access. Tianocore has a feature to enable paging within SMM and this tool helps confirm the configuration being used. This tool requires three parts to get a complete view. SMM \u00b6 The SMM driver must be included in your build and dispatched to SMM before the End Of Dxe. It is recommended that this driver should only be used on debug builds as it reports the entire SMM memory environment to the caller. The shell app will communicate to the SMM driver and request critical memory information including IDT, GDT, page tables, and loaded images. SMM Version App \u00b6 The UEFI shell application collects system information from the DXE environment and then communicates to the SMM driver/handler to collect necessary info from SMM. It then writes this data to files and then that content is used by the windows scripts. DxePagingAudit \u00b6 Code Basics \u00b6 The Dxe version of paging audit drvier/shell app intends to inspect all 4 levels of page tables and their corresponding Read/Write/Executable permissions. The driver/shell app will collect necessary memrory information from platform enviroment, then iterate through each page entries and log them on to avaiable SimpleFileSystem. The collected *.dat files can be parsed using Windows\\PagingReportGenerator.py. DXE Driver \u00b6 The DXE Driver registers an event to be notified on Exit Boot Services (to change this, replace gEfiEventExitBootServicesGuid with a different event GUID), which will then trigger the paging information collection. DXE Version App \u00b6 The DXE version of UEFI shell application collects necessary system and memory information from DXE when invoked from Shell environment. Windows \u00b6 The Windows script will look at the *.DAT files, parse their content, check for errors and then insert the formatted data into the Html report file. This report file is then double-clickable by the end user/developer to review the posture of the SMM environment. The Results tab applies our suggested rules for SMM to show if the environment passes or fails. If it fails the filters on the data tab can be configured to show where the problem exists. Usage / Enabling on EDK2 based system \u00b6 First, for the SMM driver and app you need to add them to your DSC file for your project so they get compiled. SMM Paging Audit \u00b6 [Components.X64] UefiTestingPkg\\AuditTests\\PagingAudit\\SmmPagingAudit\\Smm\\SmmPagingAuditSmm.inf UefiTestingPkg\\AuditTests\\PagingAudit\\SmmPagingAudit\\App\\SmmPagingAuditApp.inf Next, you must add the SMM driver to a firmware volume in your FDF that can dispatch SMM modules. INF UefiTestingPkg\\AuditTests\\PagingAudit\\SmmPagingAudit\\Smm\\SmmPagingAuditSmm.inf Third, after compiling your new firmware you must: 1. flash that image on the system. 2. Copy the SmmPagingAuditApp.efi to a USB key Then, boot your system running the new firmware to the shell and run the app. The tool will create a set of *.dat files on the same USB key. On a Windows PC, run the Python script on the data found on your USB key. Finally, double-click the HTML output file and check your results. DXE Paging Audit \u00b6 DxePagingAuditDxe \u00b6 Add the following entry to platform dsc file; [Components.X64] UefiTestingPkg\\AuditTests\\PagingAudit\\DxePagingAudit\\Dxe\\DxePagingAuditDxe.inf Add the driver to a firmware volume in your FDF that can dispatch it; INF UefiTestingPkg\\AuditTests\\PagingAudit\\DxePagingAudit\\Dxe\\DxePagingAuditDxe.inf After compiling your new firmware you must: a. flash that image on the system. Boot your system running the new firmware to the shell with a USB plugged in. If the USB disk is FS0:\\, the files should be in FS1:\\. Copy them to the flash drive: copy FS1:\\*.dat FS0:\\ On a Windows PC, run Windows\\PagingReportGenerator.py script with the data found on your USB key. Please use the following command for detailed script instruction: PagingReportGenerator.py -h Double-click the HTML output file and check your results; DxePagingAuditApp \u00b6 Add the following entry to platform dsc file; [Components.X64] UefiTestingPkg\\AuditTests\\PagingAudit\\DxePagingAudit\\App\\DxePagingAuditApp.inf Compile the newly added application and copy DxePagingAuditApp.efi to a USB key; Boot your system to the shell with the USB plugged in. If the USB disk is FS0:\\, the files should be in FS1:\\. Copy them to the flash drive: FS0:\\ DxePagingAuditApp.efi copy FS1:\\*.dat FS0:\\ Follow step 5 - 6 from DxePagingAuditDxe section; Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Paging Audit"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/PagingAudit/#paging-audit","text":"","title":"Paging Audit"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/PagingAudit/#smmpagingaudit","text":"","title":"SmmPagingAudit"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/PagingAudit/#code-basics","text":"SMM is a privileged mode of the ia32/x64 cpu architecture. In this environment nearly all system state can be inspected including that of the operating system, kernel, and hypervisor. Due to it's capabilities SMM has become an area of interest for those searching to exploit the system. To help minimize the interest and impact of an exploit in SMM the SMI handlers should operate in a least privileged model. To do this standard paging can be leveraged to limit the SMI handlers access. Tianocore has a feature to enable paging within SMM and this tool helps confirm the configuration being used. This tool requires three parts to get a complete view.","title":"Code Basics"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/PagingAudit/#smm","text":"The SMM driver must be included in your build and dispatched to SMM before the End Of Dxe. It is recommended that this driver should only be used on debug builds as it reports the entire SMM memory environment to the caller. The shell app will communicate to the SMM driver and request critical memory information including IDT, GDT, page tables, and loaded images.","title":"SMM"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/PagingAudit/#smm-version-app","text":"The UEFI shell application collects system information from the DXE environment and then communicates to the SMM driver/handler to collect necessary info from SMM. It then writes this data to files and then that content is used by the windows scripts.","title":"SMM Version App"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/PagingAudit/#dxepagingaudit","text":"","title":"DxePagingAudit"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/PagingAudit/#code-basics_1","text":"The Dxe version of paging audit drvier/shell app intends to inspect all 4 levels of page tables and their corresponding Read/Write/Executable permissions. The driver/shell app will collect necessary memrory information from platform enviroment, then iterate through each page entries and log them on to avaiable SimpleFileSystem. The collected *.dat files can be parsed using Windows\\PagingReportGenerator.py.","title":"Code Basics"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/PagingAudit/#dxe-driver","text":"The DXE Driver registers an event to be notified on Exit Boot Services (to change this, replace gEfiEventExitBootServicesGuid with a different event GUID), which will then trigger the paging information collection.","title":"DXE Driver"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/PagingAudit/#dxe-version-app","text":"The DXE version of UEFI shell application collects necessary system and memory information from DXE when invoked from Shell environment.","title":"DXE Version App"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/PagingAudit/#windows","text":"The Windows script will look at the *.DAT files, parse their content, check for errors and then insert the formatted data into the Html report file. This report file is then double-clickable by the end user/developer to review the posture of the SMM environment. The Results tab applies our suggested rules for SMM to show if the environment passes or fails. If it fails the filters on the data tab can be configured to show where the problem exists.","title":"Windows"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/PagingAudit/#usage-enabling-on-edk2-based-system","text":"First, for the SMM driver and app you need to add them to your DSC file for your project so they get compiled.","title":"Usage / Enabling on EDK2 based system"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/PagingAudit/#smm-paging-audit","text":"[Components.X64] UefiTestingPkg\\AuditTests\\PagingAudit\\SmmPagingAudit\\Smm\\SmmPagingAuditSmm.inf UefiTestingPkg\\AuditTests\\PagingAudit\\SmmPagingAudit\\App\\SmmPagingAuditApp.inf Next, you must add the SMM driver to a firmware volume in your FDF that can dispatch SMM modules. INF UefiTestingPkg\\AuditTests\\PagingAudit\\SmmPagingAudit\\Smm\\SmmPagingAuditSmm.inf Third, after compiling your new firmware you must: 1. flash that image on the system. 2. Copy the SmmPagingAuditApp.efi to a USB key Then, boot your system running the new firmware to the shell and run the app. The tool will create a set of *.dat files on the same USB key. On a Windows PC, run the Python script on the data found on your USB key. Finally, double-click the HTML output file and check your results.","title":"SMM Paging Audit"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/PagingAudit/#dxe-paging-audit","text":"","title":"DXE Paging Audit"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/PagingAudit/#dxepagingauditdxe","text":"Add the following entry to platform dsc file; [Components.X64] UefiTestingPkg\\AuditTests\\PagingAudit\\DxePagingAudit\\Dxe\\DxePagingAuditDxe.inf Add the driver to a firmware volume in your FDF that can dispatch it; INF UefiTestingPkg\\AuditTests\\PagingAudit\\DxePagingAudit\\Dxe\\DxePagingAuditDxe.inf After compiling your new firmware you must: a. flash that image on the system. Boot your system running the new firmware to the shell with a USB plugged in. If the USB disk is FS0:\\, the files should be in FS1:\\. Copy them to the flash drive: copy FS1:\\*.dat FS0:\\ On a Windows PC, run Windows\\PagingReportGenerator.py script with the data found on your USB key. Please use the following command for detailed script instruction: PagingReportGenerator.py -h Double-click the HTML output file and check your results;","title":"DxePagingAuditDxe"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/PagingAudit/#dxepagingauditapp","text":"Add the following entry to platform dsc file; [Components.X64] UefiTestingPkg\\AuditTests\\PagingAudit\\DxePagingAudit\\App\\DxePagingAuditApp.inf Compile the newly added application and copy DxePagingAuditApp.efi to a USB key; Boot your system to the shell with the USB plugged in. If the USB disk is FS0:\\, the files should be in FS1:\\. Copy them to the flash drive: FS0:\\ DxePagingAuditApp.efi copy FS1:\\*.dat FS0:\\ Follow step 5 - 6 from DxePagingAuditDxe section;","title":"DxePagingAuditApp"},{"location":"dyn/mu_plus/UefiTestingPkg/AuditTests/PagingAudit/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_plus/UefiTestingPkg/FunctionalSystemTests/HeapGuardTest/Readme/","text":"Heap Guard Tests \u00b6 \ud83d\udd39 Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent About This Test \u00b6 This is set of tests to ensure that heap guard, stack guard, null pointer detection, and nx protections are working properly. It consists of: An SMM driver A Shell-based test app The Shell-based app may be built at any time and run from Shell. The app can use the SMM driver to preform SMM tests if the SMM driver is installed. It is not the intention of this test to include the driver in production systems. They should only be used for purpose-built test images.","title":"Heap Guard Test"},{"location":"dyn/mu_plus/UefiTestingPkg/FunctionalSystemTests/HeapGuardTest/Readme/#heap-guard-tests","text":"","title":"Heap Guard Tests"},{"location":"dyn/mu_plus/UefiTestingPkg/FunctionalSystemTests/HeapGuardTest/Readme/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"&#x1F539; Copyright"},{"location":"dyn/mu_plus/UefiTestingPkg/FunctionalSystemTests/HeapGuardTest/Readme/#about-this-test","text":"This is set of tests to ensure that heap guard, stack guard, null pointer detection, and nx protections are working properly. It consists of: An SMM driver A Shell-based test app The Shell-based app may be built at any time and run from Shell. The app can use the SMM driver to preform SMM tests if the SMM driver is installed. It is not the intention of this test to include the driver in production systems. They should only be used for purpose-built test images.","title":"About This Test"},{"location":"dyn/mu_plus/UefiTestingPkg/FunctionalSystemTests/SmmPagingProtectionsTest/Readme/","text":"SMM Paging Protections Test \u00b6 \ud83d\udd39 Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent About This Test \u00b6 This is a [currently] small test to prove that certain SMM paging protections have been applied. It consists of: An SMM driver A DXE driver A Shell-based test app In order to use this test, the SMM and DXE drivers must be built and included in your FW image to be dispatched at boot time. The Shell-based app may be built at any time and run from Shell. The app will ask the DXE driver to pass a message to the SMM driver to invoke a particular test. It is not the intention of this test to include the two drivers in production systems. They should only be used for purpose-built test images.","title":"Smm Paging Protections Test"},{"location":"dyn/mu_plus/UefiTestingPkg/FunctionalSystemTests/SmmPagingProtectionsTest/Readme/#smm-paging-protections-test","text":"","title":"SMM Paging Protections Test"},{"location":"dyn/mu_plus/UefiTestingPkg/FunctionalSystemTests/SmmPagingProtectionsTest/Readme/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"&#x1F539; Copyright"},{"location":"dyn/mu_plus/UefiTestingPkg/FunctionalSystemTests/SmmPagingProtectionsTest/Readme/#about-this-test","text":"This is a [currently] small test to prove that certain SMM paging protections have been applied. It consists of: An SMM driver A DXE driver A Shell-based test app In order to use this test, the SMM and DXE drivers must be built and included in your FW image to be dispatched at boot time. The Shell-based app may be built at any time and run from Shell. The app will ask the DXE driver to pass a message to the SMM driver to invoke a particular test. It is not the intention of this test to include the two drivers in production systems. They should only be used for purpose-built test images.","title":"About This Test"},{"location":"dyn/mu_plus/UefiTestingPkg/FunctionalSystemTests/VarPolicyUnitTestApp/Readme/","text":"Varible Policy Unit Tests \u00b6 \ud83d\udd39 Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent About This Test \u00b6 This test verifies functionality of the Variable Policy Protocol by registering various variable policies and exercising them, as well as tests locking the policy, disabling it, and dumping the policy entries. Only policies that are created as a part of this test will be tested. 1. Try getting test context, if empty then get VP protocol, confirm that VP is not disabled by calling IsVariablePolicyEnabled. Log VP revision. 2. \"No lock\" policies: * check minsize enforcement * check maxsize enforcement * check musthave attr enforcement * check canthave attr enforcement * check one of the above with empty string policy i.e. name wildcard * check another one of the above with a \"#\" containing policy string * check policy prioritization by having a namespace-wide policy, a policy with a # wildcard, and a one-var specific policy and testing which one is enforced 3. \"Lock now\" policies (means if the var doesn't exist, it won't be created; if one exists, it can't be updated): * test a policy for an already existing variable, verify we can't write into that variable * create a policy for a non-existing variable and attempt to register such var 4. \"Lock on create\" policies (means the var can still be created, but no updates later, existing vars can't be updated): * create a var, lock it with LockOnCreate, attempt to update its contents * create LockOnCreate VP, attempt to create var with invalid size, then invalid attr, then create valid var, attempt to update its contents 5. \"Lock on var state\" policies (means the var protected by this policy can't be created or updated once the trigger is set) * create VP, trigger lock with a valid var, attempt to create a locked var, then modify the trigger var, create locked var * create VP, create targeted var, modify it, trigger lock, attempt to modify var * create VP, trigger lock with invalid (larger than one byte) var, see if VPE allows creation of the locked var (it should allow) * create VP, set locking var with wrong value, see if VPE allows creation of the locked var (should allow) 6. Attempt registering invalid policy entries * invalid required and banned attributes * large min size - let's say 2GB * max size equal to 0 * invalid policy type 7. Exercise dumping policy. No need to check the validity of the dump blob. 8. Test registering a policy with a random version. 9. Lock VPE, make sure old policies are enforced, new ones can't be registered. * Register a LockOnCreate policy * Lock VPE * Test locking it again. * Verify one of the prior policies is enforced * Make sure we can create variables even if those are protected by LockOnCreate policy, after locking the VPE * Attempt to register new policies * Make sure can't disable VPE * Cleanup: save context and reboot 10. Disable variable policy and try some things * Locate Variable Policy Protocol * Make sure VP is enabled * Register a policy * Disable VPE * Call IsVariablePolicyEnabled to confirm it's disabled. * Make sure can't lock policy * Make sure the policy from a is no longer enforced * Final cleanup: delete vars that were created in some earlier test suites","title":"Var Policy Unit Test App"},{"location":"dyn/mu_plus/UefiTestingPkg/FunctionalSystemTests/VarPolicyUnitTestApp/Readme/#varible-policy-unit-tests","text":"","title":"Varible Policy Unit Tests"},{"location":"dyn/mu_plus/UefiTestingPkg/FunctionalSystemTests/VarPolicyUnitTestApp/Readme/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"&#x1F539; Copyright"},{"location":"dyn/mu_plus/UefiTestingPkg/FunctionalSystemTests/VarPolicyUnitTestApp/Readme/#about-this-test","text":"This test verifies functionality of the Variable Policy Protocol by registering various variable policies and exercising them, as well as tests locking the policy, disabling it, and dumping the policy entries. Only policies that are created as a part of this test will be tested. 1. Try getting test context, if empty then get VP protocol, confirm that VP is not disabled by calling IsVariablePolicyEnabled. Log VP revision. 2. \"No lock\" policies: * check minsize enforcement * check maxsize enforcement * check musthave attr enforcement * check canthave attr enforcement * check one of the above with empty string policy i.e. name wildcard * check another one of the above with a \"#\" containing policy string * check policy prioritization by having a namespace-wide policy, a policy with a # wildcard, and a one-var specific policy and testing which one is enforced 3. \"Lock now\" policies (means if the var doesn't exist, it won't be created; if one exists, it can't be updated): * test a policy for an already existing variable, verify we can't write into that variable * create a policy for a non-existing variable and attempt to register such var 4. \"Lock on create\" policies (means the var can still be created, but no updates later, existing vars can't be updated): * create a var, lock it with LockOnCreate, attempt to update its contents * create LockOnCreate VP, attempt to create var with invalid size, then invalid attr, then create valid var, attempt to update its contents 5. \"Lock on var state\" policies (means the var protected by this policy can't be created or updated once the trigger is set) * create VP, trigger lock with a valid var, attempt to create a locked var, then modify the trigger var, create locked var * create VP, create targeted var, modify it, trigger lock, attempt to modify var * create VP, trigger lock with invalid (larger than one byte) var, see if VPE allows creation of the locked var (it should allow) * create VP, set locking var with wrong value, see if VPE allows creation of the locked var (should allow) 6. Attempt registering invalid policy entries * invalid required and banned attributes * large min size - let's say 2GB * max size equal to 0 * invalid policy type 7. Exercise dumping policy. No need to check the validity of the dump blob. 8. Test registering a policy with a random version. 9. Lock VPE, make sure old policies are enforced, new ones can't be registered. * Register a LockOnCreate policy * Lock VPE * Test locking it again. * Verify one of the prior policies is enforced * Make sure we can create variables even if those are protected by LockOnCreate policy, after locking the VPE * Attempt to register new policies * Make sure can't disable VPE * Cleanup: save context and reboot 10. Disable variable policy and try some things * Locate Variable Policy Protocol * Make sure VP is enabled * Register a policy * Disable VPE * Call IsVariablePolicyEnabled to confirm it's disabled. * Make sure can't lock policy * Make sure the policy from a is no longer enforced * Final cleanup: delete vars that were created in some earlier test suites","title":"About This Test"},{"location":"dyn/mu_plus/XmlSupportPkg/ReadMe/","text":"Xml Support Package \u00b6 About \u00b6 This package adds some limited XML support to the UEFI environment. Xml brings value in that there are numerous, robust, readily available parsing solutions in nearly every environment, language, and operating system. The UEFI support is limited in that it only supports the ASCII strings and does not support XSD, schema, namespaces, or other extensions to XML. XmlTreeLib \u00b6 The XmlTreeLib is the cornerstone of this package. It provides functions for: * Reading and parsing XML strings into an XML node/tree structure * Creating or altering xml nodes within a tree * Writing xml nodes/trees to ASCII string * Escaping and Un-Escaping strings XmlTreeQueryLib \u00b6 The XmlTreeQueryLib provides very basic and simple query functions allowing code to interact with the XmlTree to do things like: * Find the first child element node with a name equal to the parameter * Find the first attribute node of a given element with a name equal to the parameter UnitTestResultReportLib \u00b6 A UnitTestResultReportLib that formats the results in XML using the JUnit defined schema. This instance allows the UEFI Unit Test Framework to integrate results with existing tools and other frameworks. Testing \u00b6 There are UEFI shell application based unit tests for each library. These tests attempt to verify basic functionality of public interfaces. Check the UnitTests folder at the root of the package. Developer Notes \u00b6 These libraries have known limitations and have not been fully vetted for un-trusted input. If used in such a situation it is suggested to validate the input before leveraging the XML libraries. With that said the ability to use xml in UEFI has been invaluable for building features and tests that interact with code running in other environments. The parser has been tuned to fail fast and in invalid XML just return NULL. Copyright \u00b6 Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Modules"},{"location":"dyn/mu_plus/XmlSupportPkg/ReadMe/#xml-support-package","text":"","title":"Xml Support Package"},{"location":"dyn/mu_plus/XmlSupportPkg/ReadMe/#about","text":"This package adds some limited XML support to the UEFI environment. Xml brings value in that there are numerous, robust, readily available parsing solutions in nearly every environment, language, and operating system. The UEFI support is limited in that it only supports the ASCII strings and does not support XSD, schema, namespaces, or other extensions to XML.","title":"About"},{"location":"dyn/mu_plus/XmlSupportPkg/ReadMe/#xmltreelib","text":"The XmlTreeLib is the cornerstone of this package. It provides functions for: * Reading and parsing XML strings into an XML node/tree structure * Creating or altering xml nodes within a tree * Writing xml nodes/trees to ASCII string * Escaping and Un-Escaping strings","title":"XmlTreeLib"},{"location":"dyn/mu_plus/XmlSupportPkg/ReadMe/#xmltreequerylib","text":"The XmlTreeQueryLib provides very basic and simple query functions allowing code to interact with the XmlTree to do things like: * Find the first child element node with a name equal to the parameter * Find the first attribute node of a given element with a name equal to the parameter","title":"XmlTreeQueryLib"},{"location":"dyn/mu_plus/XmlSupportPkg/ReadMe/#unittestresultreportlib","text":"A UnitTestResultReportLib that formats the results in XML using the JUnit defined schema. This instance allows the UEFI Unit Test Framework to integrate results with existing tools and other frameworks.","title":"UnitTestResultReportLib"},{"location":"dyn/mu_plus/XmlSupportPkg/ReadMe/#testing","text":"There are UEFI shell application based unit tests for each library. These tests attempt to verify basic functionality of public interfaces. Check the UnitTests folder at the root of the package.","title":"Testing"},{"location":"dyn/mu_plus/XmlSupportPkg/ReadMe/#developer-notes","text":"These libraries have known limitations and have not been fully vetted for un-trusted input. If used in such a situation it is suggested to validate the input before leveraging the XML libraries. With that said the ability to use xml in UEFI has been invaluable for building features and tests that interact with code running in other environments. The parser has been tuned to fail fast and in invalid XML just return NULL.","title":"Developer Notes"},{"location":"dyn/mu_plus/XmlSupportPkg/ReadMe/#copyright","text":"Copyright (C) Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_silicon_arm_tiano/RepoDetails/","text":"Project Mu Silicon Arm Tiano Repository \u00b6 Git Details Repository Url: https://github.com/Microsoft/mu_silicon_arm_tiano.git Branch: release/201911 Commit: 2db66cfdc961f73c252fbea260d697eca20ba2cf Commit Date: 2019-12-07 07:27:39 +0000 This repository contains Project Mu code based on TianoCore edk2 code for ARM silicon features and ARM based platforms. More Info \u00b6 This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments. Issues \u00b6 Please open any issues in the Project Mu GitHub tracker. More Details Contributing Code or Docs \u00b6 Please follow the general Project Mu Pull Request process. More Details Code Requirements Doc Requirements Builds \u00b6 pip install --upgrade -r requirements.txt mu_build -c corebuild.mu.json Copyright & License \u00b6 Copyright \u00a9 Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent Upstream License (TianoCore) \u00b6 Copyright \u00a9 2019, TianoCore and contributors. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Subject to the terms and conditions of this license, each copyright holder and contributor hereby grants to those receiving rights under this license a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except for failure to satisfy the conditions of this license) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer this software, where such license applies only to those patent claims, already acquired or hereafter acquired, licensable by such copyright holder or contributor that are necessarily infringed by: (a) their Contribution(s) (the licensed copyrights of copyright holders and non-copyrightable additions of contributors, in source or binary form) alone; or (b) combination of their Contribution(s) with the work of authorship to which such Contribution(s) was added by such copyright holder or contributor, if, at the time the Contribution is added, such addition causes such combination to be necessarily infringed. The patent license shall not apply to any other combinations which include the Contribution. Except as expressly stated above, no rights or licenses from any copyright holder or contributor is granted under this license, whether expressly, by implication, estoppel or otherwise. DISCLAIMER THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"Repo Details"},{"location":"dyn/mu_silicon_arm_tiano/RepoDetails/#project-mu-silicon-arm-tiano-repository","text":"Git Details Repository Url: https://github.com/Microsoft/mu_silicon_arm_tiano.git Branch: release/201911 Commit: 2db66cfdc961f73c252fbea260d697eca20ba2cf Commit Date: 2019-12-07 07:27:39 +0000 This repository contains Project Mu code based on TianoCore edk2 code for ARM silicon features and ARM based platforms.","title":"Project Mu Silicon Arm Tiano Repository"},{"location":"dyn/mu_silicon_arm_tiano/RepoDetails/#more-info","text":"This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.","title":"More Info"},{"location":"dyn/mu_silicon_arm_tiano/RepoDetails/#issues","text":"Please open any issues in the Project Mu GitHub tracker. More Details","title":"Issues"},{"location":"dyn/mu_silicon_arm_tiano/RepoDetails/#contributing-code-or-docs","text":"Please follow the general Project Mu Pull Request process. More Details Code Requirements Doc Requirements","title":"Contributing Code or Docs"},{"location":"dyn/mu_silicon_arm_tiano/RepoDetails/#builds","text":"pip install --upgrade -r requirements.txt mu_build -c corebuild.mu.json","title":"Builds"},{"location":"dyn/mu_silicon_arm_tiano/RepoDetails/#copyright-license","text":"Copyright \u00a9 Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright &amp; License"},{"location":"dyn/mu_silicon_arm_tiano/RepoDetails/#upstream-license-tianocore","text":"Copyright \u00a9 2019, TianoCore and contributors. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Subject to the terms and conditions of this license, each copyright holder and contributor hereby grants to those receiving rights under this license a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except for failure to satisfy the conditions of this license) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer this software, where such license applies only to those patent claims, already acquired or hereafter acquired, licensable by such copyright holder or contributor that are necessarily infringed by: (a) their Contribution(s) (the licensed copyrights of copyright holders and non-copyrightable additions of contributors, in source or binary form) alone; or (b) combination of their Contribution(s) with the work of authorship to which such Contribution(s) was added by such copyright holder or contributor, if, at the time the Contribution is added, such addition causes such combination to be necessarily infringed. The patent license shall not apply to any other combinations which include the Contribution. Except as expressly stated above, no rights or licenses from any copyright holder or contributor is granted under this license, whether expressly, by implication, estoppel or otherwise. DISCLAIMER THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"Upstream License (TianoCore)"},{"location":"dyn/mu_silicon_arm_tiano/ArmPkg/Library/ArmSoftFloatLib/berkeley-softfloat-3/","text":"Package Overview for Berkeley SoftFloat Release 3e \u00b6 John R. Hauser 2018 January 20 Berkeley SoftFloat is a software implementation of binary floating-point that conforms to the IEEE Standard for Floating-Point Arithmetic. SoftFloat is distributed in the form of C source code. Building the SoftFloat sources generates a library file (typically softfloat.a or libsoftfloat.a ) containing the floating-point subroutines. The SoftFloat package is documented in the following files in the doc subdirectory: SoftFloat.html Documentation for using the SoftFloat functions. SoftFloat-source.html Documentation for building SoftFloat. SoftFloat-history.html History of the major changes to SoftFloat. Other files in the package comprise the source code for SoftFloat.","title":"berkeley-softfloat-3"},{"location":"dyn/mu_silicon_arm_tiano/ArmPkg/Library/ArmSoftFloatLib/berkeley-softfloat-3/#package-overview-for-berkeley-softfloat-release-3e","text":"John R. Hauser 2018 January 20 Berkeley SoftFloat is a software implementation of binary floating-point that conforms to the IEEE Standard for Floating-Point Arithmetic. SoftFloat is distributed in the form of C source code. Building the SoftFloat sources generates a library file (typically softfloat.a or libsoftfloat.a ) containing the floating-point subroutines. The SoftFloat package is documented in the following files in the doc subdirectory: SoftFloat.html Documentation for using the SoftFloat functions. SoftFloat-source.html Documentation for building SoftFloat. SoftFloat-history.html History of the major changes to SoftFloat. Other files in the package comprise the source code for SoftFloat.","title":"Package Overview for Berkeley SoftFloat Release 3e"},{"location":"dyn/mu_silicon_arm_tiano/pytool/Readme/","text":"Edk2 Continuous Integration \u00b6 Basic Status \u00b6 Package Windows VS2019 (IA32/X64) Ubuntu GCC (IA32/X64/ARM/AARCH64) Known Issues ArmPkg ArmPlatformPkg ArmVirtPkg For more detailed status look at the test results of the latest CI run on the repo readme. Background \u00b6 This Continuous integration and testing infrastructure leverages the TianoCore EDKII Tools PIP modules: library and extensions (with repos located here and here ). The primary execution flows can be found in the .azurepipelines / Windows-VS2019.yml and .azurepipelines / Ubuntu-GCC5.yml files. These YAML files are consumed by the Azure Dev Ops Build Pipeline and dictate what server resources should be used, how they should be configured, and what processes should be run on them. An overview of this schema can be found here . Inspection of these files reveals the EDKII Tools commands that make up the primary processes for the CI build: 'stuart_setup', 'stuart_update', and 'stuart_ci_build'. These commands come from the EDKII Tools PIP modules and are configured as described below. More documentation on the tools can be found here and here . Configuration \u00b6 Configuration of the CI process consists of (in order of precedence): command-line arguments passed in via the Pipeline YAML a per-package configuration file (e.g. <package-name>.ci.yaml ) that is detected by the CI system in EDKII Tools. a global configuration Python module (e.g. CISetting.py ) passed in via the command-line The global configuration file is described in this readme from the EDKII Tools documentation. This configuration is written as a Python module so that decisions can be made dynamically based on command line parameters and codebase state. The per-package configuration file can override most settings in the global configuration file, but is not dynamic. This file can be used to skip or customize tests that may be incompatible with a specific package. Each test generally requires per package configuration which comes from this file. Running CI locally \u00b6 The EDKII Tools environment (and by extension the ci) is designed to support easily and consistantly running locally and in a cloud ci environment. To do that a few steps should be followed. Details of EDKII Tools can be found in the docs folder here Prerequisets \u00b6 A supported toolchain (others might work but this is what is tested and validated) Windows 10: VS 2017 or VS 2019 Windows SDK (for rc) Windows WDK (for capsules) Ubuntu 16.04 GCC5 Easy to add more but this is the current state Python 3.7.x or newer on path git on path Recommended to setup and activate a python virtual environment Install the requirements pip install --upgrade pip-requirements.txt Running CI \u00b6 clone your edk2 repo Activate your python virtual environment in cmd window Get code dependencies (done only when submodules change) stuart_setup -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> Update other dependencies (done more often) stuart_update -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> Run CI build (--help will give you options) stuart_ci_build -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> -p : To build only certain packages use a CSV list -a : To run only certain architectures use a CSV list -t : To run only tests related to certain targets use a CSV list By default all tests are opted in. Then given a package.ci.yaml file those tests can be configured for a package. Finally setting the check to the value skip will skip that plugin. Examples: CompilerPlugin=skip skip the build test GuidCheck=skip skip the Guid check SpellCheck=skip skip the spell checker etc Detailed reports and logs per package are captured in the Build directory Current PyTool Test Capabilities \u00b6 All CI tests are instances of EDKII Tools plugins. Documentation on the plugin system can be found here and here . Upon invocation, each plugin will be passed the path to the current package under test and a dictionary containing its targeted configuration, as assembled from the command line, per-package configuration, and global configuration. Note: CI plugins are considered unique from build plugins and helper plugins, even though some CI plugins may execute steps of a build. In the example, these plugins live alongside the code under test (in the .pytool / Plugin directory), but may be moved to the 'edk2-test' repo if that location makes more sense for the community. Module Inclusion Test - DscCompleteCheck \u00b6 This test scans all available modules (via INF files) and compares them to the package-level DSC file for the package each module is contained within. The test considers it an error if any module does not appear in the Components section of at least one package-level DSC (indicating that it would not be built if the package were built). Code Compilation Test - CompilerPlugin \u00b6 Once the Module Inclusion Test has verified that all modules would be built if all package-level DSCs were built, the Code Compilation Test simply runs through and builds every package-level DSC on every toolchain and for every architecture that is supported. Any module that fails to build is considered an error. GUID Uniqueness Test - GuidCheck \u00b6 This test works on the collection of all packages rather than an individual package. It looks at all FILE_GUIDs and GUIDs declared in DEC files and ensures that they are unique for the codebase. This prevents, for example, accidental duplication of GUIDs when using an existing INF as a template for a new module. Cross-Package Dependency Test - DependencyCheck \u00b6 This test compares the list of all packages used in INFs files for a given package against a list of \"allowed dependencies\" in plugin configuration for that package. Any module that depends on a disallowed package will cause a test failure. Library Declaration Test - LibraryClassCheck \u00b6 This test scans at all library header files found in the Library folders in all of the package's declared include directories and ensures that all files have a matching LibraryClass declaration in the DEC file for the package. Any missing declarations will cause a failure. Invalid Character Test - CharEncodingCheck \u00b6 This test scans all files in a package to make sure that there are no invalid Unicode characters that may cause build errors in some character sets/localizations. Spell Checking - cspell \u00b6 This test runs a spell checker on all files within the package. This is done using the NodeJs cspell tool. For details check .pytool / Plugin / SpellCheck . For this plugin to run during ci you must install nodejs and cspell and have both available to the command line when running your CI. Install Install nodejs from https://nodejs.org/en/ Install cspell Open cmd prompt with access to node and npm Run npm install -g cspell More cspell info: https://github.com/streetsidesoftware/cspell PyTool Scopes \u00b6 Scopes are how the PyTool ext_dep, path_env, and plugins are activated. Meaning that if an invocable process has a scope active then those ext_dep and path_env will be active. To allow easy integration of PyTools capabilities there are a few standard scopes. Scope Invocable Description global edk2_invocable++ - should be base_abstract_invocable Running an invocables global-win edk2_invocable++ Running on Microsoft Windows global-nix edk2_invocable++ Running on Linux based OS edk2-build This indicates that an invocable is building EDK2 based UEFI code cibuild set in .pytool/CISettings.py Suggested target for edk2 continuous integration builds. Tools used for CiBuilds can use this scope. Example: asl compiler Future investments \u00b6 PatchCheck tests as plugins MacOS/xcode support Clang/LLVM support Visual Studio AARCH64 and ARM support BaseTools C tools CI/PR and binary release process BaseTools Python tools CI/PR process Host based unit testing Extensible private/closed source platform reporting Platform builds, validation UEFI SCTs Other automation","title":"pytool"},{"location":"dyn/mu_silicon_arm_tiano/pytool/Readme/#edk2-continuous-integration","text":"","title":"Edk2 Continuous Integration"},{"location":"dyn/mu_silicon_arm_tiano/pytool/Readme/#basic-status","text":"Package Windows VS2019 (IA32/X64) Ubuntu GCC (IA32/X64/ARM/AARCH64) Known Issues ArmPkg ArmPlatformPkg ArmVirtPkg For more detailed status look at the test results of the latest CI run on the repo readme.","title":"Basic Status"},{"location":"dyn/mu_silicon_arm_tiano/pytool/Readme/#background","text":"This Continuous integration and testing infrastructure leverages the TianoCore EDKII Tools PIP modules: library and extensions (with repos located here and here ). The primary execution flows can be found in the .azurepipelines / Windows-VS2019.yml and .azurepipelines / Ubuntu-GCC5.yml files. These YAML files are consumed by the Azure Dev Ops Build Pipeline and dictate what server resources should be used, how they should be configured, and what processes should be run on them. An overview of this schema can be found here . Inspection of these files reveals the EDKII Tools commands that make up the primary processes for the CI build: 'stuart_setup', 'stuart_update', and 'stuart_ci_build'. These commands come from the EDKII Tools PIP modules and are configured as described below. More documentation on the tools can be found here and here .","title":"Background"},{"location":"dyn/mu_silicon_arm_tiano/pytool/Readme/#configuration","text":"Configuration of the CI process consists of (in order of precedence): command-line arguments passed in via the Pipeline YAML a per-package configuration file (e.g. <package-name>.ci.yaml ) that is detected by the CI system in EDKII Tools. a global configuration Python module (e.g. CISetting.py ) passed in via the command-line The global configuration file is described in this readme from the EDKII Tools documentation. This configuration is written as a Python module so that decisions can be made dynamically based on command line parameters and codebase state. The per-package configuration file can override most settings in the global configuration file, but is not dynamic. This file can be used to skip or customize tests that may be incompatible with a specific package. Each test generally requires per package configuration which comes from this file.","title":"Configuration"},{"location":"dyn/mu_silicon_arm_tiano/pytool/Readme/#running-ci-locally","text":"The EDKII Tools environment (and by extension the ci) is designed to support easily and consistantly running locally and in a cloud ci environment. To do that a few steps should be followed. Details of EDKII Tools can be found in the docs folder here","title":"Running CI locally"},{"location":"dyn/mu_silicon_arm_tiano/pytool/Readme/#prerequisets","text":"A supported toolchain (others might work but this is what is tested and validated) Windows 10: VS 2017 or VS 2019 Windows SDK (for rc) Windows WDK (for capsules) Ubuntu 16.04 GCC5 Easy to add more but this is the current state Python 3.7.x or newer on path git on path Recommended to setup and activate a python virtual environment Install the requirements pip install --upgrade pip-requirements.txt","title":"Prerequisets"},{"location":"dyn/mu_silicon_arm_tiano/pytool/Readme/#running-ci","text":"clone your edk2 repo Activate your python virtual environment in cmd window Get code dependencies (done only when submodules change) stuart_setup -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> Update other dependencies (done more often) stuart_update -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> Run CI build (--help will give you options) stuart_ci_build -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> -p : To build only certain packages use a CSV list -a : To run only certain architectures use a CSV list -t : To run only tests related to certain targets use a CSV list By default all tests are opted in. Then given a package.ci.yaml file those tests can be configured for a package. Finally setting the check to the value skip will skip that plugin. Examples: CompilerPlugin=skip skip the build test GuidCheck=skip skip the Guid check SpellCheck=skip skip the spell checker etc Detailed reports and logs per package are captured in the Build directory","title":"Running CI"},{"location":"dyn/mu_silicon_arm_tiano/pytool/Readme/#current-pytool-test-capabilities","text":"All CI tests are instances of EDKII Tools plugins. Documentation on the plugin system can be found here and here . Upon invocation, each plugin will be passed the path to the current package under test and a dictionary containing its targeted configuration, as assembled from the command line, per-package configuration, and global configuration. Note: CI plugins are considered unique from build plugins and helper plugins, even though some CI plugins may execute steps of a build. In the example, these plugins live alongside the code under test (in the .pytool / Plugin directory), but may be moved to the 'edk2-test' repo if that location makes more sense for the community.","title":"Current PyTool Test Capabilities"},{"location":"dyn/mu_silicon_arm_tiano/pytool/Readme/#module-inclusion-test-dsccompletecheck","text":"This test scans all available modules (via INF files) and compares them to the package-level DSC file for the package each module is contained within. The test considers it an error if any module does not appear in the Components section of at least one package-level DSC (indicating that it would not be built if the package were built).","title":"Module Inclusion Test - DscCompleteCheck"},{"location":"dyn/mu_silicon_arm_tiano/pytool/Readme/#code-compilation-test-compilerplugin","text":"Once the Module Inclusion Test has verified that all modules would be built if all package-level DSCs were built, the Code Compilation Test simply runs through and builds every package-level DSC on every toolchain and for every architecture that is supported. Any module that fails to build is considered an error.","title":"Code Compilation Test - CompilerPlugin"},{"location":"dyn/mu_silicon_arm_tiano/pytool/Readme/#guid-uniqueness-test-guidcheck","text":"This test works on the collection of all packages rather than an individual package. It looks at all FILE_GUIDs and GUIDs declared in DEC files and ensures that they are unique for the codebase. This prevents, for example, accidental duplication of GUIDs when using an existing INF as a template for a new module.","title":"GUID Uniqueness Test - GuidCheck"},{"location":"dyn/mu_silicon_arm_tiano/pytool/Readme/#cross-package-dependency-test-dependencycheck","text":"This test compares the list of all packages used in INFs files for a given package against a list of \"allowed dependencies\" in plugin configuration for that package. Any module that depends on a disallowed package will cause a test failure.","title":"Cross-Package Dependency Test - DependencyCheck"},{"location":"dyn/mu_silicon_arm_tiano/pytool/Readme/#library-declaration-test-libraryclasscheck","text":"This test scans at all library header files found in the Library folders in all of the package's declared include directories and ensures that all files have a matching LibraryClass declaration in the DEC file for the package. Any missing declarations will cause a failure.","title":"Library Declaration Test - LibraryClassCheck"},{"location":"dyn/mu_silicon_arm_tiano/pytool/Readme/#invalid-character-test-charencodingcheck","text":"This test scans all files in a package to make sure that there are no invalid Unicode characters that may cause build errors in some character sets/localizations.","title":"Invalid Character Test - CharEncodingCheck"},{"location":"dyn/mu_silicon_arm_tiano/pytool/Readme/#spell-checking-cspell","text":"This test runs a spell checker on all files within the package. This is done using the NodeJs cspell tool. For details check .pytool / Plugin / SpellCheck . For this plugin to run during ci you must install nodejs and cspell and have both available to the command line when running your CI. Install Install nodejs from https://nodejs.org/en/ Install cspell Open cmd prompt with access to node and npm Run npm install -g cspell More cspell info: https://github.com/streetsidesoftware/cspell","title":"Spell Checking - cspell"},{"location":"dyn/mu_silicon_arm_tiano/pytool/Readme/#pytool-scopes","text":"Scopes are how the PyTool ext_dep, path_env, and plugins are activated. Meaning that if an invocable process has a scope active then those ext_dep and path_env will be active. To allow easy integration of PyTools capabilities there are a few standard scopes. Scope Invocable Description global edk2_invocable++ - should be base_abstract_invocable Running an invocables global-win edk2_invocable++ Running on Microsoft Windows global-nix edk2_invocable++ Running on Linux based OS edk2-build This indicates that an invocable is building EDK2 based UEFI code cibuild set in .pytool/CISettings.py Suggested target for edk2 continuous integration builds. Tools used for CiBuilds can use this scope. Example: asl compiler","title":"PyTool Scopes"},{"location":"dyn/mu_silicon_arm_tiano/pytool/Readme/#future-investments","text":"PatchCheck tests as plugins MacOS/xcode support Clang/LLVM support Visual Studio AARCH64 and ARM support BaseTools C tools CI/PR and binary release process BaseTools Python tools CI/PR process Host based unit testing Extensible private/closed source platform reporting Platform builds, validation UEFI SCTs Other automation","title":"Future investments"},{"location":"dyn/mu_silicon_intel_tiano/RepoDetails/","text":"Project Mu Basecore Repository \u00b6 Git Details Repository Url: https://github.com/Microsoft/mu_silicon_intel_tiano.git Branch: release/201911 Commit: d32f2b488c08deaabd6ee8acc14d09694db283f5 Commit Date: 2019-12-07 07:56:01 +0000 This repository contains Project Mu code based on TianoCore edk2 code for Intel silicon features and Intel based platforms. More Info \u00b6 This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments. Issues \u00b6 Please open any issues in the Project Mu GitHub tracker. More Details Contributing Code or Docs \u00b6 Please follow the general Project Mu Pull Request process. More Details Code Requirements Doc Requirements Builds \u00b6 pip install --upgrade -r requirements.txt mu_build -c corebuild.mu.json Copyright & License \u00b6 Copyright (C) Microsoft Corporation SPDX-License-Identifier: BSD-2-Clause-Patent Upstream License (TianoCore) \u00b6 Copyright \u00a9 2019, TianoCore and contributors. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Subject to the terms and conditions of this license, each copyright holder and contributor hereby grants to those receiving rights under this license a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except for failure to satisfy the conditions of this license) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer this software, where such license applies only to those patent claims, already acquired or hereafter acquired, licensable by such copyright holder or contributor that are necessarily infringed by: (a) their Contribution(s) (the licensed copyrights of copyright holders and non-copyrightable additions of contributors, in source or binary form) alone; or (b) combination of their Contribution(s) with the work of authorship to which such Contribution(s) was added by such copyright holder or contributor, if, at the time the Contribution is added, such addition causes such combination to be necessarily infringed. The patent license shall not apply to any other combinations which include the Contribution. Except as expressly stated above, no rights or licenses from any copyright holder or contributor is granted under this license, whether expressly, by implication, estoppel or otherwise. DISCLAIMER THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"Repo Details"},{"location":"dyn/mu_silicon_intel_tiano/RepoDetails/#project-mu-basecore-repository","text":"Git Details Repository Url: https://github.com/Microsoft/mu_silicon_intel_tiano.git Branch: release/201911 Commit: d32f2b488c08deaabd6ee8acc14d09694db283f5 Commit Date: 2019-12-07 07:56:01 +0000 This repository contains Project Mu code based on TianoCore edk2 code for Intel silicon features and Intel based platforms.","title":"Project Mu Basecore Repository"},{"location":"dyn/mu_silicon_intel_tiano/RepoDetails/#more-info","text":"This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.","title":"More Info"},{"location":"dyn/mu_silicon_intel_tiano/RepoDetails/#issues","text":"Please open any issues in the Project Mu GitHub tracker. More Details","title":"Issues"},{"location":"dyn/mu_silicon_intel_tiano/RepoDetails/#contributing-code-or-docs","text":"Please follow the general Project Mu Pull Request process. More Details Code Requirements Doc Requirements","title":"Contributing Code or Docs"},{"location":"dyn/mu_silicon_intel_tiano/RepoDetails/#builds","text":"pip install --upgrade -r requirements.txt mu_build -c corebuild.mu.json","title":"Builds"},{"location":"dyn/mu_silicon_intel_tiano/RepoDetails/#copyright-license","text":"Copyright (C) Microsoft Corporation SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright &amp; License"},{"location":"dyn/mu_silicon_intel_tiano/RepoDetails/#upstream-license-tianocore","text":"Copyright \u00a9 2019, TianoCore and contributors. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Subject to the terms and conditions of this license, each copyright holder and contributor hereby grants to those receiving rights under this license a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except for failure to satisfy the conditions of this license) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer this software, where such license applies only to those patent claims, already acquired or hereafter acquired, licensable by such copyright holder or contributor that are necessarily infringed by: (a) their Contribution(s) (the licensed copyrights of copyright holders and non-copyrightable additions of contributors, in source or binary form) alone; or (b) combination of their Contribution(s) with the work of authorship to which such Contribution(s) was added by such copyright holder or contributor, if, at the time the Contribution is added, such addition causes such combination to be necessarily infringed. The patent license shall not apply to any other combinations which include the Contribution. Except as expressly stated above, no rights or licenses from any copyright holder or contributor is granted under this license, whether expressly, by implication, estoppel or otherwise. DISCLAIMER THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"Upstream License (TianoCore)"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Readme/","text":"IntelFsp2Pkg \u00b6 This package provides the component to create an FSP binary. Source Repository: https://github.com/tianocore/edk2/tree/master/IntelFsp2Pkg A whitepaper to describe the IntelFsp2Pkg: https://firmware.intel.com/sites/default/files/A_Tour_Beyond_BIOS_Creating_the_Intel_Firmware_Support_Package_with_the_EFI_Developer_Kit_II_%28FSP2.0%29.pdf","title":"Readme"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Readme/#intelfsp2pkg","text":"This package provides the component to create an FSP binary. Source Repository: https://github.com/tianocore/edk2/tree/master/IntelFsp2Pkg A whitepaper to describe the IntelFsp2Pkg: https://firmware.intel.com/sites/default/files/A_Tour_Beyond_BIOS_Creating_the_Intel_Firmware_Support_Package_with_the_EFI_Developer_Kit_II_%28FSP2.0%29.pdf","title":"IntelFsp2Pkg"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/","text":"Name \u00b6 GenCfgOpt.py The python script that generates UPD text ( .txt ) files for the compiler, header files for the UPD regions, and generates a Boot Settings File ( BSF ), all from an EDK II Platform Description ( DSC ) file. Synopsis \u00b6 GenCfgOpt UPDTXT PlatformDscFile BuildFvDir [ TxtOutFile ] [ -D Macros ] GenCfgOpt HEADER PlatformDscFile BuildFvDir [ InputHFile ] [ -D Macros ] GenCfgOpt GENBSF PlatformDscFile BuildFvDir BsfOutFile [ -D Macros ] Description \u00b6 GenCfgOpt.py is a script that generates configuration options from an EDK II Platform Description (DSC) file. It has three functions. It produces a .txt file that is used by the compiler that summarizes the UPD section in the DSC file. It generates header files for the UPD regions. It generates a Boot Settings File (BSF) that can be used by the Binary Configuration Tool (BCT) to provide a graphical user interface for manipulating settings in the UPD regions. The GenCfgOpt.py script generates important files that are vital parts of your build process. The UPDTXT and HEADER use cases must be done before the 'build' command; the GENBSF use case may be done at any time. The following sections explain the three use cases. 1. GenCfgOpt.py UPDTXT \u00b6 The UPDTXT option creates a text file with all the UPD entries, offsets, size in bytes, and values. GenCfgOpt reads this information from the [PcdsDynamicVpd.Upd] section of the project's DSC file. The DSC file allows you to specify offsets and sizes for each entry, opening up the possibility of introducing gaps between entries. GenCfgOpt fills in these gaps with UPD entries that have the generic names UnusedUpdSpaceN where N begins with 0 and increments. The command signature for UPDTXT is: GenCfgOpt UPDTXT PlatformDscFile BuildFvDir [ TxtOutFile ] [ -D Macros ] PlatformDscFile must be the location of the DSC file for the platform you're building. BuildFvDir is the location where the binary will be stored. The optional TxtOutFile is a name and location for the output of GenCfgOpt . The default name and location is the <UPD_TOOL_GUID>.txt in the directory specified by BuildFvDir . The macro UPD_TOOL_GUID must be defined in the DSC file or in the optional Macros arguments. Each optional macro argument must follow the form ?D <MACRO_NAME>=<VALUE> . GenCfgOpt checks to see if the UPD txt file has already been created and will only re-create it if the DSC was modified after it was created. 2. GenCfgOpt.py HEADER \u00b6 The HEADER option creates header files in the build folder. Both header files define the _UPD_DATA_REGION data structures in FspUpd.h, FsptUpd.h, FspmUpd.h and FspsUpd.h. In these header files any undefined elements of structures will be added as ReservedUpdSpaceN beginning with N=0. The command signature for HEADER is GenCfgOpt HEADER PlatformDscFile BuildFvDir [ InputHFile ] [ -D Macros ] PlatformDscFile and BuildFvDir are described in the previous section. The optional InputHFile is a header file that may contain data definitions that are used by variables in the UPD regions. This header file must contain the special keywords !EXPORT EXTERNAL_BOOTLOADER_STRUCT_BEGIN and !EXPORT EXTERNAL_BOOTLOADER_STRUCT_END in comments. Everything between these two keywords will be included in the generated header file. The mechanism to specify whether a variable appears as ReservedUpdSpaceN in the FspUpd.h header file is in special commands that appear in the comments of the DSC file. The special commands begin with !HDR , for header. The following table summarizes the two command options. HEADER \u00b6 Use the HEADER command to hide specific variables in the public header file. In your project DSC file, use !HDR HEADER:{OFF} at the beginning of the section you wish to hide and !HDR HEADER:{ON} at the end. STRUCT \u00b6 The STRUCT command allows you to specify a specific data type for a variable. You can specify a pointer to a data struct, for example. You define the data structure in the InputHFile between !EXPORT EXTERNAL_BOOTLOADER_STRUCT_BEGIN and !EXPORT EXTERNAL_BOOTLOADER_STRUCT_END . Example: \u00b6 !HDR STRUCT:{MY_DATA_STRUCT*} You then define MY_DATA_STRUCT in InputHFile . EMBED \u00b6 The EMBED command allows you to put one or more UPD data into a specify data structure. You can utilize it as a group of UPD for example. You must specify a start and an end for the specify data structure. Example: \u00b6 !HDR EMBED:{MY_DATA_STRUCT:MyDataStructure:START} gTokenSpaceGuid.Upd1 | 0x0020 | 0x01 | 0x00 gTokenSpaceGuid.Upd2 | 0x0021 | 0x01 | 0x00 !HDR EMBED:{MY_DATA_STRUCT:MyDataStructure:END} gTokenSpaceGuid.UpdN | 0x0022 | 0x01 | 0x00 Result: \u00b6 typedef struct { /** Offset 0x0020 **/ UINT8 Upd1 ; /** Offset 0x0021 **/ UINT8 Upd2 ; /** Offset 0x0022 **/ UINT8 UpdN ; } MY_DATA_STRUCT ; typedef struct _UPD_DATA_REGION { ... /** Offset 0x0020 **/ MY_DATA_STRUCT MyDataStruct ; ... } UPD_DATA_REGION ; 3. GenCfgOpt .py GENBSF \u00b6 The GENBSF option generates a BSF from the UPD entries in a package's DSC file. It does this by parsing special commands found in the comments of the DSC file. They roughly match the keywords that define the different sections of the BSF. The command signature for GENBSF is GenCfgOpt GENBSF PlatformDscFile BuildFvDir BsfOutFile [-D Macros] In this case, the BsfOutFile parameter is required; it should be the relative path to where the BSF should be stored. Every BSF command in the DSC file begins with !BSF or @Bsf . The following table summarizes the options that come after !BSF or @Bsf : BSF Commands Description \u00b6 PAGES \u00b6 PAGES maps abbreviations to friendly-text descriptions of the pages in a BSF. Example: \u00b6 !BSF PAGES:{PG1:?Page 1?, PG2:?Page 2?} or @Bsf PAGES:{PG1:?Page 1?, PG2:?Page 2?} PAGE \u00b6 This marks the beginning of a page. Use the abbreviation specified in PAGES command. Example: \u00b6 !BSF PAGE:{PG1} or @Bsf PAGE:{PG1} All the entries that come after this command are assumed to be on that page, until the next PAGE command FIND \u00b6 FIND maps to the BSF Find command. It will be placed in the StructDef region of the BSF and should come at the beginning of the UPD sections of the DSC, immediately before the signatures that mark the beginning of these sections. The content should be the plain-text equivalent of the signature. The signature is usually 8 characters. Example: \u00b6 !BSF FIND:{PROJSIG1} or @Bsf FIND:{PROJSIG1} BLOCK \u00b6 The BLOCK command maps to the BeginInfoBlock section of the BSF. There are two elements: a version number and a plain-text description. Example: \u00b6 !BSF BLOCK:{NAME:\"My platform name\", VER:\"0.1\"} or @Bsf BLOCK:{NAME:\"My platform name\", VER:\"0.1\"} NAME \u00b6 NAME gives a plain-text for a variable. This is the text label that will appear next to the control in BCT . Example: \u00b6 !BSF NAME:{Variable 0} or @Bsf NAME:{Variable 0} If the !BSF NAME or @Bsf NAME command does not appear before an entry in the UPD region of the DSC file, then that entry will not appear in the BSF. TYPE \u00b6 The TYPE command is used either by itself or with the NAME command. It is usually used by itself when defining an EditNum field for the BSF. You specify the type of data in the second parameter and the range of valid values in the third. Example: \u00b6 !BSF TYPE:{EditNum, HEX, (0x00,0xFF)} or @Bsf TYPE:{EditNum, HEX, (0x00,0xFF)} TYPE appears on the same line as the NAME command when using a combo-box. Example: \u00b6 !BSF NAME:{Variable 1} TYPE:{Combo} or @Bsf NAME:{Variable 1} TYPE:{Combo} There is a special None type that puts the variable in the StructDef region of the BSF, but doesn't put it in any Page section. This makes the variable visible to BCT, but not to the end user. HELP \u00b6 The HELP command defines what will appear in the help text for each control in BCT. Example: \u00b6 !BSF HELP:{Enable/disable LAN controller.} or @Bsf HELP:{Enable/disable LAN controller.} OPTION \u00b6 The OPTION command allows you to custom-define combo boxes and map integer or hex values to friendly-text options. Example: \u00b6 !BSF OPTION:{0:IDE, 1:AHCI, 2:RAID} !BSF OPTION:{0x00:0 MB, 0x01:32 MB, 0x02:64 MB} or @Bsf OPTION:{0:IDE, 1:AHCI, 2:RAID} @Bsf OPTION:{0x00:0 MB, 0x01:32 MB, 0x02:64 MB} FIELD \u00b6 The FIELD command can be used to define a section of a consolidated PCD such that the PCD will be displayed in several fields via BCT interface instead of one long entry. Example: \u00b6 !BSF FIELD:{PcdDRAMSpeed:1} or @Bsf FIELD:{PcdDRAMSpeed:1} ORDER \u00b6 The ORDER command can be used to adjust the display order for the BSF items. By default the order value for a BSF item is assigned to be the UPD item (Offset * 256) . It can be overridden by declaring ORDER command using format ORDER: {HexMajor.HexMinor} . In this case the order value will be (HexMajor*256+HexMinor) . The item order value will be used as the sort key during the BSF item display. Example: \u00b6 !BSF ORDER:{0x0040.01} or @Bsf ORDER:{0x0040.01} For OPTION and HELP commands, it allows to split the contents into multiple lines by adding multiple OPTION and HELP command lines. The lines except for the very first line need to start with + in the content to tell the tool to append this string to the previous one. For example, the statement !BSF OPTION:{0x00:0 MB, 0x01:32 MB, 0x02:64 MB} is equivalent to: !BSF OPTION:{0x00:0 MB, 0x01:32 MB,} !BSF OPTION:{+ 0x02:64 MB} or @Bsf OPTION:{0x00:0 MB, 0x01:32 MB, 0x02:64 MB} is equivalent to: @Bsf OPTION:{0x00:0 MB, 0x01:32 MB,} @Bsf OPTION:{+ 0x02:64 MB} The NAME , OPTION , TYPE , and HELP commands can all appear on the same line following the !BSF or @Bsf keyword or they may appear on separate lines to improve readability. There are four alternative ways to replace current BSF commands. 1. # @Prompt \u00b6 An alternative way replacing NAME gives a plain-text for a variable. This is the text label that will appear next to the control in BCT. Example: \u00b6 # @Prompt Variable 0 The above example can replace the two methods as below. !BSF NAME:{Variable 0} or @Bsf NAME:{Variable 0} If the # @Prompt command does not appear before an entry in the UPD region of the DSC file, then that entry will not appear in the BSF. 2. ## \u00b6 An alternative way replacing HELP command defines what will appear in the help text for each control in BCT. Example: \u00b6 ## Enable/disable LAN controller. The above example can replace the two methods as below. !BSF HELP:{Enable/disable LAN controller.} or @Bsf HELP:{Enable/disable LAN controller.} 3. # @ValidList \u00b6 An alternative way replacing OPTION command allows you to custom-define combo boxes and map integer or hex values to friendly-text options. Example: \u00b6 ``` # @ValidList 0x80000003 | 0, 1, 2 | IDE, AHCI, RAID Error Code | Options | Descriptions The above example can replace the two methods as below . ```! BSF OPTION :{ 0 : IDE , 1 : AHCI , 2 : RAID }``` or ``` @Bsf OPTION :{ 0 : IDE , 1 : AHCI , 2 : RAID }``` ### 4. ```# @ValidRange ``` An alternative way replace ** EditNum ** field for the BSF . ### ##Example : ```# @ValidRange 0x80000001 | 0x0 ? 0xFF Error Code | Range The above example can replace the two methods as below. !BSF TYPE:{EditNum, HEX, (0x00,0xFF)} or @Bsf TYPE:{EditNum, HEX, (0x00,0xFF)}","title":"Gen Cfg Opt User Manual"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#name","text":"GenCfgOpt.py The python script that generates UPD text ( .txt ) files for the compiler, header files for the UPD regions, and generates a Boot Settings File ( BSF ), all from an EDK II Platform Description ( DSC ) file.","title":"Name"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#synopsis","text":"GenCfgOpt UPDTXT PlatformDscFile BuildFvDir [ TxtOutFile ] [ -D Macros ] GenCfgOpt HEADER PlatformDscFile BuildFvDir [ InputHFile ] [ -D Macros ] GenCfgOpt GENBSF PlatformDscFile BuildFvDir BsfOutFile [ -D Macros ]","title":"Synopsis"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#description","text":"GenCfgOpt.py is a script that generates configuration options from an EDK II Platform Description (DSC) file. It has three functions. It produces a .txt file that is used by the compiler that summarizes the UPD section in the DSC file. It generates header files for the UPD regions. It generates a Boot Settings File (BSF) that can be used by the Binary Configuration Tool (BCT) to provide a graphical user interface for manipulating settings in the UPD regions. The GenCfgOpt.py script generates important files that are vital parts of your build process. The UPDTXT and HEADER use cases must be done before the 'build' command; the GENBSF use case may be done at any time. The following sections explain the three use cases.","title":"Description"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#1-gencfgoptpy-updtxt","text":"The UPDTXT option creates a text file with all the UPD entries, offsets, size in bytes, and values. GenCfgOpt reads this information from the [PcdsDynamicVpd.Upd] section of the project's DSC file. The DSC file allows you to specify offsets and sizes for each entry, opening up the possibility of introducing gaps between entries. GenCfgOpt fills in these gaps with UPD entries that have the generic names UnusedUpdSpaceN where N begins with 0 and increments. The command signature for UPDTXT is: GenCfgOpt UPDTXT PlatformDscFile BuildFvDir [ TxtOutFile ] [ -D Macros ] PlatformDscFile must be the location of the DSC file for the platform you're building. BuildFvDir is the location where the binary will be stored. The optional TxtOutFile is a name and location for the output of GenCfgOpt . The default name and location is the <UPD_TOOL_GUID>.txt in the directory specified by BuildFvDir . The macro UPD_TOOL_GUID must be defined in the DSC file or in the optional Macros arguments. Each optional macro argument must follow the form ?D <MACRO_NAME>=<VALUE> . GenCfgOpt checks to see if the UPD txt file has already been created and will only re-create it if the DSC was modified after it was created.","title":"1. GenCfgOpt.py UPDTXT"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#2-gencfgoptpy-header","text":"The HEADER option creates header files in the build folder. Both header files define the _UPD_DATA_REGION data structures in FspUpd.h, FsptUpd.h, FspmUpd.h and FspsUpd.h. In these header files any undefined elements of structures will be added as ReservedUpdSpaceN beginning with N=0. The command signature for HEADER is GenCfgOpt HEADER PlatformDscFile BuildFvDir [ InputHFile ] [ -D Macros ] PlatformDscFile and BuildFvDir are described in the previous section. The optional InputHFile is a header file that may contain data definitions that are used by variables in the UPD regions. This header file must contain the special keywords !EXPORT EXTERNAL_BOOTLOADER_STRUCT_BEGIN and !EXPORT EXTERNAL_BOOTLOADER_STRUCT_END in comments. Everything between these two keywords will be included in the generated header file. The mechanism to specify whether a variable appears as ReservedUpdSpaceN in the FspUpd.h header file is in special commands that appear in the comments of the DSC file. The special commands begin with !HDR , for header. The following table summarizes the two command options.","title":"2. GenCfgOpt.py HEADER"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#header","text":"Use the HEADER command to hide specific variables in the public header file. In your project DSC file, use !HDR HEADER:{OFF} at the beginning of the section you wish to hide and !HDR HEADER:{ON} at the end.","title":"HEADER"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#struct","text":"The STRUCT command allows you to specify a specific data type for a variable. You can specify a pointer to a data struct, for example. You define the data structure in the InputHFile between !EXPORT EXTERNAL_BOOTLOADER_STRUCT_BEGIN and !EXPORT EXTERNAL_BOOTLOADER_STRUCT_END .","title":"STRUCT"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#example","text":"!HDR STRUCT:{MY_DATA_STRUCT*} You then define MY_DATA_STRUCT in InputHFile .","title":"Example:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#embed","text":"The EMBED command allows you to put one or more UPD data into a specify data structure. You can utilize it as a group of UPD for example. You must specify a start and an end for the specify data structure.","title":"EMBED"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#example_1","text":"!HDR EMBED:{MY_DATA_STRUCT:MyDataStructure:START} gTokenSpaceGuid.Upd1 | 0x0020 | 0x01 | 0x00 gTokenSpaceGuid.Upd2 | 0x0021 | 0x01 | 0x00 !HDR EMBED:{MY_DATA_STRUCT:MyDataStructure:END} gTokenSpaceGuid.UpdN | 0x0022 | 0x01 | 0x00","title":"Example:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#result","text":"typedef struct { /** Offset 0x0020 **/ UINT8 Upd1 ; /** Offset 0x0021 **/ UINT8 Upd2 ; /** Offset 0x0022 **/ UINT8 UpdN ; } MY_DATA_STRUCT ; typedef struct _UPD_DATA_REGION { ... /** Offset 0x0020 **/ MY_DATA_STRUCT MyDataStruct ; ... } UPD_DATA_REGION ;","title":"Result:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#3-gencfgopt-py-genbsf","text":"The GENBSF option generates a BSF from the UPD entries in a package's DSC file. It does this by parsing special commands found in the comments of the DSC file. They roughly match the keywords that define the different sections of the BSF. The command signature for GENBSF is GenCfgOpt GENBSF PlatformDscFile BuildFvDir BsfOutFile [-D Macros] In this case, the BsfOutFile parameter is required; it should be the relative path to where the BSF should be stored. Every BSF command in the DSC file begins with !BSF or @Bsf . The following table summarizes the options that come after !BSF or @Bsf :","title":"3. GenCfgOpt .py GENBSF"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#bsf-commands-description","text":"","title":"BSF Commands Description"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#pages","text":"PAGES maps abbreviations to friendly-text descriptions of the pages in a BSF.","title":"PAGES"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#example_2","text":"!BSF PAGES:{PG1:?Page 1?, PG2:?Page 2?} or @Bsf PAGES:{PG1:?Page 1?, PG2:?Page 2?}","title":"Example:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#page","text":"This marks the beginning of a page. Use the abbreviation specified in PAGES command.","title":"PAGE"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#example_3","text":"!BSF PAGE:{PG1} or @Bsf PAGE:{PG1} All the entries that come after this command are assumed to be on that page, until the next PAGE command","title":"Example:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#find","text":"FIND maps to the BSF Find command. It will be placed in the StructDef region of the BSF and should come at the beginning of the UPD sections of the DSC, immediately before the signatures that mark the beginning of these sections. The content should be the plain-text equivalent of the signature. The signature is usually 8 characters.","title":"FIND"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#example_4","text":"!BSF FIND:{PROJSIG1} or @Bsf FIND:{PROJSIG1}","title":"Example:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#block","text":"The BLOCK command maps to the BeginInfoBlock section of the BSF. There are two elements: a version number and a plain-text description.","title":"BLOCK"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#example_5","text":"!BSF BLOCK:{NAME:\"My platform name\", VER:\"0.1\"} or @Bsf BLOCK:{NAME:\"My platform name\", VER:\"0.1\"}","title":"Example:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#name_1","text":"NAME gives a plain-text for a variable. This is the text label that will appear next to the control in BCT .","title":"NAME"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#example_6","text":"!BSF NAME:{Variable 0} or @Bsf NAME:{Variable 0} If the !BSF NAME or @Bsf NAME command does not appear before an entry in the UPD region of the DSC file, then that entry will not appear in the BSF.","title":"Example:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#type","text":"The TYPE command is used either by itself or with the NAME command. It is usually used by itself when defining an EditNum field for the BSF. You specify the type of data in the second parameter and the range of valid values in the third.","title":"TYPE"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#example_7","text":"!BSF TYPE:{EditNum, HEX, (0x00,0xFF)} or @Bsf TYPE:{EditNum, HEX, (0x00,0xFF)} TYPE appears on the same line as the NAME command when using a combo-box.","title":"Example:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#example_8","text":"!BSF NAME:{Variable 1} TYPE:{Combo} or @Bsf NAME:{Variable 1} TYPE:{Combo} There is a special None type that puts the variable in the StructDef region of the BSF, but doesn't put it in any Page section. This makes the variable visible to BCT, but not to the end user.","title":"Example:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#help","text":"The HELP command defines what will appear in the help text for each control in BCT.","title":"HELP"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#example_9","text":"!BSF HELP:{Enable/disable LAN controller.} or @Bsf HELP:{Enable/disable LAN controller.}","title":"Example:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#option","text":"The OPTION command allows you to custom-define combo boxes and map integer or hex values to friendly-text options.","title":"OPTION"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#example_10","text":"!BSF OPTION:{0:IDE, 1:AHCI, 2:RAID} !BSF OPTION:{0x00:0 MB, 0x01:32 MB, 0x02:64 MB} or @Bsf OPTION:{0:IDE, 1:AHCI, 2:RAID} @Bsf OPTION:{0x00:0 MB, 0x01:32 MB, 0x02:64 MB}","title":"Example:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#field","text":"The FIELD command can be used to define a section of a consolidated PCD such that the PCD will be displayed in several fields via BCT interface instead of one long entry.","title":"FIELD"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#example_11","text":"!BSF FIELD:{PcdDRAMSpeed:1} or @Bsf FIELD:{PcdDRAMSpeed:1}","title":"Example:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#order","text":"The ORDER command can be used to adjust the display order for the BSF items. By default the order value for a BSF item is assigned to be the UPD item (Offset * 256) . It can be overridden by declaring ORDER command using format ORDER: {HexMajor.HexMinor} . In this case the order value will be (HexMajor*256+HexMinor) . The item order value will be used as the sort key during the BSF item display.","title":"ORDER"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#example_12","text":"!BSF ORDER:{0x0040.01} or @Bsf ORDER:{0x0040.01} For OPTION and HELP commands, it allows to split the contents into multiple lines by adding multiple OPTION and HELP command lines. The lines except for the very first line need to start with + in the content to tell the tool to append this string to the previous one. For example, the statement !BSF OPTION:{0x00:0 MB, 0x01:32 MB, 0x02:64 MB} is equivalent to: !BSF OPTION:{0x00:0 MB, 0x01:32 MB,} !BSF OPTION:{+ 0x02:64 MB} or @Bsf OPTION:{0x00:0 MB, 0x01:32 MB, 0x02:64 MB} is equivalent to: @Bsf OPTION:{0x00:0 MB, 0x01:32 MB,} @Bsf OPTION:{+ 0x02:64 MB} The NAME , OPTION , TYPE , and HELP commands can all appear on the same line following the !BSF or @Bsf keyword or they may appear on separate lines to improve readability. There are four alternative ways to replace current BSF commands.","title":"Example:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#1-prompt","text":"An alternative way replacing NAME gives a plain-text for a variable. This is the text label that will appear next to the control in BCT.","title":"1. # @Prompt"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#example_13","text":"# @Prompt Variable 0 The above example can replace the two methods as below. !BSF NAME:{Variable 0} or @Bsf NAME:{Variable 0} If the # @Prompt command does not appear before an entry in the UPD region of the DSC file, then that entry will not appear in the BSF.","title":"Example:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#2","text":"An alternative way replacing HELP command defines what will appear in the help text for each control in BCT.","title":"2. ##"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#example_14","text":"## Enable/disable LAN controller. The above example can replace the two methods as below. !BSF HELP:{Enable/disable LAN controller.} or @Bsf HELP:{Enable/disable LAN controller.}","title":"Example:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#3-validlist","text":"An alternative way replacing OPTION command allows you to custom-define combo boxes and map integer or hex values to friendly-text options.","title":"3. # @ValidList"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/GenCfgOptUserManual/#example_15","text":"``` # @ValidList 0x80000003 | 0, 1, 2 | IDE, AHCI, RAID Error Code | Options | Descriptions The above example can replace the two methods as below . ```! BSF OPTION :{ 0 : IDE , 1 : AHCI , 2 : RAID }``` or ``` @Bsf OPTION :{ 0 : IDE , 1 : AHCI , 2 : RAID }``` ### 4. ```# @ValidRange ``` An alternative way replace ** EditNum ** field for the BSF . ### ##Example : ```# @ValidRange 0x80000001 | 0x0 ? 0xFF Error Code | Range The above example can replace the two methods as below. !BSF TYPE:{EditNum, HEX, (0x00,0xFF)} or @Bsf TYPE:{EditNum, HEX, (0x00,0xFF)}","title":"Example:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/PatchFvUserManual/","text":"Name \u00b6 PatchFv.py - The python script that patches the firmware volumes ( FV ) with in the flash device ( FD ) file post FSP build. Synopsis \u00b6 PatchFv FvBuildDir [FvFileBaseNames:]FdFileBaseNameToPatch [\"Offset, Value\"]+ | [\"Offset, Value, @Comment\"]+ | [\"Offset, Value, $Command\"]+ | [\"Offset, Value, $Command, @Comment\"]+ Description \u00b6 The PatchFv.py tool allows the developer to fix up FD images to follow the Intel FSP Architecture specification. It also makes the FD image relocatable. The tool is written in Python and uses Python 2.7 or later to run. Consider using the tool in a build script. FvBuildDir (Argument 1) \u00b6 This is the first argument that PatchFv.py requires. It is the build directory for all firmware volumes created during the FSP build. The path must be either an absolute path or a relevant path, relevant to the top level of the FSP tree. Example usage: \u00b6 Build\\YouPlatformFspPkg\\%BD_TARGET%_%VS_VERSION%%VS_X86%\\FV The example used contains Windows batch script %VARIABLES%. FvFileBaseNames (Argument 2: Optional Part 1) \u00b6 The firmware volume file base names ( FvFileBaseNames ) are the independent Fv?s that are to be patched within the FD. (0 or more in the form FVFILEBASENAME: ) The colon : is used for delimiting the single argument and must be appended to the end of each ( FvFileBaseNames ). Example usage: \u00b6 STAGE1 : STAGE2 : MANIFEST : YOURPLATFORM In the example STAGE1 is STAGE1.Fv in YOURPLATFORM.fd . FdFileNameToPatch (Argument 2: Mandatory Part 2) \u00b6 Firmware device file name to patch ( FdFileNameToPatch ) is the base name of the FD file that is to be patched. (1 only, in the form YOURPLATFORM ) Example usage: \u00b6 STAGE1 : STAGE2 : MANIFEST : YOURPLATFORM In the example YOURPLATFORM is from YOURPLATFORM.fd \"Offset, Value[, Command][, Comment]\" (Argument 3) \u00b6 The Offset can be a positive or negative number and represents where the Value to be patched is located within the FD. The Value is what will be written at the given Offset in the FD. Constants may be used for both offsets and values. Also, this argument handles expressions for both offsets and values using these operators: = - * & | ~ ( ) [ ] { } < > The entire argument includes the quote marks like in the example argument below: 0xFFFFFFC0, SomeCore:__EntryPoint - [0x000000F0],@SomeCore Entry Constants: \u00b6 Hexadecimal (use 0x as prefix) | Decimal Examples: \u00b6 Positive Hex Negative Hex Positive Decimal Negative Decimal 0x000000BC 0xFFFFFFA2 188 -94 ModuleName : FunctionName | ModuleName : GlobalVariableName ModuleGuid : Offset Operators: \u00b6 + Addition - Subtraction * Multiplication & Logical and | Logical or ~ Complement ( ) Evaluation control [ ] Get a DWord value at the specified offset expression from [ expr ] { } Convert an offset { expr } into an absolute address ( FSP_BASE + expr ) < > Convert absolute address < expr > into an image offset ( expr & FSP_SIZE ) Special Commands: \u00b6 Special commands must use the $ symbol as a prefix to the command itself. There is only one command available at this time. $COPY ? Copy a binary block from source to destination. Example: \u00b6 0x94, [PlatformInit:__gPcd_BinPatch_FvRecOffset] + 0x94, [0x98], $COPY, @Sync up 2nd FSP Header Comments: \u00b6 Comments are allowed in the Offset, Value [, Comment] argument. Comments must use the @ symbol as a prefix. The comment will output to the build window upon successful completion of patching along with the offset and value data.","title":"Patch Fv User Manual"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/PatchFvUserManual/#name","text":"PatchFv.py - The python script that patches the firmware volumes ( FV ) with in the flash device ( FD ) file post FSP build.","title":"Name"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/PatchFvUserManual/#synopsis","text":"PatchFv FvBuildDir [FvFileBaseNames:]FdFileBaseNameToPatch [\"Offset, Value\"]+ | [\"Offset, Value, @Comment\"]+ | [\"Offset, Value, $Command\"]+ | [\"Offset, Value, $Command, @Comment\"]+","title":"Synopsis"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/PatchFvUserManual/#description","text":"The PatchFv.py tool allows the developer to fix up FD images to follow the Intel FSP Architecture specification. It also makes the FD image relocatable. The tool is written in Python and uses Python 2.7 or later to run. Consider using the tool in a build script.","title":"Description"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/PatchFvUserManual/#fvbuilddir-argument-1","text":"This is the first argument that PatchFv.py requires. It is the build directory for all firmware volumes created during the FSP build. The path must be either an absolute path or a relevant path, relevant to the top level of the FSP tree.","title":"FvBuildDir (Argument 1)"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/PatchFvUserManual/#example-usage","text":"Build\\YouPlatformFspPkg\\%BD_TARGET%_%VS_VERSION%%VS_X86%\\FV The example used contains Windows batch script %VARIABLES%.","title":"Example usage:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/PatchFvUserManual/#fvfilebasenames-argument-2-optional-part-1","text":"The firmware volume file base names ( FvFileBaseNames ) are the independent Fv?s that are to be patched within the FD. (0 or more in the form FVFILEBASENAME: ) The colon : is used for delimiting the single argument and must be appended to the end of each ( FvFileBaseNames ).","title":"FvFileBaseNames (Argument 2: Optional Part 1)"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/PatchFvUserManual/#example-usage_1","text":"STAGE1 : STAGE2 : MANIFEST : YOURPLATFORM In the example STAGE1 is STAGE1.Fv in YOURPLATFORM.fd .","title":"Example usage:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/PatchFvUserManual/#fdfilenametopatch-argument-2-mandatory-part-2","text":"Firmware device file name to patch ( FdFileNameToPatch ) is the base name of the FD file that is to be patched. (1 only, in the form YOURPLATFORM )","title":"FdFileNameToPatch (Argument 2: Mandatory Part 2)"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/PatchFvUserManual/#example-usage_2","text":"STAGE1 : STAGE2 : MANIFEST : YOURPLATFORM In the example YOURPLATFORM is from YOURPLATFORM.fd","title":"Example usage:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/PatchFvUserManual/#offset-value-command-comment-argument-3","text":"The Offset can be a positive or negative number and represents where the Value to be patched is located within the FD. The Value is what will be written at the given Offset in the FD. Constants may be used for both offsets and values. Also, this argument handles expressions for both offsets and values using these operators: = - * & | ~ ( ) [ ] { } < > The entire argument includes the quote marks like in the example argument below: 0xFFFFFFC0, SomeCore:__EntryPoint - [0x000000F0],@SomeCore Entry","title":"\"Offset, Value[, Command][, Comment]\" (Argument 3)"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/PatchFvUserManual/#constants","text":"Hexadecimal (use 0x as prefix) | Decimal","title":"Constants:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/PatchFvUserManual/#examples","text":"Positive Hex Negative Hex Positive Decimal Negative Decimal 0x000000BC 0xFFFFFFA2 188 -94 ModuleName : FunctionName | ModuleName : GlobalVariableName ModuleGuid : Offset","title":"Examples:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/PatchFvUserManual/#operators","text":"+ Addition - Subtraction * Multiplication & Logical and | Logical or ~ Complement ( ) Evaluation control [ ] Get a DWord value at the specified offset expression from [ expr ] { } Convert an offset { expr } into an absolute address ( FSP_BASE + expr ) < > Convert absolute address < expr > into an image offset ( expr & FSP_SIZE )","title":"Operators:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/PatchFvUserManual/#special-commands","text":"Special commands must use the $ symbol as a prefix to the command itself. There is only one command available at this time. $COPY ? Copy a binary block from source to destination.","title":"Special Commands:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/PatchFvUserManual/#example","text":"0x94, [PlatformInit:__gPcd_BinPatch_FvRecOffset] + 0x94, [0x98], $COPY, @Sync up 2nd FSP Header","title":"Example:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/PatchFvUserManual/#comments","text":"Comments are allowed in the Offset, Value [, Comment] argument. Comments must use the @ symbol as a prefix. The comment will output to the build window upon successful completion of patching along with the offset and value data.","title":"Comments:"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/SplitFspBinUserManual/","text":"SplitFspBin.py is a python script to support some operations on Intel FSP 1.x/2.x image. \u00b6 It supports: Split Intel FSP 2.x image into individual FSP-T/M/S/O component Rebase Intel FSP 1.x/2.x components to different base addresses Generate Intel FSP 1.x/2.x C header file Display Intel FSP 1.x/2.x information header for each FSP component Split Intel FSP 2.x image \u00b6 FSP 1.x image is not supported by split command. To split individual FSP component in Intel FSP 2.x image, the following command can be used: python SplitFspBin.py split [-h] -f FSPBINARY [-o OUTPUTDIR] [-n NAMETEMPLATE] For example: python SplitFspBin.py split -f FSP.bin It will create FSP_T.bin, FSP_M.bin and FSP_S.bin in current directory. Rebase Intel FSP 1.x/2.x components \u00b6 To rebase one or multiple FSP components in Intel FSP 1.x/2.x image, the following command can be used: python SplitFspBin.py rebase [-h] -f FSPBINARY -c {t,m,s,o} [{t,m,s,o} ...] -b FSPBASE [FSPBASE ...] [-o OUTPUTDIR] [-n OUTPUTFILE] For example: python SplitFspBin.py rebase -f FSP.bin -c t -b 0xFFF00000 -n FSP_new.bin It will rebase FSP-T component inside FSP.bin to new base 0xFFF00000 and save the rebased Intel FSP 2.x image into file FSP_new.bin. For FSP 1.x image there is only one component in binary so above command also works for FSP 1.x image. python SplitFspBin.py rebase -f FSP.bin -c t m -b 0xFFF00000 0xFEF80000 -n FSP_new.bin It will rebase FSP-T and FSP-M components inside FSP.bin to new base 0xFFF00000 and 0xFEF80000 respectively, and save the rebased Intel FSP 2.x image into file FSP_new.bin file. Generate Intel FSP 1.x/2.x C header file \u00b6 To generate Intel FSP 1.x/2.x C header file, the following command can be used: Python SplitFspBin.py genhdr [-h] -f FSPBINARY [-o OUTPUTDIR] [-n HFILENAME] For example: python SplitFspBin.py genhdr -f FSP.bin -n FSP.h It will create the C header file FSP.h containing the image ID, revision, offset and size for each individual FSP component. Display Intel FSP 1.x/2.x information header \u00b6 To display Intel FSP 1.x/2.x information headers, the following command can be used: Python SplitFspBin.py info [-h] -f FSPBINARY For example: python SplitFspBin.py info -f FSP.bin It will print out the FSP information header for each FSP component.","title":"Split Fsp Bin User Manual"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/SplitFspBinUserManual/#splitfspbinpy-is-a-python-script-to-support-some-operations-on-intel-fsp-1x2x-image","text":"It supports: Split Intel FSP 2.x image into individual FSP-T/M/S/O component Rebase Intel FSP 1.x/2.x components to different base addresses Generate Intel FSP 1.x/2.x C header file Display Intel FSP 1.x/2.x information header for each FSP component","title":"SplitFspBin.py is a python script to support some operations on Intel FSP 1.x/2.x image."},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/SplitFspBinUserManual/#split-intel-fsp-2x-image","text":"FSP 1.x image is not supported by split command. To split individual FSP component in Intel FSP 2.x image, the following command can be used: python SplitFspBin.py split [-h] -f FSPBINARY [-o OUTPUTDIR] [-n NAMETEMPLATE] For example: python SplitFspBin.py split -f FSP.bin It will create FSP_T.bin, FSP_M.bin and FSP_S.bin in current directory.","title":"Split Intel FSP 2.x image"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/SplitFspBinUserManual/#rebase-intel-fsp-1x2x-components","text":"To rebase one or multiple FSP components in Intel FSP 1.x/2.x image, the following command can be used: python SplitFspBin.py rebase [-h] -f FSPBINARY -c {t,m,s,o} [{t,m,s,o} ...] -b FSPBASE [FSPBASE ...] [-o OUTPUTDIR] [-n OUTPUTFILE] For example: python SplitFspBin.py rebase -f FSP.bin -c t -b 0xFFF00000 -n FSP_new.bin It will rebase FSP-T component inside FSP.bin to new base 0xFFF00000 and save the rebased Intel FSP 2.x image into file FSP_new.bin. For FSP 1.x image there is only one component in binary so above command also works for FSP 1.x image. python SplitFspBin.py rebase -f FSP.bin -c t m -b 0xFFF00000 0xFEF80000 -n FSP_new.bin It will rebase FSP-T and FSP-M components inside FSP.bin to new base 0xFFF00000 and 0xFEF80000 respectively, and save the rebased Intel FSP 2.x image into file FSP_new.bin file.","title":"Rebase Intel FSP 1.x/2.x components"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/SplitFspBinUserManual/#generate-intel-fsp-1x2x-c-header-file","text":"To generate Intel FSP 1.x/2.x C header file, the following command can be used: Python SplitFspBin.py genhdr [-h] -f FSPBINARY [-o OUTPUTDIR] [-n HFILENAME] For example: python SplitFspBin.py genhdr -f FSP.bin -n FSP.h It will create the C header file FSP.h containing the image ID, revision, offset and size for each individual FSP component.","title":"Generate Intel FSP 1.x/2.x C header file"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2Pkg/Tools/UserManuals/SplitFspBinUserManual/#display-intel-fsp-1x2x-information-header","text":"To display Intel FSP 1.x/2.x information headers, the following command can be used: Python SplitFspBin.py info [-h] -f FSPBINARY For example: python SplitFspBin.py info -f FSP.bin It will print out the FSP information header for each FSP component.","title":"Display Intel FSP 1.x/2.x information header"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2WrapperPkg/Readme/","text":"IntelFsp2WrapperPkg \u00b6 This package provides the component to use an FSP binary. Source Repository: https://github.com/tianocore/edk2/tree/master/IntelFsp2WrapperPkg A whitepaper to describe the IntelFsp2WrapperPkg: https://firmware.intel.com/sites/default/files/A_Tour_Beyond_BIOS_Using_the_Intel_Firmware_Support_Package_with_the_EFI_Developer_Kit_II_%28FSP2.0%29.pdf","title":"Modules"},{"location":"dyn/mu_silicon_intel_tiano/IntelFsp2WrapperPkg/Readme/#intelfsp2wrapperpkg","text":"This package provides the component to use an FSP binary. Source Repository: https://github.com/tianocore/edk2/tree/master/IntelFsp2WrapperPkg A whitepaper to describe the IntelFsp2WrapperPkg: https://firmware.intel.com/sites/default/files/A_Tour_Beyond_BIOS_Using_the_Intel_Firmware_Support_Package_with_the_EFI_Developer_Kit_II_%28FSP2.0%29.pdf","title":"IntelFsp2WrapperPkg"},{"location":"dyn/mu_silicon_intel_tiano/IntelSiliconPkg/Feature/Capsule/MicrocodeCapsulePdb/Readme/","text":"How to generate Microcode FMP from Microcode PDB file \u00b6 1) Copy directory UefiCpuPkg/Feature/Capsule/MicrocodeUpdatePdb to <Your Platform Package>/MicrocodeUpdatePdb . 2) Uncomment and update FILE DATA statement in <Your Platform Package>/MicrocodeUpdatePdb/MicrocodeCapsulePdb.fdf with path to a Microcode PDB file. The PDB file can placed in <Your Platform Package>/MicrocodeUpdatePdb or any other path. FILE DATA = <your Microcode PDB file path> Uncomment and update PLATFORM_NAME , FLASH_DEFINITION , OUTPUT_DIRECTORY section in <Your Platform Package>/MicrocodeUpdatePdb/MicrocodeCapsulePdb.dsc with . PLATFORM_NAME = <Your Platform Package> FLASH_DEFINITION = <Your Platform Package>/MicrocodeCapsulePdb/MicrocodeCapsulePdb.fdf OUTPUT_DIRECTORY = Build/<Your Platform Package> 3) Use EDK II build tools to generate the Microcode FMP Capsule build -p <Your Platform Package>/MicrocodeCapsulePdb/MicrocodeCapsulePdb.dsc 4) The Microcode FMP Capsule is generated at $(WORKSPACE)/$(OUTPUT_DIRECTORY)/$(TARGET)_$(TOOL_CHAIN_TAG)/FV/MicrocodeCapsule.Cap","title":"Microcode Capsule Pdb"},{"location":"dyn/mu_silicon_intel_tiano/IntelSiliconPkg/Feature/Capsule/MicrocodeCapsulePdb/Readme/#how-to-generate-microcode-fmp-from-microcode-pdb-file","text":"1) Copy directory UefiCpuPkg/Feature/Capsule/MicrocodeUpdatePdb to <Your Platform Package>/MicrocodeUpdatePdb . 2) Uncomment and update FILE DATA statement in <Your Platform Package>/MicrocodeUpdatePdb/MicrocodeCapsulePdb.fdf with path to a Microcode PDB file. The PDB file can placed in <Your Platform Package>/MicrocodeUpdatePdb or any other path. FILE DATA = <your Microcode PDB file path> Uncomment and update PLATFORM_NAME , FLASH_DEFINITION , OUTPUT_DIRECTORY section in <Your Platform Package>/MicrocodeUpdatePdb/MicrocodeCapsulePdb.dsc with . PLATFORM_NAME = <Your Platform Package> FLASH_DEFINITION = <Your Platform Package>/MicrocodeCapsulePdb/MicrocodeCapsulePdb.fdf OUTPUT_DIRECTORY = Build/<Your Platform Package> 3) Use EDK II build tools to generate the Microcode FMP Capsule build -p <Your Platform Package>/MicrocodeCapsulePdb/MicrocodeCapsulePdb.dsc 4) The Microcode FMP Capsule is generated at $(WORKSPACE)/$(OUTPUT_DIRECTORY)/$(TARGET)_$(TOOL_CHAIN_TAG)/FV/MicrocodeCapsule.Cap","title":"How to generate Microcode FMP from Microcode PDB file"},{"location":"dyn/mu_silicon_intel_tiano/IntelSiliconPkg/Feature/Capsule/MicrocodeCapsuleTxt/Readme/","text":"How to generate Microcode FMP from Microcode TXT file \u00b6 1) Copy directory UefiCpuPkg/Feature/Capsule/MicrocodeUpdateTxt to <Your Platform Package>/MicrocodeUpdateTxt 2) Copy microcode TXT file to <Your Platform Package>/MicrocodeUpdateTxt/Microcode 3) Uncomment and update statement in [ Sources ] section of <Your Platform Package>/MicrocodeUpdateTxt/Microcode/Microcode.inf with name of Microcode TXT file copied in previous step. [Sources] <Your Microcode TXT file> Uncomment and update FILE DATA statement in <Your Platform Package>/MicrocodeUpdateTxt/MicrocodeCapsuleTxt.fdf with path to a Microcode MCB file. The MCB file is placed in $(WORKSPACE)/$(OUTPUT_DIRECTORY)/$(TARGET)_$(TOOL_CHAIN_TAG)/IA32/<Your Platform Package>/MicrocodeUpdateTxt/Microcode/Microcode/OUTPUT/ . FILE DATA = <your Microcode MCB file path> Uncomment and update PLATFORM_NAME , FLASH_DEFINITION , OUTPUT_DIRECTORY section in <Your Platform Package>/MicrocodeUpdateTxt/MicrocodeCapsuleTxt.dsc with . PLATFORM_NAME = <Your Platform Package> FLASH_DEFINITION = <Your Platform Package>/MicrocodeCapsuleTxt/MicrocodeCapsuleTxt.fdf OUTPUT_DIRECTORY = Build/<Your Platform Package> Uncomment and update statement in Components section of <Your Platform Package>/MicrocodeUpdateTxt/MicrocodeCapsuleTxt.dsc with path to a Microcode INF file. [Components] <Your Microcode INF file> 4) Use EDK II build tools to generate the Microcode FMP Capsule build -p <Your Platform Package>/MicrocodeCapsuleTxt/MicrocodeCapsuleTxt.dsc 5) The generated Microcode FMP Capsule is found at $(WORKSPACE)/$(OUTPUT_DIRECTORY)/$(TARGET)_$(TOOL_CHAIN_TAG)/FV/MicrocodeCapsule.Cap","title":"Microcode Capsule Txt"},{"location":"dyn/mu_silicon_intel_tiano/IntelSiliconPkg/Feature/Capsule/MicrocodeCapsuleTxt/Readme/#how-to-generate-microcode-fmp-from-microcode-txt-file","text":"1) Copy directory UefiCpuPkg/Feature/Capsule/MicrocodeUpdateTxt to <Your Platform Package>/MicrocodeUpdateTxt 2) Copy microcode TXT file to <Your Platform Package>/MicrocodeUpdateTxt/Microcode 3) Uncomment and update statement in [ Sources ] section of <Your Platform Package>/MicrocodeUpdateTxt/Microcode/Microcode.inf with name of Microcode TXT file copied in previous step. [Sources] <Your Microcode TXT file> Uncomment and update FILE DATA statement in <Your Platform Package>/MicrocodeUpdateTxt/MicrocodeCapsuleTxt.fdf with path to a Microcode MCB file. The MCB file is placed in $(WORKSPACE)/$(OUTPUT_DIRECTORY)/$(TARGET)_$(TOOL_CHAIN_TAG)/IA32/<Your Platform Package>/MicrocodeUpdateTxt/Microcode/Microcode/OUTPUT/ . FILE DATA = <your Microcode MCB file path> Uncomment and update PLATFORM_NAME , FLASH_DEFINITION , OUTPUT_DIRECTORY section in <Your Platform Package>/MicrocodeUpdateTxt/MicrocodeCapsuleTxt.dsc with . PLATFORM_NAME = <Your Platform Package> FLASH_DEFINITION = <Your Platform Package>/MicrocodeCapsuleTxt/MicrocodeCapsuleTxt.fdf OUTPUT_DIRECTORY = Build/<Your Platform Package> Uncomment and update statement in Components section of <Your Platform Package>/MicrocodeUpdateTxt/MicrocodeCapsuleTxt.dsc with path to a Microcode INF file. [Components] <Your Microcode INF file> 4) Use EDK II build tools to generate the Microcode FMP Capsule build -p <Your Platform Package>/MicrocodeCapsuleTxt/MicrocodeCapsuleTxt.dsc 5) The generated Microcode FMP Capsule is found at $(WORKSPACE)/$(OUTPUT_DIRECTORY)/$(TARGET)_$(TOOL_CHAIN_TAG)/FV/MicrocodeCapsule.Cap","title":"How to generate Microcode FMP from Microcode TXT file"},{"location":"dyn/mu_silicon_intel_tiano/pytool/Readme/","text":"Edk2 Continuous Integration \u00b6 Basic Status \u00b6 Package Windows VS2019 (IA32/X64) Ubuntu GCC (IA32/X64/ARM/AARCH64) Known Issues IntelFsp2Pkg IntelFsp2WrapperPkg For more detailed status look at the test results of the latest CI run on the repo readme. Background \u00b6 This Continuous integration and testing infrastructure leverages the TianoCore EDKII Tools PIP modules: library and extensions (with repos located here and here ). The primary execution flows can be found in the .azurepipelines / Windows-VS2019.yml and .azurepipelines / Ubuntu-GCC5.yml files. These YAML files are consumed by the Azure Dev Ops Build Pipeline and dictate what server resources should be used, how they should be configured, and what processes should be run on them. An overview of this schema can be found here . Inspection of these files reveals the EDKII Tools commands that make up the primary processes for the CI build: 'stuart_setup', 'stuart_update', and 'stuart_ci_build'. These commands come from the EDKII Tools PIP modules and are configured as described below. More documentation on the tools can be found here and here . Configuration \u00b6 Configuration of the CI process consists of (in order of precedence): command-line arguments passed in via the Pipeline YAML a per-package configuration file (e.g. <package-name>.ci.yaml ) that is detected by the CI system in EDKII Tools. a global configuration Python module (e.g. CISetting.py ) passed in via the command-line The global configuration file is described in this readme from the EDKII Tools documentation. This configuration is written as a Python module so that decisions can be made dynamically based on command line parameters and codebase state. The per-package configuration file can override most settings in the global configuration file, but is not dynamic. This file can be used to skip or customize tests that may be incompatible with a specific package. Each test generally requires per package configuration which comes from this file. Running CI locally \u00b6 The EDKII Tools environment (and by extension the ci) is designed to support easily and consistently running locally and in a cloud ci environment. To do that a few steps should be followed. Details of EDKII Tools can be found in the docs folder here Prerequisites \u00b6 A supported toolchain (others might work but this is what is tested and validated) Windows 10: VS 2017 or VS 2019 Windows SDK (for rc) Windows WDK (for capsules) Ubuntu 16.04 GCC5 Easy to add more but this is the current state Python 3.7.x or newer on path git on path Recommended to setup and activate a python virtual environment Install the requirements pip install --upgrade pip-requirements.txt Running CI \u00b6 clone your edk2 repo Activate your python virtual environment in cmd window Get code dependencies (done only when submodules change) stuart_setup -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> Update other dependencies (done more often) stuart_update -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> Run CI build (--help will give you options) stuart_ci_build -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> -p : To build only certain packages use a CSV list -a : To run only certain architectures use a CSV list -t : To run only tests related to certain targets use a CSV list By default all tests are opted in. Then given a package.ci.yaml file those tests can be configured for a package. Finally setting the check to the value skip will skip that plugin. Examples: CompilerPlugin=skip skip the build test GuidCheck=skip skip the Guid check SpellCheck=skip skip the spell checker etc Detailed reports and logs per package are captured in the Build directory Current PyTool Test Capabilities \u00b6 All CI tests are instances of EDKII Tools plugins. Documentation on the plugin system can be found here and here . Upon invocation, each plugin will be passed the path to the current package under test and a dictionary containing its targeted configuration, as assembled from the command line, per-package configuration, and global configuration. Note: CI plugins are considered unique from build plugins and helper plugins, even though some CI plugins may execute steps of a build. In the example, these plugins live alongside the code under test (in the .pytool / Plugin directory), but may be moved to the 'edk2-test' repo if that location makes more sense for the community. Module Inclusion Test - DscCompleteCheck \u00b6 This test scans all available modules (via INF files) and compares them to the package-level DSC file for the package each module is contained within. The test considers it an error if any module does not appear in the Components section of at least one package-level DSC (indicating that it would not be built if the package were built). Code Compilation Test - CompilerPlugin \u00b6 Once the Module Inclusion Test has verified that all modules would be built if all package-level DSCs were built, the Code Compilation Test simply runs through and builds every package-level DSC on every toolchain and for every architecture that is supported. Any module that fails to build is considered an error. GUID Uniqueness Test - GuidCheck \u00b6 This test works on the collection of all packages rather than an individual package. It looks at all FILE_GUIDs and GUIDs declared in DEC files and ensures that they are unique for the codebase. This prevents, for example, accidental duplication of GUIDs when using an existing INF as a template for a new module. Cross-Package Dependency Test - DependencyCheck \u00b6 This test compares the list of all packages used in INFs files for a given package against a list of \"allowed dependencies\" in plugin configuration for that package. Any module that depends on a disallowed package will cause a test failure. Library Declaration Test - LibraryClassCheck \u00b6 This test scans at all library header files found in the Library folders in all of the package's declared include directories and ensures that all files have a matching LibraryClass declaration in the DEC file for the package. Any missing declarations will cause a failure. Invalid Character Test - CharEncodingCheck \u00b6 This test scans all files in a package to make sure that there are no invalid Unicode characters that may cause build errors in some character sets/localizations. Spell Checking - cspell \u00b6 This test runs a spell checker on all files within the package. This is done using the NodeJs cspell tool. For details check .pytool / Plugin / SpellCheck . For this plugin to run during ci you must install nodejs and cspell and have both available to the command line when running your CI. Install Install nodejs from https://nodejs.org/en/ Install cspell Open cmd prompt with access to node and npm Run npm install -g cspell More cspell info: https://github.com/streetsidesoftware/cspell PyTool Scopes \u00b6 Scopes are how the PyTool ext_dep, path_env, and plugins are activated. Meaning that if an invocable process has a scope active then those ext_dep and path_env will be active. To allow easy integration of PyTools capabilities there are a few standard scopes. Scope Invocable Description global edk2_invocable++ - should be base_abstract_invocable Running an invocables global-win edk2_invocable++ Running on Microsoft Windows global-nix edk2_invocable++ Running on Linux based OS edk2-build This indicates that an invocable is building EDK2 based UEFI code cibuild set in .pytool/CISettings.py Suggested target for edk2 continuous integration builds. Tools used for CiBuilds can use this scope. Example: asl compiler Future investments \u00b6 PatchCheck tests as plugins MacOS/xcode support Clang/LLVM support Visual Studio AARCH64 and ARM support BaseTools C tools CI/PR and binary release process BaseTools Python tools CI/PR process Host based unit testing Extensible private/closed source platform reporting Platform builds, validation UEFI SCTs Other automation","title":"pytool"},{"location":"dyn/mu_silicon_intel_tiano/pytool/Readme/#edk2-continuous-integration","text":"","title":"Edk2 Continuous Integration"},{"location":"dyn/mu_silicon_intel_tiano/pytool/Readme/#basic-status","text":"Package Windows VS2019 (IA32/X64) Ubuntu GCC (IA32/X64/ARM/AARCH64) Known Issues IntelFsp2Pkg IntelFsp2WrapperPkg For more detailed status look at the test results of the latest CI run on the repo readme.","title":"Basic Status"},{"location":"dyn/mu_silicon_intel_tiano/pytool/Readme/#background","text":"This Continuous integration and testing infrastructure leverages the TianoCore EDKII Tools PIP modules: library and extensions (with repos located here and here ). The primary execution flows can be found in the .azurepipelines / Windows-VS2019.yml and .azurepipelines / Ubuntu-GCC5.yml files. These YAML files are consumed by the Azure Dev Ops Build Pipeline and dictate what server resources should be used, how they should be configured, and what processes should be run on them. An overview of this schema can be found here . Inspection of these files reveals the EDKII Tools commands that make up the primary processes for the CI build: 'stuart_setup', 'stuart_update', and 'stuart_ci_build'. These commands come from the EDKII Tools PIP modules and are configured as described below. More documentation on the tools can be found here and here .","title":"Background"},{"location":"dyn/mu_silicon_intel_tiano/pytool/Readme/#configuration","text":"Configuration of the CI process consists of (in order of precedence): command-line arguments passed in via the Pipeline YAML a per-package configuration file (e.g. <package-name>.ci.yaml ) that is detected by the CI system in EDKII Tools. a global configuration Python module (e.g. CISetting.py ) passed in via the command-line The global configuration file is described in this readme from the EDKII Tools documentation. This configuration is written as a Python module so that decisions can be made dynamically based on command line parameters and codebase state. The per-package configuration file can override most settings in the global configuration file, but is not dynamic. This file can be used to skip or customize tests that may be incompatible with a specific package. Each test generally requires per package configuration which comes from this file.","title":"Configuration"},{"location":"dyn/mu_silicon_intel_tiano/pytool/Readme/#running-ci-locally","text":"The EDKII Tools environment (and by extension the ci) is designed to support easily and consistently running locally and in a cloud ci environment. To do that a few steps should be followed. Details of EDKII Tools can be found in the docs folder here","title":"Running CI locally"},{"location":"dyn/mu_silicon_intel_tiano/pytool/Readme/#prerequisites","text":"A supported toolchain (others might work but this is what is tested and validated) Windows 10: VS 2017 or VS 2019 Windows SDK (for rc) Windows WDK (for capsules) Ubuntu 16.04 GCC5 Easy to add more but this is the current state Python 3.7.x or newer on path git on path Recommended to setup and activate a python virtual environment Install the requirements pip install --upgrade pip-requirements.txt","title":"Prerequisites"},{"location":"dyn/mu_silicon_intel_tiano/pytool/Readme/#running-ci","text":"clone your edk2 repo Activate your python virtual environment in cmd window Get code dependencies (done only when submodules change) stuart_setup -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> Update other dependencies (done more often) stuart_update -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> Run CI build (--help will give you options) stuart_ci_build -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> -p : To build only certain packages use a CSV list -a : To run only certain architectures use a CSV list -t : To run only tests related to certain targets use a CSV list By default all tests are opted in. Then given a package.ci.yaml file those tests can be configured for a package. Finally setting the check to the value skip will skip that plugin. Examples: CompilerPlugin=skip skip the build test GuidCheck=skip skip the Guid check SpellCheck=skip skip the spell checker etc Detailed reports and logs per package are captured in the Build directory","title":"Running CI"},{"location":"dyn/mu_silicon_intel_tiano/pytool/Readme/#current-pytool-test-capabilities","text":"All CI tests are instances of EDKII Tools plugins. Documentation on the plugin system can be found here and here . Upon invocation, each plugin will be passed the path to the current package under test and a dictionary containing its targeted configuration, as assembled from the command line, per-package configuration, and global configuration. Note: CI plugins are considered unique from build plugins and helper plugins, even though some CI plugins may execute steps of a build. In the example, these plugins live alongside the code under test (in the .pytool / Plugin directory), but may be moved to the 'edk2-test' repo if that location makes more sense for the community.","title":"Current PyTool Test Capabilities"},{"location":"dyn/mu_silicon_intel_tiano/pytool/Readme/#module-inclusion-test-dsccompletecheck","text":"This test scans all available modules (via INF files) and compares them to the package-level DSC file for the package each module is contained within. The test considers it an error if any module does not appear in the Components section of at least one package-level DSC (indicating that it would not be built if the package were built).","title":"Module Inclusion Test - DscCompleteCheck"},{"location":"dyn/mu_silicon_intel_tiano/pytool/Readme/#code-compilation-test-compilerplugin","text":"Once the Module Inclusion Test has verified that all modules would be built if all package-level DSCs were built, the Code Compilation Test simply runs through and builds every package-level DSC on every toolchain and for every architecture that is supported. Any module that fails to build is considered an error.","title":"Code Compilation Test - CompilerPlugin"},{"location":"dyn/mu_silicon_intel_tiano/pytool/Readme/#guid-uniqueness-test-guidcheck","text":"This test works on the collection of all packages rather than an individual package. It looks at all FILE_GUIDs and GUIDs declared in DEC files and ensures that they are unique for the codebase. This prevents, for example, accidental duplication of GUIDs when using an existing INF as a template for a new module.","title":"GUID Uniqueness Test - GuidCheck"},{"location":"dyn/mu_silicon_intel_tiano/pytool/Readme/#cross-package-dependency-test-dependencycheck","text":"This test compares the list of all packages used in INFs files for a given package against a list of \"allowed dependencies\" in plugin configuration for that package. Any module that depends on a disallowed package will cause a test failure.","title":"Cross-Package Dependency Test - DependencyCheck"},{"location":"dyn/mu_silicon_intel_tiano/pytool/Readme/#library-declaration-test-libraryclasscheck","text":"This test scans at all library header files found in the Library folders in all of the package's declared include directories and ensures that all files have a matching LibraryClass declaration in the DEC file for the package. Any missing declarations will cause a failure.","title":"Library Declaration Test - LibraryClassCheck"},{"location":"dyn/mu_silicon_intel_tiano/pytool/Readme/#invalid-character-test-charencodingcheck","text":"This test scans all files in a package to make sure that there are no invalid Unicode characters that may cause build errors in some character sets/localizations.","title":"Invalid Character Test - CharEncodingCheck"},{"location":"dyn/mu_silicon_intel_tiano/pytool/Readme/#spell-checking-cspell","text":"This test runs a spell checker on all files within the package. This is done using the NodeJs cspell tool. For details check .pytool / Plugin / SpellCheck . For this plugin to run during ci you must install nodejs and cspell and have both available to the command line when running your CI. Install Install nodejs from https://nodejs.org/en/ Install cspell Open cmd prompt with access to node and npm Run npm install -g cspell More cspell info: https://github.com/streetsidesoftware/cspell","title":"Spell Checking - cspell"},{"location":"dyn/mu_silicon_intel_tiano/pytool/Readme/#pytool-scopes","text":"Scopes are how the PyTool ext_dep, path_env, and plugins are activated. Meaning that if an invocable process has a scope active then those ext_dep and path_env will be active. To allow easy integration of PyTools capabilities there are a few standard scopes. Scope Invocable Description global edk2_invocable++ - should be base_abstract_invocable Running an invocables global-win edk2_invocable++ Running on Microsoft Windows global-nix edk2_invocable++ Running on Linux based OS edk2-build This indicates that an invocable is building EDK2 based UEFI code cibuild set in .pytool/CISettings.py Suggested target for edk2 continuous integration builds. Tools used for CiBuilds can use this scope. Example: asl compiler","title":"PyTool Scopes"},{"location":"dyn/mu_silicon_intel_tiano/pytool/Readme/#future-investments","text":"PatchCheck tests as plugins MacOS/xcode support Clang/LLVM support Visual Studio AARCH64 and ARM support BaseTools C tools CI/PR and binary release process BaseTools Python tools CI/PR process Host based unit testing Extensible private/closed source platform reporting Platform builds, validation UEFI SCTs Other automation","title":"Future investments"},{"location":"dyn/mu_tiano_plus/RepoDetails/","text":"Project Mu Tiano Plus \u00b6 Git Details Repository Url: https://github.com/Microsoft/mu_tiano_plus.git Branch: release/201911 Commit: 1ef85caa930bf4c0e94fc1734558119c2c30c67b Commit Date: 2019-12-07 07:59:31 +0000 About \u00b6 This repo contains Project Mu common code that should only take Basecore as a dependency and be applicable to almost any FW project. The modules in this repo were taken with minimal modification from TianoCore. For full documentation, please see the Project Mu Docs site . This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments. Copyright & License \u00b6 Copyright \u00a9 Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent Upstream License (TianoCore) \u00b6 Copyright \u00a9 2019, TianoCore and contributors. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Subject to the terms and conditions of this license, each copyright holder and contributor hereby grants to those receiving rights under this license a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except for failure to satisfy the conditions of this license) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer this software, where such license applies only to those patent claims, already acquired or hereafter acquired, licensable by such copyright holder or contributor that are necessarily infringed by: (a) their Contribution(s) (the licensed copyrights of copyright holders and non-copyrightable additions of contributors, in source or binary form) alone; or (b) combination of their Contribution(s) with the work of authorship to which such Contribution(s) was added by such copyright holder or contributor, if, at the time the Contribution is added, such addition causes such combination to be necessarily infringed. The patent license shall not apply to any other combinations which include the Contribution. Except as expressly stated above, no rights or licenses from any copyright holder or contributor is granted under this license, whether expressly, by implication, estoppel or otherwise. DISCLAIMER THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"Repo Details"},{"location":"dyn/mu_tiano_plus/RepoDetails/#project-mu-tiano-plus","text":"Git Details Repository Url: https://github.com/Microsoft/mu_tiano_plus.git Branch: release/201911 Commit: 1ef85caa930bf4c0e94fc1734558119c2c30c67b Commit Date: 2019-12-07 07:59:31 +0000","title":"Project Mu Tiano Plus"},{"location":"dyn/mu_tiano_plus/RepoDetails/#about","text":"This repo contains Project Mu common code that should only take Basecore as a dependency and be applicable to almost any FW project. The modules in this repo were taken with minimal modification from TianoCore. For full documentation, please see the Project Mu Docs site . This project has adopted the Microsoft Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.","title":"About"},{"location":"dyn/mu_tiano_plus/RepoDetails/#copyright-license","text":"Copyright \u00a9 Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright &amp; License"},{"location":"dyn/mu_tiano_plus/RepoDetails/#upstream-license-tianocore","text":"Copyright \u00a9 2019, TianoCore and contributors. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Subject to the terms and conditions of this license, each copyright holder and contributor hereby grants to those receiving rights under this license a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except for failure to satisfy the conditions of this license) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer this software, where such license applies only to those patent claims, already acquired or hereafter acquired, licensable by such copyright holder or contributor that are necessarily infringed by: (a) their Contribution(s) (the licensed copyrights of copyright holders and non-copyrightable additions of contributors, in source or binary form) alone; or (b) combination of their Contribution(s) with the work of authorship to which such Contribution(s) was added by such copyright holder or contributor, if, at the time the Contribution is added, such addition causes such combination to be necessarily infringed. The patent license shall not apply to any other combinations which include the Contribution. Except as expressly stated above, no rights or licenses from any copyright holder or contributor is granted under this license, whether expressly, by implication, estoppel or otherwise. DISCLAIMER THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"Upstream License (TianoCore)"},{"location":"dyn/mu_tiano_plus/CryptoPkg/UnitTests/VerifyPkcs7EkuUnitTestApp/readme/","text":"Verify Pkcs7 Enhanced Key Usage Unit Test App \u00b6 Testing the verification of Enhanced Key Usages in PKCS7 formatted signatures. About \u00b6 This application tests the VerifyEKUsInPkcs7Signature() function in BaseCryptLib. VerifyPkcs7EkuUnitTestApp \u00b6 This application consumes the UnitTestLib and implements various test cases for the verification of Enhanced Key Usages (EKUs) contained in the end-entity (leaf) signing certificate. TestEKUCerts folder \u00b6 This folder contains information on how the test certificate chain and various leaf certificates were created. It also contains all the files required, and the certificates used in the unit-tests. ChainCreationInstructions.txt \u00b6 This file contains the instructions for creating your own chain, and how to use certreq.exe to create chains. INF files These INF files specify the properties of the certificate to be created. PFX files These Personal Information Exchange (PFX) files contain the certificate, and private key associated with the certificate. The passwords for these files are specified in ChainCreationInstructions.txt. P7B files These are the detached PKCS7 formatted test signatures used in the unit-tests. CER files These are the certificates that were created. SignFirmwareWithEKUs.cmd This script calls signtool.exe to sign a file with various leaf certificates. Note signtool.exe must be in your PATH. Use the Visual Studio command line with the Windows Software Development kit installed. To run SignFirmwareWithEKUs.cmd, install the PFX files. The passwords are in the ChainCreationInstructions.txt file. Copyright \u00b6 Copyright \u00a9 Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Verify Pkcs7Eku Unit Test App"},{"location":"dyn/mu_tiano_plus/CryptoPkg/UnitTests/VerifyPkcs7EkuUnitTestApp/readme/#verify-pkcs7-enhanced-key-usage-unit-test-app","text":"Testing the verification of Enhanced Key Usages in PKCS7 formatted signatures.","title":"Verify Pkcs7 Enhanced Key Usage Unit Test App"},{"location":"dyn/mu_tiano_plus/CryptoPkg/UnitTests/VerifyPkcs7EkuUnitTestApp/readme/#about","text":"This application tests the VerifyEKUsInPkcs7Signature() function in BaseCryptLib.","title":"About"},{"location":"dyn/mu_tiano_plus/CryptoPkg/UnitTests/VerifyPkcs7EkuUnitTestApp/readme/#verifypkcs7ekuunittestapp","text":"This application consumes the UnitTestLib and implements various test cases for the verification of Enhanced Key Usages (EKUs) contained in the end-entity (leaf) signing certificate.","title":"VerifyPkcs7EkuUnitTestApp"},{"location":"dyn/mu_tiano_plus/CryptoPkg/UnitTests/VerifyPkcs7EkuUnitTestApp/readme/#testekucerts-folder","text":"This folder contains information on how the test certificate chain and various leaf certificates were created. It also contains all the files required, and the certificates used in the unit-tests.","title":"TestEKUCerts folder"},{"location":"dyn/mu_tiano_plus/CryptoPkg/UnitTests/VerifyPkcs7EkuUnitTestApp/readme/#chaincreationinstructionstxt","text":"This file contains the instructions for creating your own chain, and how to use certreq.exe to create chains. INF files These INF files specify the properties of the certificate to be created. PFX files These Personal Information Exchange (PFX) files contain the certificate, and private key associated with the certificate. The passwords for these files are specified in ChainCreationInstructions.txt. P7B files These are the detached PKCS7 formatted test signatures used in the unit-tests. CER files These are the certificates that were created. SignFirmwareWithEKUs.cmd This script calls signtool.exe to sign a file with various leaf certificates. Note signtool.exe must be in your PATH. Use the Visual Studio command line with the Windows Software Development kit installed. To run SignFirmwareWithEKUs.cmd, install the PFX files. The passwords are in the ChainCreationInstructions.txt file.","title":"ChainCreationInstructions.txt"},{"location":"dyn/mu_tiano_plus/CryptoPkg/UnitTests/VerifyPkcs7EkuUnitTestApp/readme/#copyright","text":"Copyright \u00a9 Microsoft Corporation. All rights reserved. SPDX-License-Identifier: BSD-2-Clause-Patent","title":"Copyright"},{"location":"dyn/mu_tiano_plus/EmbeddedPkg/Scripts/LauterbachT32/Readme/","text":"DXE Phase Debug \u00b6 Update the memsize variable in EfiLoadDxe.cmm for the actual amount of memory available in your system. Allow your system to boot to the point that the DXE core is initialized (so that the System Table and Debug Information table is present in memory) and execute this script (using the toolbar button or 'do EfiLoadDxe' from the command area). It will scan memory for the debug info table and load modules in it. SEC/PEI Phase Debug \u00b6 There is no way to autodetect where these images reside so you must pass an address for the memory-mapped Firmware Volume containing these images. To do this, enter 'do EfiLoadFv ' where is the base address for the firmware volume containing the SEC or PEI code. To be more efficient you may want to create a script that calls this, like MyBoardLoadSec.cmm which contains the call to EfiLoadFv. You can them map this script to a T32 menu or toolbar button for quick access.","title":"Lauterbach T32"},{"location":"dyn/mu_tiano_plus/EmbeddedPkg/Scripts/LauterbachT32/Readme/#dxe-phase-debug","text":"Update the memsize variable in EfiLoadDxe.cmm for the actual amount of memory available in your system. Allow your system to boot to the point that the DXE core is initialized (so that the System Table and Debug Information table is present in memory) and execute this script (using the toolbar button or 'do EfiLoadDxe' from the command area). It will scan memory for the debug info table and load modules in it.","title":"DXE Phase Debug"},{"location":"dyn/mu_tiano_plus/EmbeddedPkg/Scripts/LauterbachT32/Readme/#secpei-phase-debug","text":"There is no way to autodetect where these images reside so you must pass an address for the memory-mapped Firmware Volume containing these images. To do this, enter 'do EfiLoadFv ' where is the base address for the firmware volume containing the SEC or PEI code. To be more efficient you may want to create a script that calls this, like MyBoardLoadSec.cmm which contains the call to EfiLoadFv. You can them map this script to a T32 menu or toolbar button for quick access.","title":"SEC/PEI Phase Debug"},{"location":"dyn/mu_tiano_plus/FmpDevicePkg/FmpDxe/","text":"Introduction \u00b6 TODO: Give a short introduction of FmpDxe driver. Getting Started \u00b6 TODO: Guide users through getting code up and running on their own system. Ideally including: 1. Installation process 2. Software dependencies 3. Latest releases 4. API references Build and Test \u00b6 TODO: Describe and show how to build code and run the tests. Design Changes \u00b6 Date: 10/07/2019 Description/Rationale: Capsule update is the process where each OEM has a lot of interest. Especially when there is capsule update failure, it is helpful to gather more information of the failure. With existed implementation, SetImage routine from FmpDxe driver, which is the most heavy lifting function call during capsule update, will only populate LastAttemptStatus with limited pre-defined error codes which could be consumed/inspected by the OS when it recovers and boots. Thus our proposal is to update the SetImage routine and leverage the LAST_ATTEMPT_STATUS_ERROR_UNSUCCESSFUL_VENDOR_RANGE range newly defined in UEFI Spec 2.8 Section 23.4, so that the error code will provide better granularity when viewing capsule update failure from OS device manager. Changes: A few error codes (128 total) are reserved from LAST_ATTEMPT_STATUS_ERROR_UNSUCCESSFUL_VENDOR_RANGE range for FmpDxe driver usage, which ranges from thermal and power API failure to capsule payload header check failure. Furthermore, an output pointer of the LastAttemptStatus is added as an input argument for FmpDeviceSetImage function in FmpDeviceLib to allow platform to provide their own platform specific error codes (SPI write failure, SVN checking failure, and more). Impact/Mitigation: The italic text above will cause a breaking change for all the FmpDeviceLib instances due to API change. This is to provide a better visibility for OEMs to decode the capsule update failure more efficiently. Each FmpDeviceLib should change to new API definition and populate proper LastAttemptStatus value when applicable.","title":"Fmp Dxe"},{"location":"dyn/mu_tiano_plus/FmpDevicePkg/FmpDxe/#introduction","text":"TODO: Give a short introduction of FmpDxe driver.","title":"Introduction"},{"location":"dyn/mu_tiano_plus/FmpDevicePkg/FmpDxe/#getting-started","text":"TODO: Guide users through getting code up and running on their own system. Ideally including: 1. Installation process 2. Software dependencies 3. Latest releases 4. API references","title":"Getting Started"},{"location":"dyn/mu_tiano_plus/FmpDevicePkg/FmpDxe/#build-and-test","text":"TODO: Describe and show how to build code and run the tests.","title":"Build and Test"},{"location":"dyn/mu_tiano_plus/FmpDevicePkg/FmpDxe/#design-changes","text":"Date: 10/07/2019 Description/Rationale: Capsule update is the process where each OEM has a lot of interest. Especially when there is capsule update failure, it is helpful to gather more information of the failure. With existed implementation, SetImage routine from FmpDxe driver, which is the most heavy lifting function call during capsule update, will only populate LastAttemptStatus with limited pre-defined error codes which could be consumed/inspected by the OS when it recovers and boots. Thus our proposal is to update the SetImage routine and leverage the LAST_ATTEMPT_STATUS_ERROR_UNSUCCESSFUL_VENDOR_RANGE range newly defined in UEFI Spec 2.8 Section 23.4, so that the error code will provide better granularity when viewing capsule update failure from OS device manager. Changes: A few error codes (128 total) are reserved from LAST_ATTEMPT_STATUS_ERROR_UNSUCCESSFUL_VENDOR_RANGE range for FmpDxe driver usage, which ranges from thermal and power API failure to capsule payload header check failure. Furthermore, an output pointer of the LastAttemptStatus is added as an input argument for FmpDeviceSetImage function in FmpDeviceLib to allow platform to provide their own platform specific error codes (SPI write failure, SVN checking failure, and more). Impact/Mitigation: The italic text above will cause a breaking change for all the FmpDeviceLib instances due to API change. This is to provide a better visibility for OEMs to decode the capsule update failure more efficiently. Each FmpDeviceLib should change to new API definition and populate proper LastAttemptStatus value when applicable.","title":"Design Changes"},{"location":"dyn/mu_tiano_plus/pytool/Readme/","text":"Edk2 Continuous Integration \u00b6 Basic Status \u00b6 Package Windows VS2019 (IA32/X64) Ubuntu GCC (IA32/X64/ARM/AARCH64) Known Issues CryptoPkg Spell checking in audit mode EmbeddedPkg FatPkg FmpDevicePkg ShellPkg Spell checking in audit mode, 3 modules are not being built by DSC SourceLevelDebugPkg For more detailed status look at the test results of the latest CI run on the repo readme. Background \u00b6 This Continuous integration and testing infrastructure leverages the TianoCore EDKII Tools PIP modules: library and extensions (with repos located here and here ). The primary execution flows can be found in the .azurepipelines / Windows-VS2019.yml and .azurepipelines / Ubuntu-GCC5.yml files. These YAML files are consumed by the Azure Dev Ops Build Pipeline and dictate what server resources should be used, how they should be configured, and what processes should be run on them. An overview of this schema can be found here . Inspection of these files reveals the EDKII Tools commands that make up the primary processes for the CI build: 'stuart_setup', 'stuart_update', and 'stuart_ci_build'. These commands come from the EDKII Tools PIP modules and are configured as described below. More documentation on the tools can be found here and here . Configuration \u00b6 Configuration of the CI process consists of (in order of precedence): command-line arguments passed in via the Pipeline YAML a per-package configuration file (e.g. <package-name>.ci.yaml ) that is detected by the CI system in EDKII Tools. a global configuration Python module (e.g. CISetting.py ) passed in via the command-line The global configuration file is described in this readme from the EDKII Tools documentation. This configuration is written as a Python module so that decisions can be made dynamically based on command line parameters and codebase state. The per-package configuration file can override most settings in the global configuration file, but is not dynamic. This file can be used to skip or customize tests that may be incompatible with a specific package. Each test generally requires per package configuration which comes from this file. Running CI locally \u00b6 The EDKII Tools environment (and by extension the ci) is designed to support easily and consistantly running locally and in a cloud ci environment. To do that a few steps should be followed. Details of EDKII Tools can be found in the docs folder here Prerequisets \u00b6 A supported toolchain (others might work but this is what is tested and validated) Windows 10: VS 2017 or VS 2019 Windows SDK (for rc) Windows WDK (for capsules) Ubuntu 16.04 GCC5 Easy to add more but this is the current state Python 3.7.x or newer on path git on path Recommended to setup and activate a python virtual environment Install the requirements pip install --upgrade pip-requirements.txt Running CI \u00b6 clone your edk2 repo Activate your python virtual environment in cmd window Get code dependencies (done only when submodules change) stuart_setup -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> Update other dependencies (done more often) stuart_update -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> Run CI build (--help will give you options) stuart_ci_build -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> -p : To build only certain packages use a CSV list -a : To run only certain architectures use a CSV list -t : To run only tests related to certain targets use a CSV list By default all tests are opted in. Then given a package.ci.yaml file those tests can be configured for a package. Finally setting the check to the value skip will skip that plugin. Examples: CompilerPlugin=skip skip the build test GuidCheck=skip skip the Guid check SpellCheck=skip skip the spell checker etc Detailed reports and logs per package are captured in the Build directory Current PyTool Test Capabilities \u00b6 All CI tests are instances of EDKII Tools plugins. Documentation on the plugin system can be found here and here . Upon invocation, each plugin will be passed the path to the current package under test and a dictionary containing its targeted configuration, as assembled from the command line, per-package configuration, and global configuration. Note: CI plugins are considered unique from build plugins and helper plugins, even though some CI plugins may execute steps of a build. In the example, these plugins live alongside the code under test (in the .pytool / Plugin directory), but may be moved to the 'edk2-test' repo if that location makes more sense for the community. Module Inclusion Test - DscCompleteCheck \u00b6 This test scans all available modules (via INF files) and compares them to the package-level DSC file for the package each module is contained within. The test considers it an error if any module does not appear in the Components section of at least one package-level DSC (indicating that it would not be built if the package were built). Code Compilation Test - CompilerPlugin \u00b6 Once the Module Inclusion Test has verified that all modules would be built if all package-level DSCs were built, the Code Compilation Test simply runs through and builds every package-level DSC on every toolchain and for every architecture that is supported. Any module that fails to build is considered an error. GUID Uniqueness Test - GuidCheck \u00b6 This test works on the collection of all packages rather than an individual package. It looks at all FILE_GUIDs and GUIDs declared in DEC files and ensures that they are unique for the codebase. This prevents, for example, accidental duplication of GUIDs when using an existing INF as a template for a new module. Cross-Package Dependency Test - DependencyCheck \u00b6 This test compares the list of all packages used in INFs files for a given package against a list of \"allowed dependencies\" in plugin configuration for that package. Any module that depends on a disallowed package will cause a test failure. Library Declaration Test - LibraryClassCheck \u00b6 This test scans at all library header files found in the Library folders in all of the package's declared include directories and ensures that all files have a matching LibraryClass declaration in the DEC file for the package. Any missing declarations will cause a failure. Invalid Character Test - CharEncodingCheck \u00b6 This test scans all files in a package to make sure that there are no invalid Unicode characters that may cause build errors in some character sets/localizations. Spell Checking - cspell \u00b6 This test runs a spell checker on all files within the package. This is done using the NodeJs cspell tool. For details check .pytool / Plugin / SpellCheck . For this plugin to run during ci you must install nodejs and cspell and have both available to the command line when running your CI. Install Install nodejs from https://nodejs.org/en/ Install cspell Open cmd prompt with access to node and npm Run npm install -g cspell More cspell info: https://github.com/streetsidesoftware/cspell PyTool Scopes \u00b6 Scopes are how the PyTool ext_dep, path_env, and plugins are activated. Meaning that if an invocable process has a scope active then those ext_dep and path_env will be active. To allow easy integration of PyTools capabilities there are a few standard scopes. Scope Invocable Description global edk2_invocable++ - should be base_abstract_invocable Running an invocables global-win edk2_invocable++ Running on Microsoft Windows global-nix edk2_invocable++ Running on Linux based OS edk2-build This indicates that an invocable is building EDK2 based UEFI code cibuild set in .pytool/CISettings.py Suggested target for edk2 continuous integration builds. Tools used for CiBuilds can use this scope. Example: asl compiler Future investments \u00b6 PatchCheck tests as plugins MacOS/xcode support Clang/LLVM support Visual Studio AARCH64 and ARM support BaseTools C tools CI/PR and binary release process BaseTools Python tools CI/PR process Host based unit testing Extensible private/closed source platform reporting Platform builds, validation UEFI SCTs Other automation","title":"pytool"},{"location":"dyn/mu_tiano_plus/pytool/Readme/#edk2-continuous-integration","text":"","title":"Edk2 Continuous Integration"},{"location":"dyn/mu_tiano_plus/pytool/Readme/#basic-status","text":"Package Windows VS2019 (IA32/X64) Ubuntu GCC (IA32/X64/ARM/AARCH64) Known Issues CryptoPkg Spell checking in audit mode EmbeddedPkg FatPkg FmpDevicePkg ShellPkg Spell checking in audit mode, 3 modules are not being built by DSC SourceLevelDebugPkg For more detailed status look at the test results of the latest CI run on the repo readme.","title":"Basic Status"},{"location":"dyn/mu_tiano_plus/pytool/Readme/#background","text":"This Continuous integration and testing infrastructure leverages the TianoCore EDKII Tools PIP modules: library and extensions (with repos located here and here ). The primary execution flows can be found in the .azurepipelines / Windows-VS2019.yml and .azurepipelines / Ubuntu-GCC5.yml files. These YAML files are consumed by the Azure Dev Ops Build Pipeline and dictate what server resources should be used, how they should be configured, and what processes should be run on them. An overview of this schema can be found here . Inspection of these files reveals the EDKII Tools commands that make up the primary processes for the CI build: 'stuart_setup', 'stuart_update', and 'stuart_ci_build'. These commands come from the EDKII Tools PIP modules and are configured as described below. More documentation on the tools can be found here and here .","title":"Background"},{"location":"dyn/mu_tiano_plus/pytool/Readme/#configuration","text":"Configuration of the CI process consists of (in order of precedence): command-line arguments passed in via the Pipeline YAML a per-package configuration file (e.g. <package-name>.ci.yaml ) that is detected by the CI system in EDKII Tools. a global configuration Python module (e.g. CISetting.py ) passed in via the command-line The global configuration file is described in this readme from the EDKII Tools documentation. This configuration is written as a Python module so that decisions can be made dynamically based on command line parameters and codebase state. The per-package configuration file can override most settings in the global configuration file, but is not dynamic. This file can be used to skip or customize tests that may be incompatible with a specific package. Each test generally requires per package configuration which comes from this file.","title":"Configuration"},{"location":"dyn/mu_tiano_plus/pytool/Readme/#running-ci-locally","text":"The EDKII Tools environment (and by extension the ci) is designed to support easily and consistantly running locally and in a cloud ci environment. To do that a few steps should be followed. Details of EDKII Tools can be found in the docs folder here","title":"Running CI locally"},{"location":"dyn/mu_tiano_plus/pytool/Readme/#prerequisets","text":"A supported toolchain (others might work but this is what is tested and validated) Windows 10: VS 2017 or VS 2019 Windows SDK (for rc) Windows WDK (for capsules) Ubuntu 16.04 GCC5 Easy to add more but this is the current state Python 3.7.x or newer on path git on path Recommended to setup and activate a python virtual environment Install the requirements pip install --upgrade pip-requirements.txt","title":"Prerequisets"},{"location":"dyn/mu_tiano_plus/pytool/Readme/#running-ci","text":"clone your edk2 repo Activate your python virtual environment in cmd window Get code dependencies (done only when submodules change) stuart_setup -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> Update other dependencies (done more often) stuart_update -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> Run CI build (--help will give you options) stuart_ci_build -c .pytool/CISettings.py TOOL_CHAIN_TAG=<your tag here> -p : To build only certain packages use a CSV list -a : To run only certain architectures use a CSV list -t : To run only tests related to certain targets use a CSV list By default all tests are opted in. Then given a package.ci.yaml file those tests can be configured for a package. Finally setting the check to the value skip will skip that plugin. Examples: CompilerPlugin=skip skip the build test GuidCheck=skip skip the Guid check SpellCheck=skip skip the spell checker etc Detailed reports and logs per package are captured in the Build directory","title":"Running CI"},{"location":"dyn/mu_tiano_plus/pytool/Readme/#current-pytool-test-capabilities","text":"All CI tests are instances of EDKII Tools plugins. Documentation on the plugin system can be found here and here . Upon invocation, each plugin will be passed the path to the current package under test and a dictionary containing its targeted configuration, as assembled from the command line, per-package configuration, and global configuration. Note: CI plugins are considered unique from build plugins and helper plugins, even though some CI plugins may execute steps of a build. In the example, these plugins live alongside the code under test (in the .pytool / Plugin directory), but may be moved to the 'edk2-test' repo if that location makes more sense for the community.","title":"Current PyTool Test Capabilities"},{"location":"dyn/mu_tiano_plus/pytool/Readme/#module-inclusion-test-dsccompletecheck","text":"This test scans all available modules (via INF files) and compares them to the package-level DSC file for the package each module is contained within. The test considers it an error if any module does not appear in the Components section of at least one package-level DSC (indicating that it would not be built if the package were built).","title":"Module Inclusion Test - DscCompleteCheck"},{"location":"dyn/mu_tiano_plus/pytool/Readme/#code-compilation-test-compilerplugin","text":"Once the Module Inclusion Test has verified that all modules would be built if all package-level DSCs were built, the Code Compilation Test simply runs through and builds every package-level DSC on every toolchain and for every architecture that is supported. Any module that fails to build is considered an error.","title":"Code Compilation Test - CompilerPlugin"},{"location":"dyn/mu_tiano_plus/pytool/Readme/#guid-uniqueness-test-guidcheck","text":"This test works on the collection of all packages rather than an individual package. It looks at all FILE_GUIDs and GUIDs declared in DEC files and ensures that they are unique for the codebase. This prevents, for example, accidental duplication of GUIDs when using an existing INF as a template for a new module.","title":"GUID Uniqueness Test - GuidCheck"},{"location":"dyn/mu_tiano_plus/pytool/Readme/#cross-package-dependency-test-dependencycheck","text":"This test compares the list of all packages used in INFs files for a given package against a list of \"allowed dependencies\" in plugin configuration for that package. Any module that depends on a disallowed package will cause a test failure.","title":"Cross-Package Dependency Test - DependencyCheck"},{"location":"dyn/mu_tiano_plus/pytool/Readme/#library-declaration-test-libraryclasscheck","text":"This test scans at all library header files found in the Library folders in all of the package's declared include directories and ensures that all files have a matching LibraryClass declaration in the DEC file for the package. Any missing declarations will cause a failure.","title":"Library Declaration Test - LibraryClassCheck"},{"location":"dyn/mu_tiano_plus/pytool/Readme/#invalid-character-test-charencodingcheck","text":"This test scans all files in a package to make sure that there are no invalid Unicode characters that may cause build errors in some character sets/localizations.","title":"Invalid Character Test - CharEncodingCheck"},{"location":"dyn/mu_tiano_plus/pytool/Readme/#spell-checking-cspell","text":"This test runs a spell checker on all files within the package. This is done using the NodeJs cspell tool. For details check .pytool / Plugin / SpellCheck . For this plugin to run during ci you must install nodejs and cspell and have both available to the command line when running your CI. Install Install nodejs from https://nodejs.org/en/ Install cspell Open cmd prompt with access to node and npm Run npm install -g cspell More cspell info: https://github.com/streetsidesoftware/cspell","title":"Spell Checking - cspell"},{"location":"dyn/mu_tiano_plus/pytool/Readme/#pytool-scopes","text":"Scopes are how the PyTool ext_dep, path_env, and plugins are activated. Meaning that if an invocable process has a scope active then those ext_dep and path_env will be active. To allow easy integration of PyTools capabilities there are a few standard scopes. Scope Invocable Description global edk2_invocable++ - should be base_abstract_invocable Running an invocables global-win edk2_invocable++ Running on Microsoft Windows global-nix edk2_invocable++ Running on Linux based OS edk2-build This indicates that an invocable is building EDK2 based UEFI code cibuild set in .pytool/CISettings.py Suggested target for edk2 continuous integration builds. Tools used for CiBuilds can use this scope. Example: asl compiler","title":"PyTool Scopes"},{"location":"dyn/mu_tiano_plus/pytool/Readme/#future-investments","text":"PatchCheck tests as plugins MacOS/xcode support Clang/LLVM support Visual Studio AARCH64 and ARM support BaseTools C tools CI/PR and binary release process BaseTools Python tools CI/PR process Host based unit testing Extensible private/closed source platform reporting Platform builds, validation UEFI SCTs Other automation","title":"Future investments"}]}